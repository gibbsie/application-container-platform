{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Application Container Platform # This page will give you an overview of ACP, the services it entails and the ways to get access to them. This is a general overview. Developers should check out the Developer Docs . The Application Container Platform provides an accredited, secure, scalable, reliable and performant hosting platform for Home Office applications.\u200b We use open source technology to allow delivery teams to develop, build, deploy and manage applications in a trusted environment. \u200b We focus on enabling self-service where possible, and have developed a Platform Hub to act as a one stop shop for all things ACP, including support requests for things that aren't self serve (yet). Benefits of using ACP # Reliance on specialist DevOps resource significantly reduced\u200b Central place to host applications\u200b Saves time for the projects\u200b Significant hosting cost reduction\u200b Common tools and patterns across projects i.e. CI\u200b Everyone using our platform MUST adhere to technical service standards . All Services will be under a review process. For definitions on each phase of the lifecycle and the criteria for production-ready, please refer here . Accessing the Services # When trying to get onto the platform for the first time you will need to get access to a various things that will allow you to get to other things. The full flow for this can be found here Support # You can find more information about support here If you require support you can talk to us on our Slack Channel or raise an issue on our BAU Board .","title":"Home"},{"location":"index.html#application-container-platform","text":"This page will give you an overview of ACP, the services it entails and the ways to get access to them. This is a general overview. Developers should check out the Developer Docs . The Application Container Platform provides an accredited, secure, scalable, reliable and performant hosting platform for Home Office applications.\u200b We use open source technology to allow delivery teams to develop, build, deploy and manage applications in a trusted environment. \u200b We focus on enabling self-service where possible, and have developed a Platform Hub to act as a one stop shop for all things ACP, including support requests for things that aren't self serve (yet).","title":"Application Container Platform"},{"location":"index.html#benefits-of-using-acp","text":"Reliance on specialist DevOps resource significantly reduced\u200b Central place to host applications\u200b Saves time for the projects\u200b Significant hosting cost reduction\u200b Common tools and patterns across projects i.e. CI\u200b Everyone using our platform MUST adhere to technical service standards . All Services will be under a review process. For definitions on each phase of the lifecycle and the criteria for production-ready, please refer here .","title":"Benefits of using ACP"},{"location":"index.html#accessing-the-services","text":"When trying to get onto the platform for the first time you will need to get access to a various things that will allow you to get to other things. The full flow for this can be found here","title":"Accessing the Services"},{"location":"index.html#support","text":"You can find more information about support here If you require support you can talk to us on our Slack Channel or raise an issue on our BAU Board .","title":"Support"},{"location":"external-addresses.html","text":"ACP External Addresses # The ACP clusters all run behind NAT gateways, with fixed external addresses. Note, unless you've specifically choosen to run in a specific availability zone, your pods can run in any of the AZ's for that cluster, so all three addresses should be taken as the external ip's. Cluster Zone External Address ACP-CI eu-west-2a 52.56.254.215 eu-west-2b 35.176.205.192 eu-west-2c 35.178.14.117 ACP-OPS eu-west-2a 35.176.238.26 eu-west-2b 35.177.41.205 eu-west-2c 35.176.100.151 ACP-PROD eu-west-2a 35.177.82.170 eu-west-2b 52.56.249.216 eu-west-2c 35.176.70.245 ACP-PROD-PX eu-west-2a 35.177.156.249 eu-west-2b 35.177.245.179 eu-west-2c 35.177.14.177 ACP-NOTPROD eu-west-2a 35.177.248.60 eu-west-2b 35.176.168.231 eu-west-2c 52.56.158.209 ACP-NOTPROD-PX eu-west-2a 52.56.221.144 eu-west-2b 52.56.45.52 eu-west-2c 35.177.167.246 ACP-TEST eu-west-2a 35.176.184.49 eu-west-2b 35.176.217.238 eu-west-2c 35.177.169.118 ACP-VPN (Access) eu-west-2a 52.56.221.216 eu-west-2b 18.130.11.142 eu-west-2c 18.130.6.5 Note: The ACP-VPN external IPs addresses only work for the \"Tunnel All Traffic\" VPN profiles.","title":"ACP External IPs"},{"location":"external-addresses.html#acp-external-addresses","text":"The ACP clusters all run behind NAT gateways, with fixed external addresses. Note, unless you've specifically choosen to run in a specific availability zone, your pods can run in any of the AZ's for that cluster, so all three addresses should be taken as the external ip's. Cluster Zone External Address ACP-CI eu-west-2a 52.56.254.215 eu-west-2b 35.176.205.192 eu-west-2c 35.178.14.117 ACP-OPS eu-west-2a 35.176.238.26 eu-west-2b 35.177.41.205 eu-west-2c 35.176.100.151 ACP-PROD eu-west-2a 35.177.82.170 eu-west-2b 52.56.249.216 eu-west-2c 35.176.70.245 ACP-PROD-PX eu-west-2a 35.177.156.249 eu-west-2b 35.177.245.179 eu-west-2c 35.177.14.177 ACP-NOTPROD eu-west-2a 35.177.248.60 eu-west-2b 35.176.168.231 eu-west-2c 52.56.158.209 ACP-NOTPROD-PX eu-west-2a 52.56.221.144 eu-west-2b 52.56.45.52 eu-west-2c 35.177.167.246 ACP-TEST eu-west-2a 35.176.184.49 eu-west-2b 35.176.217.238 eu-west-2c 35.177.169.118 ACP-VPN (Access) eu-west-2a 52.56.221.216 eu-west-2b 18.130.11.142 eu-west-2c 18.130.6.5 Note: The ACP-VPN external IPs addresses only work for the \"Tunnel All Traffic\" VPN profiles.","title":"ACP External Addresses"},{"location":"newuser.html","text":"New Users # Flow for new users # This document will go over the flow for getting access to our various services if you are finding difficulties with the specifics of the tasks outlined in this document please see the more detailed Developer Docs . Office 365 Account # Licensed Office 365 accounts should be requested from your Programme Management Office. We can request AD-only accounts to facilitate access to ACP services via Single Sign On as explained on the Support page Please note that to use SSO, which is using your Office 365 account to log onto other services, pop-ups will need to be enabled on your computer. VPN # VPN profiles are used to be able to get through to our multiple AWS environments as well as other data centers. The profiles can be downloaded from access which you can access using your Office 365 SSO. The following services can be only accessed when connected to the ACP Platform VPN profile. Platform Hub # The Platform Hub serves as a central portal for users of ACP. It acts as an all-in-one place to find information, requests and also support for the platform. The Hub also provides tools to develop, build, deploy and manage all your projects. Internally Accessible # Artifactory # Artifactory - Once you have an Office 365 account, you can sign in using the HOD SSO button. Requests for an Artifactory robot can be made on the Platform Hub . The docker repository in Artifactory is accessed via the docker.digital.homeoffice.gov.uk address. GitLab # GitLab - Once you have an Office 365 account, you can sign in using the Office 365 button. Kubernetes # You will need a Kubernetes user token before you can access any of the namespaces in the clusters. Follow the instructions here to get one for your teams namespaces: Getting a Kubernetes Token for the UK cluster Drone # We have two instances; One for GitHub and one for GitLab . Kibana # Kibana is accessible to ACP users via the default VPN profile. Sysdig # Sysdig is also accessible to ACP users via the default VPN profile and requests for specific team views can be made on the Platform Hub . AWS Resources (S3 Buckets and RDS Instances) # If you require an S3 bucket or an RDS instance you can submit a support request on the Platform Hub . Externally Accessible (for open source projects) # The following services can be access without a VPN connection and with personal accounts. GitHub # Github - To access our repositories you must have your personal Github invited to the UK Home Office organisation on Github. This is done during your first login to the Platform Hub. Please note that you will have to have your full name on your Github profile along with 2 Factor Authentication. If you don't have a personal account on Github, then you will need to create one. Quay # Quay - You can access Quay using your GitHub account. Once you have been added to the ukhomeoffice organisation on Quay, you can create repositories in the organisation. If you are not already part of the organisation you can submit a support request on the Platform Hub to be added . You will also need a robot created for you to push to your repository once you have created one. Requests for a Quay robot can be made on the Platform Hub .","title":"New User Guide"},{"location":"newuser.html#new-users","text":"","title":"New Users"},{"location":"newuser.html#flow-for-new-users","text":"This document will go over the flow for getting access to our various services if you are finding difficulties with the specifics of the tasks outlined in this document please see the more detailed Developer Docs .","title":"Flow for new users"},{"location":"newuser.html#office-365-account","text":"Licensed Office 365 accounts should be requested from your Programme Management Office. We can request AD-only accounts to facilitate access to ACP services via Single Sign On as explained on the Support page Please note that to use SSO, which is using your Office 365 account to log onto other services, pop-ups will need to be enabled on your computer.","title":"Office 365 Account"},{"location":"newuser.html#vpn","text":"VPN profiles are used to be able to get through to our multiple AWS environments as well as other data centers. The profiles can be downloaded from access which you can access using your Office 365 SSO. The following services can be only accessed when connected to the ACP Platform VPN profile.","title":"VPN"},{"location":"newuser.html#platform-hub","text":"The Platform Hub serves as a central portal for users of ACP. It acts as an all-in-one place to find information, requests and also support for the platform. The Hub also provides tools to develop, build, deploy and manage all your projects.","title":"Platform Hub"},{"location":"newuser.html#internally-accessible","text":"","title":"Internally Accessible"},{"location":"newuser.html#artifactory","text":"Artifactory - Once you have an Office 365 account, you can sign in using the HOD SSO button. Requests for an Artifactory robot can be made on the Platform Hub . The docker repository in Artifactory is accessed via the docker.digital.homeoffice.gov.uk address.","title":"Artifactory"},{"location":"newuser.html#gitlab","text":"GitLab - Once you have an Office 365 account, you can sign in using the Office 365 button.","title":"GitLab"},{"location":"newuser.html#kubernetes","text":"You will need a Kubernetes user token before you can access any of the namespaces in the clusters. Follow the instructions here to get one for your teams namespaces: Getting a Kubernetes Token for the UK cluster","title":"Kubernetes"},{"location":"newuser.html#drone","text":"We have two instances; One for GitHub and one for GitLab .","title":"Drone"},{"location":"newuser.html#kibana","text":"Kibana is accessible to ACP users via the default VPN profile.","title":"Kibana"},{"location":"newuser.html#sysdig","text":"Sysdig is also accessible to ACP users via the default VPN profile and requests for specific team views can be made on the Platform Hub .","title":"Sysdig"},{"location":"newuser.html#aws-resources-s3-buckets-and-rds-instances","text":"If you require an S3 bucket or an RDS instance you can submit a support request on the Platform Hub .","title":"AWS Resources (S3 Buckets and RDS Instances)"},{"location":"newuser.html#externally-accessible-for-open-source-projects","text":"The following services can be access without a VPN connection and with personal accounts.","title":"Externally Accessible (for open source projects)"},{"location":"newuser.html#github","text":"Github - To access our repositories you must have your personal Github invited to the UK Home Office organisation on Github. This is done during your first login to the Platform Hub. Please note that you will have to have your full name on your Github profile along with 2 Factor Authentication. If you don't have a personal account on Github, then you will need to create one.","title":"GitHub"},{"location":"newuser.html#quay","text":"Quay - You can access Quay using your GitHub account. Once you have been added to the ukhomeoffice organisation on Quay, you can create repositories in the organisation. If you are not already part of the organisation you can submit a support request on the Platform Hub to be added . You will also need a robot created for you to push to your repository once you have created one. Requests for a Quay robot can be made on the Platform Hub .","title":"Quay"},{"location":"rbac.html","text":"RBAC Groups # RBAC groups are used to control the level of access a user has to a namespace (or cluster). The RBAC groups that you need assign when creating a token will be dependant on the level of access needed for the user. Group names are created in the format: acp: cluster-name : user-type :[cw|ns]- role-type : namespace with cw|ns being cluster wide or namespace specific. Here is an example of an RBAC group: acp:notprod:robot:ns-robot:hello-world This indicates that this token: is for the notprod cluster is for a robot user (normal users would have user here instead) is defined for this particular namespace has a role type is robot which will give the user (which will most likely be a robot) that will use the token certain permissions in the namespace. Another role type is readonly which allows a user to list/get all of the resources in a namespace * will be used with a namespace called hello-world","title":"RBAC Guide"},{"location":"rbac.html#rbac-groups","text":"RBAC groups are used to control the level of access a user has to a namespace (or cluster). The RBAC groups that you need assign when creating a token will be dependant on the level of access needed for the user. Group names are created in the format: acp: cluster-name : user-type :[cw|ns]- role-type : namespace with cw|ns being cluster wide or namespace specific. Here is an example of an RBAC group: acp:notprod:robot:ns-robot:hello-world This indicates that this token: is for the notprod cluster is for a robot user (normal users would have user here instead) is defined for this particular namespace has a role type is robot which will give the user (which will most likely be a robot) that will use the token certain permissions in the namespace. Another role type is readonly which allows a user to list/get all of the resources in a namespace * will be used with a namespace called hello-world","title":"RBAC Groups"},{"location":"service-lifecycle.html","text":"Service Lifecycle # This defines the current service lifecycle stages. This isn't specific to the hosting platform and will no doubt move to somewhere more organistionally wide. However, this is a starting point to clearly communicate the phases of our evolution. Alpha # Alpha is an experimental phase to test the hypothesis, by building prototypes to validate the direction and the intention of the service. It is there to explore ways of achieving and meeting the user needs in the right way. There is more information on this here and here Beta # This is the phase where software is more feature complete. It is aimed at being functional and meeting the definition of a Minimal Viable Product, (MVP). The service is accessible and secured and meets a lot of the production requirements and needs. However, there will be an assessment of the service through closed user groups, to evaluate that it meets the user needs. SLA's, SLO's and availability may not meet the user expectation of a live service. Live # Once the products have gone through the beta process, (which is to identify that the MVP is viable), the service will transition to live. In live, we are happy that the service has undergone enough evaluation to meet user expectations as well as being able to commit confidentially to SLA's and SLO's. Production-Ready Criteria # To define what it means to be \"production-ready\", there is a set of criteria by which we assess services. This is not a concrete list and is likely to evolve. We would expect Beta and Live services to adhere to this criteria list, however, it is possible that a beta may decide that some are not necessary, depending on the size of the user group and the communication to said group. Generally speaking, it is best not to rely or depend on a Beta service. Resilient (Recover from Restart) Highly available (Multiple Instances) Backups of all data Validation of backup, (not 0 file size) Restores of data tested CI Release process Continuous deployment (if needed) to prod, (done via CI) Is it independent of itself i.e. it doesn't rely on itself Tests defined and ran as part of CI process Monitoring of product Does it have an ATO? Monitoring Dashboards produced Alerting in place and tested Patching Process Defined + Tested Intrusion detection in place Security tested Does it log adequately? Logs persisted Non root user if docker is used Readonly root filesystem and hardening of container Documentation in place and standards Default Admin Password Changed Dev instance / playground SSO where needed Change Management process defined Incident Management process defined (incl agreed SLAs) Monitoring of cert expiry","title":"Service Lifecycle"},{"location":"service-lifecycle.html#service-lifecycle","text":"This defines the current service lifecycle stages. This isn't specific to the hosting platform and will no doubt move to somewhere more organistionally wide. However, this is a starting point to clearly communicate the phases of our evolution.","title":"Service Lifecycle"},{"location":"service-lifecycle.html#alpha","text":"Alpha is an experimental phase to test the hypothesis, by building prototypes to validate the direction and the intention of the service. It is there to explore ways of achieving and meeting the user needs in the right way. There is more information on this here and here","title":"Alpha"},{"location":"service-lifecycle.html#beta","text":"This is the phase where software is more feature complete. It is aimed at being functional and meeting the definition of a Minimal Viable Product, (MVP). The service is accessible and secured and meets a lot of the production requirements and needs. However, there will be an assessment of the service through closed user groups, to evaluate that it meets the user needs. SLA's, SLO's and availability may not meet the user expectation of a live service.","title":"Beta"},{"location":"service-lifecycle.html#live","text":"Once the products have gone through the beta process, (which is to identify that the MVP is viable), the service will transition to live. In live, we are happy that the service has undergone enough evaluation to meet user expectations as well as being able to commit confidentially to SLA's and SLO's.","title":"Live"},{"location":"service-lifecycle.html#production-ready-criteria","text":"To define what it means to be \"production-ready\", there is a set of criteria by which we assess services. This is not a concrete list and is likely to evolve. We would expect Beta and Live services to adhere to this criteria list, however, it is possible that a beta may decide that some are not necessary, depending on the size of the user group and the communication to said group. Generally speaking, it is best not to rely or depend on a Beta service. Resilient (Recover from Restart) Highly available (Multiple Instances) Backups of all data Validation of backup, (not 0 file size) Restores of data tested CI Release process Continuous deployment (if needed) to prod, (done via CI) Is it independent of itself i.e. it doesn't rely on itself Tests defined and ran as part of CI process Monitoring of product Does it have an ATO? Monitoring Dashboards produced Alerting in place and tested Patching Process Defined + Tested Intrusion detection in place Security tested Does it log adequately? Logs persisted Non root user if docker is used Readonly root filesystem and hardening of container Documentation in place and standards Default Admin Password Changed Dev instance / playground SSO where needed Change Management process defined Incident Management process defined (incl agreed SLAs) Monitoring of cert expiry","title":"Production-Ready Criteria"},{"location":"services.html","text":"These are the core services which we provide across our platform. VPN # Many of our services are behind a VPN (using OpenVPN protocol) for security reasons. We have many different roles and profiles for accessing different environments. These profiles are all found at Access ACP . You can download VPN profiles for the environments you have access to. New profiles can be set up to connect through to things like project specific AWS accounts. Each profile will expire after a certain amount of time (2-12 hours), users will have to download a new profile once their certificates which are baked into the openvpn profiles expire. Source Code Management # GitHub GitHub is where we store our open source code. You need to be added to the Uk HomeOffice organisation to access most of our code. More documentation can be found here GitLab GitLab is where we store our more private code. Hosted in our ops cluster, you will need an office 365 account to get in via single sign on. Each project has its own group which are managed by members of that project. More in depth guides to gitlab can be found here . CI # Drone Drone is our CI tool for building and deploying applications. We have two instances, one for GitLab and one for GitHub . More instructions can be found here . Jenkins Jenkins is considered a legacy system and is not supported. Binary / Artifact Storage # Quay Quay is where store all of our open source container images. ECR We use AWS ECR to host our private container images for high availability and resiliency. For more information on how to use ECR see here Artifactory We use Artifactory as our internal binary repository store, projects can push build artifacts / dependencies here. Prior to the introduction of ECR, docker images were also push here, and can be access via https://docker.digital.homeoffice.gov.uk, log in via the Office 365 button to view your artifacts. Domain Name System (DNS) Pattern # To standardise on how services route their application traffic to the appropriate hosting platform and to offer consistency in how we approach DNS we have a standard DNS naming convention for services. In parallel to this, users need to also be aware of the limits on certificates and letsencrypt if they are wanting external TLS certificates for their services. For Non-production Services The following categories are something we would expect a service to specify: Service Name - The name of the service users or other services will attempt to consume i.e. web-portal Env - The environment of the service i.e. Dev Service - The overall service name or project name i.e. example-service servicename . env . service -notprod.homeoffice.gov.uk web-portal.dev.example-service-notprod.homeoffice.gov.uk For Production Services As we want to protect production services from hitting limits and to create a distinction between services that are non-production, (not prod) and production, we simplify the overall approach by using the main service name as the domain. Service Name - The name of the service users or other services will attempt to consume i.e. web-portal Service - The overall service name or project name i.e. example-service servicename . service .homeoffice.gov.uk web-portal.example-service.homeoffice.gov.uk Application Composition # The following are containers that we create for use alongside your own application Keycloak Gatekeeper Keycloak Proxy : a container for putting auth in front of your application. Nginx Proxy Nginx Proxy : for TLS and proxying your application container. cfssl Sidekick cfssl-sidecar : for providing a server TLS cert on demand from a cluster hosted cfssl server. Logging # Logging stack consists of Elasticsearch , Logstash , Kibana ). Logstash agents deployed as a daemonSet will collect all workload logs and index them in Elasticsearch. Logs are searchable for a period of 5 days through Kibana UI . Access to view logs can be requested via Support request . Current Log Retention Policy Logs are searchable in Kibana for 5 days and remain within Elasticsearch for 10 days. Collected workload logs will be persisted in S3 indefinitely and migrated to the infrequent access storage class and then glacier storage after 60 and 180 days respectively. NOTE: this may change in the future! The same policy applies to all logs within ELK Metrics / Monitoring # Sysdig Sysdig is our metric collection tool. We are working closely with Sysdig on the development of this. It can be used for dashboards, alerting and much more. More information can be found on the Sysdig Monitor site and for the open-source tool, Sysdig (command line tool) and Sysdig Inspect . Sysdig training Sysdig have an online training course which covers the basics of using the product, Sysdig 101 . Sysdig recommends new users complete this training. Security and disaster recovery # The Application Container Platform employs robust security principles, including but not limited to: encryption of data at rest and in transit restricting access to resources according to operational needs strict authorisation requirements for all endpoints role based access control The Platform is spread across multiple availability zones, which are essentially three different data centres within a region. In case of an entire AWS region going down for a prolonged period of time, the Platform can be recreated in another region within a few hours. The recovery of products hosted on the Platform are subject to considerations set out for the Production Ready criteria in Service Lifecycle . For further information on security and disaster recovery considerations, please raise a ticket on the BAU Board . Reusable components # Whilst building ACP, we've written a things that other projects may be interested in reusing. These can be found on GitHub here Terraform modules Base docker images","title":"Services"},{"location":"services.html#vpn","text":"Many of our services are behind a VPN (using OpenVPN protocol) for security reasons. We have many different roles and profiles for accessing different environments. These profiles are all found at Access ACP . You can download VPN profiles for the environments you have access to. New profiles can be set up to connect through to things like project specific AWS accounts. Each profile will expire after a certain amount of time (2-12 hours), users will have to download a new profile once their certificates which are baked into the openvpn profiles expire.","title":"VPN"},{"location":"services.html#source-code-management","text":"","title":"Source Code Management"},{"location":"services.html#ci","text":"","title":"CI"},{"location":"services.html#binary-artifact-storage","text":"","title":"Binary / Artifact Storage"},{"location":"services.html#domain-name-system-dns-pattern","text":"To standardise on how services route their application traffic to the appropriate hosting platform and to offer consistency in how we approach DNS we have a standard DNS naming convention for services. In parallel to this, users need to also be aware of the limits on certificates and letsencrypt if they are wanting external TLS certificates for their services.","title":"Domain Name System (DNS) Pattern"},{"location":"services.html#application-composition","text":"The following are containers that we create for use alongside your own application","title":"Application Composition"},{"location":"services.html#logging","text":"Logging stack consists of Elasticsearch , Logstash , Kibana ). Logstash agents deployed as a daemonSet will collect all workload logs and index them in Elasticsearch. Logs are searchable for a period of 5 days through Kibana UI . Access to view logs can be requested via Support request .","title":"Logging"},{"location":"services.html#metrics-monitoring","text":"","title":"Metrics / Monitoring"},{"location":"services.html#security-and-disaster-recovery","text":"The Application Container Platform employs robust security principles, including but not limited to: encryption of data at rest and in transit restricting access to resources according to operational needs strict authorisation requirements for all endpoints role based access control The Platform is spread across multiple availability zones, which are essentially three different data centres within a region. In case of an entire AWS region going down for a prolonged period of time, the Platform can be recreated in another region within a few hours. The recovery of products hosted on the Platform are subject to considerations set out for the Production Ready criteria in Service Lifecycle . For further information on security and disaster recovery considerations, please raise a ticket on the BAU Board .","title":"Security and disaster recovery"},{"location":"services.html#reusable-components","text":"Whilst building ACP, we've written a things that other projects may be interested in reusing. These can be found on GitHub here","title":"Reusable components"},{"location":"developer-docs/index.html","text":"ACP Developer Documentation # Introduction # ACP serves as a platform for teams to build and deploy projects in the Home Office. In addition to other technologies that we use, we strongly recommend to get an understanding of two of the core technologies that ACP is based on - Docker and Kubernetes: Docker is a software tool designed to make it easier to create, deploy and run applications by packaging up along with all its dependencies to containers. Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure. For further information on Docker and Kubernetes: Docker * Docker getting started tutorial Kubernetes * What is Kubernetes? * Kubernetes talk to introduce the concepts Developer getting started guide # Some prerequisites are required before developing on ACP. This guide will show you how to: Get access to the VPNs that allows you to connect to the platform Get setup with the kubectl client that lets you deploy applications to kubernetes Get access to Quay and Artifactory In addition, new developers should look at the new user flow documentation whilst going through this doc to serve as a checklist and make sure you are onboarded to all necessary platform services. Platform Hub # The Platform Hub serves as a central portal for users of ACP. It acts as an all-in-one place to find information, requests and also support for the platform. The hub also provides tools to develop, build, deploy and manage all your projects. You can submit support requests via the hub located in Support Requests under the Help Support tab on the sidebar. These request templates cover a wide majority of general requests that users normally need. If your issue/request is not listed, please use the General Request support request. Updates are also on its way to include more self-service tools, along with documentation, FAQs and live status updates. Access to the Platform Hub requires the ACP Platform VPN profile. Please make sure you have followed the Developer getting started guide for instructions on connecting to VPNs. Project getting started guide # Kubernetes resources (including your applications!) are always deployed into a particular namespace. These namespaces provide separation between the different projects hosted on the platform. A project service can have more than one namespace. For example, you can have namespaces that are different environments but are part of the same project service (e.g. dev and prod namespaces). Or if you have different namespaces that are all related to one project service (e.g. a web-api and an application namespace). For instructions on getting new namespaces and other relevant resources on getting started can be found below: ACP How-to Docs ACP How-to Docs # The How To Docs within the ACP repo provides a collection of how-to guides for both Developers and DevOps to use this Developer guide to Continuous Integration with Drone # Drone is what we use for Continuous Integration in ACP. This guide will cover how to use drone to test your PRs, build and push docker images, and to deploy. Writing Dockerfiles # This guide covers best practice for building Dockerfiles, and lists our standard base images.","title":"Developer Getting Started Guide"},{"location":"developer-docs/index.html#acp-developer-documentation","text":"","title":"ACP Developer Documentation"},{"location":"developer-docs/index.html#introduction","text":"ACP serves as a platform for teams to build and deploy projects in the Home Office. In addition to other technologies that we use, we strongly recommend to get an understanding of two of the core technologies that ACP is based on - Docker and Kubernetes: Docker is a software tool designed to make it easier to create, deploy and run applications by packaging up along with all its dependencies to containers. Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure. For further information on Docker and Kubernetes: Docker * Docker getting started tutorial Kubernetes * What is Kubernetes? * Kubernetes talk to introduce the concepts","title":"Introduction"},{"location":"developer-docs/index.html#developer-getting-started-guide","text":"Some prerequisites are required before developing on ACP. This guide will show you how to: Get access to the VPNs that allows you to connect to the platform Get setup with the kubectl client that lets you deploy applications to kubernetes Get access to Quay and Artifactory In addition, new developers should look at the new user flow documentation whilst going through this doc to serve as a checklist and make sure you are onboarded to all necessary platform services.","title":"Developer getting started guide"},{"location":"developer-docs/index.html#platform-hub","text":"The Platform Hub serves as a central portal for users of ACP. It acts as an all-in-one place to find information, requests and also support for the platform. The hub also provides tools to develop, build, deploy and manage all your projects. You can submit support requests via the hub located in Support Requests under the Help Support tab on the sidebar. These request templates cover a wide majority of general requests that users normally need. If your issue/request is not listed, please use the General Request support request. Updates are also on its way to include more self-service tools, along with documentation, FAQs and live status updates. Access to the Platform Hub requires the ACP Platform VPN profile. Please make sure you have followed the Developer getting started guide for instructions on connecting to VPNs.","title":"Platform Hub"},{"location":"developer-docs/index.html#project-getting-started-guide","text":"Kubernetes resources (including your applications!) are always deployed into a particular namespace. These namespaces provide separation between the different projects hosted on the platform. A project service can have more than one namespace. For example, you can have namespaces that are different environments but are part of the same project service (e.g. dev and prod namespaces). Or if you have different namespaces that are all related to one project service (e.g. a web-api and an application namespace). For instructions on getting new namespaces and other relevant resources on getting started can be found below: ACP How-to Docs","title":"Project getting started guide"},{"location":"developer-docs/index.html#acp-how-to-docs","text":"The How To Docs within the ACP repo provides a collection of how-to guides for both Developers and DevOps to use this","title":"ACP How-to Docs"},{"location":"developer-docs/index.html#developer-guide-to-continuous-integration-with-drone","text":"Drone is what we use for Continuous Integration in ACP. This guide will cover how to use drone to test your PRs, build and push docker images, and to deploy.","title":"Developer guide to Continuous Integration with Drone"},{"location":"developer-docs/index.html#writing-dockerfiles","text":"This guide covers best practice for building Dockerfiles, and lists our standard base images.","title":"Writing Dockerfiles"},{"location":"developer-docs/dev-setup.html","text":"Developer setup guide # Introduction # This guide aims to prepare developers to use the Application Container Platform. You must complete the steps below before attending the ACP Induction. All examples in this document are for Linux distributions and instructions for other operating systems will vary. If you choose to use a Windows device, please ensure that Windows Subsystem for Linux is installed. Set up GovWifi credentials Office 365 Connecting to ACP VPN Platform Hub registration Add a ssh key to Gitlab Required binaries User agreement Connecting to GovWifi # Please refer to Connect to GovWifi to obtain your credentials and set up wireless internet access via GovWifi. Office 365 # Platform users must have Office 365 Single Sign-On (SSO) credentials for the digital.homeoffice.gov.uk domain. Please get in touch with your Programme Management Office to request an account or raise an issue on the BAU Board. If you can't access the Board, please ask a colleague to raise a request on your behalf. You will not be able to follow through the rest of this guide unless you have Office 365 credentials. Connecting to ACP VPN # Most of ACP's services operate behind a VPN which is accessible with an openvpn client. Instructions on installing openvpn for your OS can be found at OpenVPN Once you've got your Office 365 SSO credentials, you can now navigate to Remote Access and login with your Office 365 account by clicking on the link on the right. Please download the VPN profile named \"ACP Platform (Ops, Dev, CI, Test)\" and use the openvpn client to connect. Verify that you can resolve the Platform Hub before continuing on. VPN profiles expire after 12 hours. You'll need to download and connect with a new VPN Profile when it expires. Platform Hub registration # Please note that you need to have your VPN running to access the Platform Hub and also talk to cluster APIs. You will need to register with the Platform Hub in order to gain access tokens for our Kubernetes clusters. Head to Platform Hub , you will need your O365 credentials to login/sign up. You will be asked to connect your Github account to your Platform Hub account - this will give you access to our BAU board, which is used to raise requests and issues regarding the Platform or your project. More information can be found in the developer documentation . Click on Support Requests under the Help Support heading on the navigation bar after connecting your Github identity to the Hub. Create a support request for Request access to Developer Induction . The support request will be sent and reviewed by a member of ACP team. Updates on the request will be seen as a notification on your Github account. Once created, your token will be shown under Kubernetes section of the Connected Identities tab on the sidebar. You can view all of your tokens by pressing the Show Tokens button. Add a ssh key to Gitlab # You will need to add a ssh public key to your Gitlab profile before attending the Induction. Please sign into Gitlab with Office 365 and add a ssh public key to your profile. Instructions for generating a ssh keypair can be found in Gitlab Docs . Required binaries # Before joining the Developer Induction, we kindly ask you to install binaries which are used for the deployment of applications in ACP - instructions on how to install these are shown below: Git Docker Drone Kubectl Install Git Verify if you have Git installed by: $ git --version git version 2.16.2 If Git is not installed, instructions on how to download and install it can be found over at the Git website . Please note We assume a basic knowledge of Git for the ACP Induction. If you've not used git before, or need to brush up on your skills please see Git basics . Install Docker You can follow the instructions to install docker from the Docker website . You can verify the installation is successful with: $ docker --version Docker version 18.03.1-ce, build 9ee9f40 Install Drone Drone CLI can be downloaded from the Drone CI website . Instructions installing on multiple operating systems are shown on the webpage. Verify that the installation is successful: $ drone --version drone version 0.8.0 Install Kubectl You can follow the instructions to install version 1.8 of kubectl from the Kubernetes website . You can verify the installation is successful with: $ kubectl version Client Version: version.Info{Major: 1 , Minor: 8 , GitVersion: v1.8.4 , GitCommit: 9befc2b8928a9426501d3bf62f72849d5cbcd5a3 , GitTreeState: clean , BuildDate: 2017-11-20T05:28:34Z , GoVersion: go1.8.3 , Compiler: gc , Platform: linux/amd64 } Connecting to the cluster In order to access a namespace you will need to configure kubectl to connect to our clusters - instructions on setting it up can be found on the Set up Kube Config button on the Connected Identities page. Verify that your kubectl is configured properly by trying to list pods and secrets in the acp-induction namespace: $ kubectl --context=acp-notprod_acp-induction --namespace=acp-induction get pods No resources found. $ kubectl --context=acp-notprod_acp-induction --namespace=acp-induction get secrets NAME TYPE DATA AGE default-token-dcnmg kubernetes.io/service-account-token 3 105d The output of the command above may differ if there are other pods or extra secrets deployed to the namespace. User agreement # Finally, please head over and read through our SLA documentation to familiarise yourself with the level of service ACP provides to its users including the level and hours of support on offer and issue escalation procedures.","title":"Developer setup guide"},{"location":"developer-docs/dev-setup.html#developer-setup-guide","text":"","title":"Developer setup guide"},{"location":"developer-docs/dev-setup.html#introduction","text":"This guide aims to prepare developers to use the Application Container Platform. You must complete the steps below before attending the ACP Induction. All examples in this document are for Linux distributions and instructions for other operating systems will vary. If you choose to use a Windows device, please ensure that Windows Subsystem for Linux is installed. Set up GovWifi credentials Office 365 Connecting to ACP VPN Platform Hub registration Add a ssh key to Gitlab Required binaries User agreement","title":"Introduction"},{"location":"developer-docs/dev-setup.html#connecting-to-govwifi","text":"Please refer to Connect to GovWifi to obtain your credentials and set up wireless internet access via GovWifi.","title":"Connecting to GovWifi"},{"location":"developer-docs/dev-setup.html#office-365","text":"Platform users must have Office 365 Single Sign-On (SSO) credentials for the digital.homeoffice.gov.uk domain. Please get in touch with your Programme Management Office to request an account or raise an issue on the BAU Board. If you can't access the Board, please ask a colleague to raise a request on your behalf. You will not be able to follow through the rest of this guide unless you have Office 365 credentials.","title":"Office 365"},{"location":"developer-docs/dev-setup.html#connecting-to-acp-vpn","text":"Most of ACP's services operate behind a VPN which is accessible with an openvpn client. Instructions on installing openvpn for your OS can be found at OpenVPN Once you've got your Office 365 SSO credentials, you can now navigate to Remote Access and login with your Office 365 account by clicking on the link on the right. Please download the VPN profile named \"ACP Platform (Ops, Dev, CI, Test)\" and use the openvpn client to connect. Verify that you can resolve the Platform Hub before continuing on. VPN profiles expire after 12 hours. You'll need to download and connect with a new VPN Profile when it expires.","title":"Connecting to ACP VPN"},{"location":"developer-docs/dev-setup.html#platform-hub-registration","text":"Please note that you need to have your VPN running to access the Platform Hub and also talk to cluster APIs. You will need to register with the Platform Hub in order to gain access tokens for our Kubernetes clusters. Head to Platform Hub , you will need your O365 credentials to login/sign up. You will be asked to connect your Github account to your Platform Hub account - this will give you access to our BAU board, which is used to raise requests and issues regarding the Platform or your project. More information can be found in the developer documentation . Click on Support Requests under the Help Support heading on the navigation bar after connecting your Github identity to the Hub. Create a support request for Request access to Developer Induction . The support request will be sent and reviewed by a member of ACP team. Updates on the request will be seen as a notification on your Github account. Once created, your token will be shown under Kubernetes section of the Connected Identities tab on the sidebar. You can view all of your tokens by pressing the Show Tokens button.","title":"Platform Hub registration"},{"location":"developer-docs/dev-setup.html#add-a-ssh-key-to-gitlab","text":"You will need to add a ssh public key to your Gitlab profile before attending the Induction. Please sign into Gitlab with Office 365 and add a ssh public key to your profile. Instructions for generating a ssh keypair can be found in Gitlab Docs .","title":"Add a ssh key to Gitlab"},{"location":"developer-docs/dev-setup.html#required-binaries","text":"Before joining the Developer Induction, we kindly ask you to install binaries which are used for the deployment of applications in ACP - instructions on how to install these are shown below: Git Docker Drone Kubectl","title":"Required binaries"},{"location":"developer-docs/dev-setup.html#user-agreement","text":"Finally, please head over and read through our SLA documentation to familiarise yourself with the level of service ACP provides to its users including the level and hours of support on offer and issue escalation procedures.","title":"User agreement"},{"location":"how-to-docs/index.html","text":"How To's # This tree contains a collection of how-to guides for Developers. Create an Artifactory access token # Note: These instructions are intended for the ACP team. If you would like to request an Artifactory token, please use the relevant support request on the Platform Hub . The requester should state the name of the token, how they would like to receive the token and post their GPG key. Create an Artifactory access token using the following command: curl -u username : api-key -XPOST https://artifactory.digital.homeoffice.gov.uk/artifactory/api/security/token -d username= robot-username -d scope=member-of-groups: appropriate-groups -d expires_in=0 where robot-username is the name of the access token and appropriate-groups is a comma separated list of the groups the token should be in (normally this will only be ci ). Note: If you set the expires_in time higher than 0, you will not be able to revoke the token via the UI. Once the token has been created, JSON data should be returned which will include the access key. The JSON data you receive will be the only time you will be able to see the access key as it is not shown on Artifactory. You should, however, be able to see the name and expiry date (if you set an expiry time) of the access key in the \"Access Keys\" section. Kubernetes Pod Autoscaling # For full documentation on kubernetes autoscaling feature please go here . As of writing the ACP cluster supports standard autoscaling based on a CPU metric, there are however plans to support custom-metrics in the near future. Assuming you have a deployment 'web' and you wish to autoscale the deployment when it hit's a 40% CPU usage with min/max of 5/10 pods. apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: web spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: web metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 Sysdig Metrics - Experimental The autoscaler can also consume and make scaling decisions from sysdig metrics. Note, this feature is currently experimental but tested as working. An example of sysdig would be scaling on http_request apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: autoscaler spec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: myapplication minReplicas: 3 maxReplicas: 10 metrics: - type: Object object: target: kind: Service name: myservice metricName: net.http.request.count targetValue: 100 Certificates # Application Certificates Before reading about certificates and how you can create and manage them. Please familiarise yourself with our DNS naming convention first The platform provides two ways of managing HTTPS certificates : - Internal based certificates i.e. hostname.namespace.svc.cluster.local using CFSSL - External based certificates for external services i.e. service.homeoffice.gov.uk using kube-cert-manager and letsencrypt In most systems, it's likely that your service will have a user facing service, that will be served through an external endpoint i.e. it can be routed to externally by users as well as having non-external facing services i.e. internal services. You would want all communication between the user, through to the service and service dependencies to be encrypted, so that the traffic flow has encryption and that all endpoints trust who they are speaking to. Certificates kube-cert-manager and cfssl LetsEncrypt Limits Note Letsencrypt while a free service does come with a number of service limits detailed here . Probably one of the most crucial for projects is the max certificate requests per week; currently standing at 20. In addition, there is a max 5 failures for per hostname with a freeze of 1 hour, so if you accidently mess up configuration you might hit this. Chisel # The Problem : we want to provide services running in ACP access to the third party services as well as the ability to have user-based access controls. At present network access in ACP is provided via Calico, but this becomes redundant when the traffic egresses the cluster. Simply peering networks together either through VPC peering or VPN connections doesn't provide the controls we want. We could rely on user-authentication on third-party service but not all services are authenticated (take POISE) and beyond that peering networks provides no means of auditing traffic that is traversing the bridged networks. One pattern we are exploring is the use of a proxy cluster with an authenticated side-kick to route traffic and provide end-to-end encryption. Both ACP Notprod and Prod are peered to an respective proxy cluster that is running a Chisel server. Below is rough idea of how the chisel service works. The workflow for this is as follows, note the following example is assuming we have peered with a network in the proxy cluster which is exposing x services. A request via BAU the provisioning of a service on the Chisel server. Once done user is provided credentials for service. You add into your deployment a chisel container running in client mode and add the configuration as described to route the traffic. In regard to DNS and hostnames, kubernetes pods permit the user to add host entries into the container DNS, enabling you to override. The traffic is picked up, encrypted over an ssh tunnel and pushed to the Chisel server where the user credentials are evaluated. Assuming everything is ok the traffic is then proxied on to destination. A Working Example We have a two services called example-api.internal.homeoffice.gov.uk and another-service.example.com and we wish to consume the API from the pods. Lets assume the service has already been provisioned on the Chisel server and we have the credentials at hand. kind: Deployment metadata: name: consumer spec: replicas: 1 template: metadata: labels: name: consumer spec: hostAliases: - hostnames: - another-service.example.com - example-api.internal.homeoffice.gov.uk ip: 127.0.0.1 securityContext: fsGroup: 1000 volumes: - name: bundle configMap: name: bundle containers: - name: consumer image: quay.io/ukhomeofficedigital/someimage:someversion - name: chisel image: quay.io/ukhomeofficedigital/chisel:latest securityContext: runAsNonRoot: true env: # essentially user:password - name: AUTH valueFrom: secretKeyRef: name: chisel key: chisel.auth # this optional BUT recommended this is fingerprint for the SSH service - name: CHISEL_KEY valueFrom: secretKeyRef: name: chisel key: chisel.key args: - client - -v # this the chisel endpoint service hostname - gateway-internal.px.notprod.acp.homeoffice.gov.uk:443 # this is saying listen on port 10443 and route all traffic to another-service.example.com:443 endpoint - 127.0.0.1:10443:another-service.example.com:443 - 127.0.0.1:10444:example-api.internal.homeoffice.gov.uk:443 volumeMounts: - name: bundle mountPath: /etc/ssl/certs readOnly: true The above embeds the sidekick into the Pod and requests the client to listen on localhost:10443 and 10444 to redirect traffic via the Chisel service. The one annoying point here is the port requirements, placing things on different ports, but unfortunately this is required. You should be able to call the service via curl https://another-service.example.com:10443 at this point. Debug Issues with your deployments # Debug with secrets Sometimes your app doesn't want to talk to an API or a DB and you've stored the credentials or just the details of that in secret. The following approaches can be used to validate that your secret is set correctly $ kubectl exec -ti my-pod -c my-container -- mysql -h\\$DBHOST -u\\$DBUSER -p\\$DBPASS ## or $ kubectl exec -ti my-pod -c my-container -- openssl verify /secrets/certificate.pem ## or $ kubectl exec -ti my-pod -c my-container bash ## and you'll naturally have all the environment variables set and volumes mounted. ## however we recommend against outputing them to the console e.g. echo $DBHOST ## instead if you want to assert a variable is set correctly use $ [[ -z $DBHOST ]]; echo $? ## if it returns 1 then the variable is set. Debugging issues with your deployments to the platform If you get to the end of the above guide but can't access your application there are a number of places something could be going wrong. This section of the guide aims to give you some basic starting points for how to debug your application. Debugging deployments We suggest the following steps: 1. Check your deployment, replicaset and pods created properly $ kubectl get deployments $ kubectl get rs $ kubectl get pods 2. Investigate potential issues with your pods (this is most likely) If the get pods command shows that your pods aren't all running then this is likely where the issue is. You can then try curling your application to see if it is alive and responding as expected. e.g. $ curl localhost:4000 You can get further details on why the pods couldn't be deployed by running: $ kubectl describe pods *pods_name_here* If your pods are running you can check they are operating as expected by exec ing into them (this gets you a shell on one of your containers). $ kubectl exec -ti *pods_name_here* -c *container_name_here* /bin/sh Please note that the -c argument isn't needed if there is only one container in the pod.* 3. Investigate potential issues with your service A good way to do this is to run a container in your namespace with a bash terminal: $ kubectl run -ti --image quay.io/ukhomeofficedigital/centos-base debugger bash From this container you can then try curling your service. Your service will have a nice DNS name by default, so you can for example run: $ curl my-service-name 4. Investigate potential issues with ingress Minikube runs an ingress service using nginx. It's possible to ssh into the nginx container and cat the nginx.conf to inspect the configuration for nginx. In order to attach to the nginx container, you need to know the name of the container: $ kubectl get pods NAME READY STATUS RESTARTS AGE default-http-backend-2kodr 1/1 Running 1 5d acp-hello-world-3757754181-x1kdu 1/1 Running 2 6d ingress-3879072234-5f4uq 1/1 Running 2 5d You can attach to the running container with: $ kubectl exec -ti ingress-3879072234-5f4uq -c proxy bash where proxy is the container name of the nginx proxy inside the pod. You can find the name by describing the pod. You're inside the container. You can cat the nginx.conf with: $ cat /etc/nginx/nginx.conf You can also inspect the logs with: $ kubectl logs ingress-3879072234-5f4uq DMS Migration # Prerequisite The following need to be true before you follow this guide: AWS console logon Access to the DMS service from console * A region where STS has been activated DMS Setup Login to the AWS console using your auth, switch to a role with the correct access policies and verify you're in the right region. Next, select DMS from the services on the main dashboard to access the data migration home screen. Under the \"Get started\" section click on the \"create migration\" button then next to the Replication instance. You should see the following screen: The following are the options and example answers for the replication instance: Option Example answer Description Name dev-team-dms A name for the replication image. This name should be unique. Description DMS instance for migration Brief description of the instance Instance class dms.t2.medium The class of replication resource with the configuration you need for your migration. VPC vpc-* The virtual private cloud resource where you wish to add your dms instance. This should be as close to both the source and target instance as possible. Multi-AZ No Optional parameter to create a standby replica of your replication instance in another Availability Zone. Used for failover. Publicly Accessible False Option to access your instance from the internet You won't need to set any of the advanced settings. To create the instance click on the next button. You should now see a screen like this: The following are the options and example answers for the endpoints instances: Option Example answer Description Endpoint identifer database-source/target This is the name you use to identify the endpoint. Source/target engine postgres Choose the type of database engine that for this endpoint. Server name mysqlsrvinst.abcd123456789.us-west-1.rds.amazonaws.com Type of server name. For an on-premises database, this can be the IP address or the public hostname. For an Amazon RDS DB instance, this can be the endpoint for the DB instance. Port 5432 The port used by the database. SSL mode None SSL mode for encryption for your endpoints. Username root The user name with the permissions required to allow data migration. Password * * The password for the account with the required permissions. Database Name (target) dev-db The name of the attached database to the selected endpoint. Repeat these options for both source and target and make sure to test connection before clicking next. You might need to append security group rules to allow the replication instance access, for example: Replication instance has internal ip address 10.20.0.0 and the RDS is on port 5432 and uses TCP. Append rule Type Procol Port Range Source Custom TCP rule TCP 5432 Custom 10.20.0.0/32 Once this has fully been setup click next and you should be able to view the tasks page: The following are the options and example answers for these tasks: Option Example answer Description Task name Migration-task A name for the task. Task Description Task for migrating A description for the task. Source endpoint source-instance The source endpoint for migration. Target endpoint target-instance The target endpoint for migration. Replication instance replication-instance The replication instance to be used. Migration type Migrate existing data Migration method you want to use. Start task on create True When selected the task begins as soon as it is created. Target table preparation Drop table on target Migration strategy on target. Include LOB columns in replication Limited LOB mode Migration of large objects on target. Max LOB size 32 kb Maximum size of large objects. Enable logging False When selected migration events are logged. After completion the job will automatically run if \"start task on create\" has been selected. If not, the job can be started in the tasks section by selecting it and clicking on the \"Start/Resume\" button. Downscaling Services Out Of Hours # In an effort to reduce costs on running the platform, we've enabled to capability to scale down specific resources Out Of Hours (OOH) for Non-Production environments. AWS RDS (Relational Database Service) Non-Production RDS resources can be transitioned to a stopped state OOH to save on resource utilisation costs. This is currently managed with the use of tags on the RDS instance defining a cronjob schedule to stop and start the instance. To set a schedule for your RDS instances, please use the related Platform Hub support request template titled \"Shutdown RDS Instance(s) Out Of Hours\" . Note: Shutting down an RDS instance will have cost savings based on the instance size, however you will still be charged for the allocated storage. Kubernetes Pods Automatically scale down Kubernetes Deployments Statefulsets to 0 replicas during non-working hours for Non-Production or Production Environments. Downscaling for Deployments Statefulsets are managed by an annotation set within the manifest, and are processed every 30 seconds for changes, by a service running within the Kubernetes Clusters. Usage Set ONE of the following annotations on your Deployment / Statefulset: - downscaler/uptime : A time schedule in which the Deployment should be scaled up - downscaler/downtime : A time schedule in which the Deployment should be scaled down to 0 replicas The annotation values for the timeframe must have the following format to be processed correctly: WEEKDAY-FROM - WEEKDAY-TO-INCLUSIVE HH : MM - HH : MM TIMEZONE For example, to schedule a Deployment to only run on weekdays during working hours, the following annotation would be set: downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London Note: When the deployment is downscaled, an additional annotation downscaler/original-replicas is automatically set to retain a history of the desired replicas prior to the downscale action. If this annotation has been deleted before the service is automatically scaled back up, the downscaler service will not know what to set the replicas back to, and so it won't attempt to scale up the resource. Example Spec: apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London labels: name: example-app name: example-app namespace: acp-example spec: replicas: 2 template: spec: containers: image: docker.digital.homeoffice.gov.uk/acp-example-app:v0.0.1@sha256:07397c41ac25c4b19e0485006849201f04168703f0016fad75b8ba5d9885d6d4 ... Drone How To # Install Drone CLI Github drone instance: https://drone.acp.homeoffice.gov.uk/ Gitlab drone instance: https://drone-gitlab.acp.homeoffice.gov.uk/ Download and install the Drone CLI . At the time of writing, we are using version 0.8 of Drone. You can also install a release from Drone CLI's GitHub repo . Once you have downloaded the relevant file, extract it and move it to the /usr/local/bin directory. Verify it works as expected: $ drone --version drone version 0.8.0 Export the DRONE_SERVER and DRONE_TOKEN variables. You can find your token on Drone by clicking the icon in the top right corner and going to Token . export DRONE_SERVER=https://drone.acp.homeoffice.gov.uk export DRONE_TOKEN= your_drone_token If your installation is successful, you should be able to query the current Drone instance: $ drone info User: youruser Email: youremail@gmail.com If the output is bash Error: you must provide the Drone server address. or Error: you must provide your Drone access token. Please make sure that you have exported the DRONE_SERVER and DRONE_TOKEN variables properly. Activate your pipeline Once you are logged in to Drone, you will find a list of repos by clicking the icon in the top right corner and going to Repositories . Select the repo you want to activate. Navigate to your repository's settings in Github (or Gitlab) and you will see a webhook has been created. You need to update the url for the newly created web hook so that it matches this pattern: https://drone-external.acp.homeoffice.gov.uk/hook?access_token=some_token If it is already in that format there is no need to change anything. The token in the payload url will not be the same as the personal token that you exported and it should be left unchanged. Please note that this does not apply to Gitlab. When you activate the repo in Drone, you should not change anything for a Gitlab repo. Configure your pipeline In the root folder of your project, create a .drone.yml file with the following content: pipeline: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t image_name . when: branch: master event: push Commit and push your changes: $ git add .drone.yml $ git commit $ git push origin master Please note you should replace the name ... with the name of your app. You should be able to watch your build succeed in the Drone UI. Publishing Docker images Publishing to Quay If your repository is hosted on Gitlab, you don't want to publish your images to Quay. Images published to Quay are public and can be inspected and downloaded by anyone. You should publish your private images to Artifactory . Register for a free Quay account using your Github account linked to the Home Office organisation. Once you've logged into Quay check that you have ukhomeofficedigital under Users and Organisations. If you do not, submit a support request on the platform hub for access to the ukhomeoffice organisation . Once you have access to view the ukhomeofficedigital repositories, click repositories and click the + Create New Repositories that is: public empty - no need to create a repo from a Dockerfile or link it to an existing repository Add your project to the UKHomeOffice Quay account and submit a support request on the platform hub for a new Quay robot . Add the step to publish the docker image to Quay in your Drone pipeline: image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+ your_robot_username registry: quay.io repo: quay.io/ukhomeofficedigital/ your_quay_repo tags: - ${DRONE_COMMIT_SHA} - latest when: branch: master event: push Where your_quay_repo in: quay.io/ukhomeofficedigital/ your_quay_repo is the name of the Quay repo you (should) have already created. Note: ${DRONE_COMMIT_SHA} is a Drone environment variable that is passed to the container at runtime. The build should fail with the following error: Error response from daemon: Get https://quay.io/v2/: unauthorized: Could not find robot with username: ukhomeofficedigital+ your_robot_username and supplied password. The error points to the missing password for the Quay robot. You will need to add this as a drone secret. You can do this through the Drone UI by going to your repo, clicking the menu icon in the top right and then clicking Secrets . You should be presented with a list of the secrets for that repo (if there are any) and you should be able to add secrets giving them a name and value. Add a secret with the name DOCKER_PASSWORD and with the value being the robot token that was supplied to you. Alternatively, you can use the Drone CLI to add the secret: $ drone secret add --repository ukhomeoffice/ your_github_repo --name DOCKER_PASSWORD --value your_robot_token Restarting the build should be enough to make it pass. The Drone CLI allows for more control over the secret as opposed to the UI. For example, the CLI allows you to specify the image and the events that the secret will be allowed to be used with. Also note that the secret was specified in the secrets section of the pipeline to give it access to the secret. Without this, the pipeline would not be able to use the secret and it would fail. Secrets in this section are automatically uppercased at runtime so it is important that the secret is uppercased in your commands. You can also push specifically tagged images by using the DRONE_TAG Drone environment variable and by using the tag event: tagged_image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+ your_robot_username registry: quay.io repo: quay.io/ukhomeofficedigital/ your_quay_repo tags: - ${DRONE_TAG} when: event: tag Tag using git tag v1.0 and push your tag with git push origin v1.0 (replace v1.0 with the tag you actually want to use). Note: These pipeline configurations are using the Docker plugin for Drone. For more information, see http://plugins.drone.io/drone-plugins/drone-docker/ Publishing to Artifactory Images hosted on Artifactory are private. If your repository is hosted publicly on GitHub, you shouldn't publish your images to Artifactory. Artifactory is only used to publish private images. You should use Quay to publish your public images . Submit a support request for a new Artifactory access token . You should be supplied an access token in response. You can inject the token that has been supplied to you with: $ drone secret add --repository gitlab_repo_group / your_gitlab_repo --name DOCKER_PASSWORD --value your_robot_token You can add the following step in your .drone.yml : image_to_artifactory: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME= your_robots_username registry: docker.digital.homeoffice.gov.uk repo: docker.digital.homeoffice.gov.uk/ your_artifactory_repo tags: - ${DRONE_COMMIT_SHA} - latest when: branch: master event: push Where the image_name in: docker tag image_name docker.digital.homeoffice.gov.uk/ukhomeofficedigital/ your_artifactory_repo :$${DRONE_COMMIT_SHA} is the name of the image you tagged previously in the build step. The image should now be published on Artifactory. Deployments Deployments and promotions Create a step that runs only on deployments: deploy-to-preprod: image: busybox commands: - /bin/echo hello preprod when: environment: preprod event: deployment Push the changes to your remote repository. You can deploy the build you just pushed with the following command: $ drone deploy ukhomeoffice/ your_repo 16 preprod Where 16 is the successful build number on drone that you wish to deploy to the preprod environment. You can pass additional parameters to your deployment as environment variables: $ drone deploy ukhomeoffice/ your_repo 16 preprod -p DEBUG=1 -p NAME=Dan and use them in the step like this: deploy-to-preprod: image: busybox commands: - /bin/echo hello $${NAME} when: environment: preprod event: deployment Environments are strings and can be set to any value. When you wish to deploy to several environments you can create a step for each one of them: deploy-to-preprod: image: busybox commands: - /bin/echo hello preprod when: environment: preprod event: deployment deploy-to-prod: image: busybox commands: - /bin/echo hello prod when: environment: prod event: deployment And deploy them accordingly: $ drone deploy ukhomeoffice/ your_repo 16 preprod $ drone deploy ukhomeoffice/ your_repo 16 prod Read more on environments . Drone as a Pull Request builder Drone pipelines are triggered when events occurs. Event triggers can be as simple as a push , a tagged commit , a pull request or as granular as only for pull requests for a branch named test . You can limit the execution of build steps at runtime using the when block. As an example, this block executes only on pull requests: pr-builder: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t image_name . when: event: pull_request Drone will only execute that step when a new pull request is raised (and when pushes are made to the branch while a pull request is open). Read more about Drone conditions . Deploying to ACP Please note that this section assumes that you already have kube files to work with (specifically, deployment, service and ingress files). Examples of these files can be found in the kube-signed-commit-check project. Add a deployment script with the following: #!/bin/bash export KUBE_NAMESPACE= dev-induction export KUBE_SERVER=${KUBE_SERVER} export KUBE_TOKEN=${KUBE_TOKEN} kd -f deployment.yaml \\ -f service.yaml \\ -f ingress.yaml Please note that this is only an example script and it will need to be changed to fit your particular application's needs. If you deployed this now you would likely receive an error similar to this: error: You must be logged in to the server (the server has asked for the client to provide credentials) This error appears because kd needs 3 environment variables to be set before deploying: KUBE_NAMESPACE - The kubernetes namespace you wish to deploy to. You need to provide the kubernetes namespace as part of the deployment job . KUBE_TOKEN - This is the token used to authenticate against the kubernetes cluster. If you do not already have a kube token, here are docs explaining how to get one . KUBE_SERVER - This is the address of the kubernetes cluster that you want to deploy to. You will need to add KUBE_TOKEN and KUBE_SERVER as drone secrets. Information about how to add Drone secrets can be found in the publishing to Quay section . You can verify that the secrets for your repo are present with: $ drone secret ls --repository ukhomeoffice/ your-repo Once the secrets have been added, add a new step to your drone pipeline that will execute the deployment script: deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - ./deploy.sh when: environment: uat event: deployment Using Another Repo It is possible to access files or deployment scripts from another repo, there are two ways of doing this. The recommended method is to clone another repo in the current repo (since this only requires maintaining one .drone.yml) using the following step: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/ your_repo .git when: environment: uat event: deployment Your repository is saved in the workspace, which in turn is shared among all steps in the pipeline. However, if you decide that you want to trigger a completely different pipeline on a separate repository, you can leverage the drone-trigger plugin. If you have a secondary repository, you can setup Drone on that repository like so: pipeline: deploy_to_uat: image: busybox commands: - echo ${SHA} when: event: deployment environment: uat Once you are ready, you can push the changes to the remote repository. In your main repository you can add the following step: trigger_deploy: image: quay.io/ukhomeofficedigital/drone-trigger:latest drone_server: https://drone.acp.homeoffice.gov.uk repo: UKHomeOffice/ deployment_repo branch: master deploy_to: uat params: SHA=${DRONE_COMMIT_SHA} when: event: deployment environment: uat The settings are very similar to the drone deploy command: deploy_to is the environment constraint params is a list of comma separated list of arguments. In the command line tool, this is equivalent to -p PARAM1=ONE -p PARAM2=TWO repo the repository where the deployment scripts are located The next time you trigger a deployment on the main repository with: $ drone deploy UKHomeOffice/ your_repo 16 uat This will trigger a new deployment on the second repository. Please note that in this scenario you need to inspect 2 builds on 2 separate repositories if you just want to inspect the logs. Versioned deployments When you restart your build, Drone will automatically use the latest version of the code. However always using the latest version of the deployment configuration can cause major issues and isn't recommended. For example when promoting from preprod to prod you want to use the preprod version of the deployment configuration. If you use the latest it could potentially break your production environment, especially as it won't necessarily have been tested. To counteract this you should use a specific version of your deployment scripts. In fact, you should git checkout the tag or sha as part of your deployment step. Here is an example of this: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/ your_repo .git when: environment: uat event: deployment deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - apk update apk add git - git checkout v1.1 - ./deploy.sh when: environment: uat event: deployment Migrating your pipeline Secrets and Signing It is no longer necessary to sign your .drone.yml so the .drone.yml.sig can be deleted. Secrets can be defined in the Drone UI or using the CLI. Secrets created using the UI will be available to push, tag and deployment events. To restrict to selected events, or to allow pull request builds to access secrets you must use the CLI. Pipelines by default do not have access to any Drone secrets that you have added. You must now define which secrets a pipeline is allowed access to in a secrets section in your pipeline. Here is an example of a pipeline that has access to the DOCKER_PASSWORD secret which will be used to push an image to Quay: image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+ your_robot_username registry: quay.io repo: quay.io/ukhomeofficedigital/ your_quay_repo tags: - latest when: branch: master event: push Note: Secrets names in the secrets section will have their names uppercased at runtime. Organisation secrets are no longer available. This means that if you are using any organisation secrets such as KUBE_TOKEN_DEV , you will need to add a secret in Drone to replace it. Docker-in-Docker The Docker-in-Docker (dind) service is no longer required. Instead, change the Docker host to DOCKER_HOST=tcp://172.17.0.1:2375 in the environment section of your pipline, and you will be able to access the shared Docker server on the drone agent. Note that it is only possible to run one Docker build at a time per Drone agent. Since privileged mode was primarily used for docker in docker, you should remove the privileged: true line from your .drone.yml . You can also use your freshly built image directly and run commands as part of your pipeline. Example: pipeline: build_image: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t hello_world . when: branch: master event: push test_image: image: hello_world commands: - ./run-hello-world.sh when: branch: master event: push Services If you use the services section of your .drone.yml it is possible to reference them using the DNS name of the service. For example, if using the following section: services: database: image: mysql The mysql server would be available on tcp://database:3306 Variable Escaping Any Drone variables (secrets and environment variables) must now be escaped by having two $$ instead of one. Examples: ${DOCKER_PASSWORD} -- $${DOCKER_PASSWORD} ${DRONE_TAG} -- $${DRONE_TAG} ${DRONE_COMMIT_SHA} -- $${DRONE_COMMIT_SHA} Scanning Images in Drone # ACP provides Anchore as scanning solution to images built into the Drone pipeline, allowing users to scan both ephemeral (built within the context of the drone, but not pushed to a repository yet) as well and well any public images. pipeline: build: image: docker:17.09.0-ce environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t docker.digital.homeoffice.gov.uk/myimage:$${DRONE_BUILD_NUMBER} . scan: # The location of the drone plugin image: quay.io/ukhomeofficedigital/anchore-submission:latest # The optional path of a Dockerfile dockerfile: Dockerfile # Note the lack of double $ here (due to the way drone injects variables image_name: docker.digital.homeoffice.gov.uk/myimage:${DRONE_BUILD_NUMBER} # This indicates we are willing tolerate any vulnerabilities which are below medium tolarates: medium # An optional whitelist (comman separated list of CVE's) whitelist: CVE_SOMENAME_1,CVE_SOMENAME_2 # An optional whitelist file containing a list of CSV relative to the repo path whitelist_file: PATH # By default the pligin will exit will fail if any vulnerabilities are discovered which are not tolarated, # you change this behaviour by setting the bellow fail_on_detection: false Q As Q: The build fails with \"ERROR: Insufficient privileges to use privileged mode\" A: Remove privileged: true from your .drone.yml . As explained in the migrating your pipeline section , the primary use of this was for Docker-in-Docker which is not required. Q: The build fails with \"Cannot connect to the Docker daemon. Is the docker daemon running on this host?\" A: Make sure that your steps contain the environment variable DOCKER_HOST=tcp://172.17.0.1:2375 like in this case: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t image_name . when: branch: master event: push Q: The build fails when uploading to Quay with the error \"Error response from daemon: Get https://quay.io/v2/: unauthorized:...\" A: This is likely because the secret wasn't added correctly or the password is incorrect. Check that the secret has been added to Drone and that you have added the secrets section in your .drone.yaml it to the pipeline that requires it. Q: As part of my build process I have two Dockerfiles to produce a Docker image. How can I share files between builds in the same step? A: When the pipeline starts, Drone creates a Docker data volume that is passed along all active steps in the pipeline. If the first step creates a test.txt file, the second step can use that file. As an example, this pipeline uses a two step build process: pipeline: first-step: image: busybox commands: - echo hello test.txt when: branch: master event: push second-step: image: busybox commands: - cat test.txt when: branch: master event: push Q: Should I use Gitlab with Quay? A: Please don't. If your repository is hosted in Gitlab then use Artifactory to publish your images. Images published to Artifactory are kept private. If you still want to use Quay, you should consider hosting your repository on the open (Github). Q: Can I create a token that has permission to create ephemeral/temporary namespaces? A: No. This is because there is currently no way to give access to namespaces via regex. I.e. There is no way to give access to any namespace with the format: my-temp-namespace-* (where * would be build number or something similar). Alternatively, you can be given a named namespace in the CI cluster. Please create an issue on our BAU board if you require this. AWS ECR for Private Docker Images # AWS ECR (Elastic Container Registry) is now available as a self-service feature via the Platform Hub. Each project has the capability to create their own Docker Repositories and define individual access to each via the use of IAM Credentials. Creating a Docker Repository Anybody that is part of a Project within the Platform Hub will have the ability to create a new Docker Repository. Login to the Platform Hub via https://hub.acp.homeoffice.gov.uk Navigate to the Projects list: https://hub.acp.homeoffice.gov.uk/projects/list Select your Project from the list to go to the detail page (e.g. https://hub.acp.homeoffice.gov.uk/projects/detail/acp) Ensure you have a Service defined within your Project for the Docker Repository to be associated with (check under the SERVICES tab) Select the ALL DOCKER REPOS tab Select the REQUEST NEW DOCKER REPO button Choose the Service to associate this Repository with and provide the name of the Repository to be created (e.g. hello-world-app ) The request to create a new Docker Repository can take a few seconds to complete. You can view the status of a Repository by navigating to the ALL DOCKER REPOS tab and viewing the list. Once the request has completed, your Repository should have the Active label associated with it. This repository won't automatically refresh, but you can hit the REFRESH button above the Repository list or just manually refresh your browser window for updates. Generating Access Credentials Access to ECR Repositories is managed via AWS IAM. These IAM credentials are generated via the Platform Hub and access can be managed per user, per Docker Repository. Navigate to the ALL DOCKER REPOS tab for your Project within the Platform Hub For the Repository you have created, select the MANAGE ACCESS button At this stage, you can: Create a Robot Account(s), which can be used in deployment pipelines in Drone CI for publishing new images to AWS ECR Select which Project Members have the ability to pull images, and additionally push updates using their own IAM credentials (separate to the Robot Account(s) and CI builds) For this example, select your own User and press Save . Note: Generally users should never be granted write access, as any write actions should be performed via CI (using the Robot Accounts). Press the REFRESH button at the top of the page and check the User Access has a status of active Robot Accounts are visible under the Docker Repository, and once they reveal an active status the IAM Credentials are displayed alongside it. Accessing a Docker Repository Accessing the AWS Container Registry to Pull Push images is currently a two-step process: 1. Use IAM Credentials to generate a temporary authorisation token 1. Use the temporary authorisation token to authenticate your docker client with ECR Note: The authorisation token generated for docker login is only valid for 12 hours, and so the process above will need to be repeated. Pre-Requisites To follow the below steps you must have: AWS CLI (version 1.11.91 or above, check with aws --version ) * Install Guides: Linux , OSX , Windows Docker (version 17.06 or above, check with docker --version ) Step 1: Retrieve an authorisation token Navigate to the Connected Identities page: https://hub.acp.homeoffice.gov.uk/identities Under Amazon ECR you will have access to your own personal IAM Credentials. These credentials will work across multiple projects whose Repositories you have been granted access to. With the AWS IAM Credentials retrieved from the Connected Identities page, setup a local IAM Profile via the Terminal: $ aws configure --profile acp-ecr AWS Access Key ID [None]: XXXXXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Default region name [None]: eu-west-2 Default output format [None]: json $ export AWS_PROFILE=acp-ecr Now, using the aws-cli you can request an authorisation token to perform a docker login: $ aws ecr get-login --no-include-email docker login -u AWS -p long-auth-token https://340268328991.dkr.ecr.eu-west-2.amazonaws.com Step 2: Login with Authorisation Token Following a successful ecr get-login , a full docker login command should be returned. Copy and paste the command exactly, to login to the ECR endpoint: $ docker login -u AWS -p long-auth-token https://340268328991.dkr.ecr.eu-west-2.amazonaws.com WARNING! Using --password via the CLI is insecure. Use --password-stdin. Login Succeeded Note: If you get an error from Step 1 such as Unknown options: --no-include-email , your aws-cli client needs updating. You can omit --no-include-email rather than updating your aws-cli client, but the resulting docker login command will include a deprecated -e none flag (needs to be removed prior to running the command). Pulling Pushing Images Within the ACP Kubernetes Clusters, you do not need to provide an imagePullSecret as was previously required for images in Artifactory. The ACP Clusters will authenticate behind-the-scenes and be able to successfully pull images from any Docker Repositories you create via the Platform Hub. The Docker Repositories section of the Platform Hub will provide a URL such as follows for the Repository you have created: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app Now that you have locally authenticated with AWS ECR, you can pull and push (if write access was granted) images as normal: $ docker build . -t 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 Sending build context to Docker daemon 32.78MB ... Successfully built 882e2cadb649 Successfully tagged 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 $ docker push 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 The push refers to repository [340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app] afbe4b47c182: Pushed 78147c906fce: Pushed 86177d14466d: Pushed f55514f6bd18: Pushed ce74984572d7: Pushed 67d7e5db87ee: Pushed 12d012372115: Pushed b0bb54920d03: Pushed 835c2760f26b: Pushed e9bcacee1741: Pushed cd7100a72410: Pushed v0.0.1: digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f size: 2628 $ docker pull 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f: Pulling from acp/hello-world-app Digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Status: Image is up to date for 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Managing Image Deployments via Drone CI The Docker Authorisation Token generated via the aws-cli command is only valid for 12 hours, and so this can't be used as a Drone Secret for Docker Image builds. Instead, you would need to store the IAM Credentials for a Robot Account as Drone Secrets and perform the aws ecr get-login + docker login .. step on each build. To simplify this process you can use a custom Drone ECR plugin, which: - Builds a docker image in the root repository directory, with custom build arguments passed in (optional) - Authenticates to ECR using your AWS IAM credentials (stored as Drone Secrets) - Pushes the image to ECR with the given tags in the list (latest and commit sha) Example Pipeline: pipeline: build_push_to_ecr: image: quay.io/ukhomeofficedigital/ecr:latest secrets: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY repo: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app build_args: - APP_BUILD=${DRONE_COMMIT_SHA} tags: - latest - ${DRONE_COMMIT_SHA} The UKHomeOffice ECR image above is based off the official Docker ECR Plugin , with amendments to run in ACP Drone CI. Using Ingress # An Ingress is a type of Kubernetes resource that allows you to expose your services outside the cluster. It gets deployed and managed exactly like other Kube resources. Our ingress setup offers two different ingresses based on how restrictively you want to expose your services: - internal - only people within the VPN can access services - external - anyone with internet access can access services The annotation kubernetes.io/ingress.class: \"nginx-internal\" is used to specify whether the ingress is internal. ( kubernetes.io/ingress.class: \"nginx-external\" is used for an external ingress.) In the following example the terms \"myapp\" and \"myproject\" have been used, these will need to be changed to the relevant names for your project. Where internal is used, this can be changed for an external ingress - everything else stays the same. apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # used to select which ingress this resource should be configured on kubernetes.io/ingress.class: nginx-internal # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated. ingress.kubernetes.io/secure-backends: true # This replaces the old annotation secure-backends ingress.kubernetes.io/backend-protocol: HTTPS name: myapp-server-internal spec: rules: - host: myapp.myproject.homeoffice.gov.uk http: paths: - backend: serviceName: myapp servicePort: 8000 path: / tls: - hosts: - myapp.myproject.homeoffice.gov.uk # the name of the kubernetes secret in your namespace with tls.crt and tls.key secretName: myapp-github-internal-tls Always ensure you are using TLS between the ingress controller and your pods by placing the annotation: ingress.kubernetes.io/backend-protocol: \"HTTPS\". At the moment; this is not enforced though there are plans to enforce by policy at a later date. Kube-Cert-Managaer Cloudflare SSL # Services: - kube-cert-manager is used to retrieve Letencrypt certificates. - cfssl is an internal certificate service used to provide internal tls between pods / services. Domains and Challenge types At present two Let's Encrypt challenge types are supported for certificates which is controlled via the stable.k8s.psg.io/kcm.provider annotation on the Ingress resource; note if no annotation is present the default is stable.k8s.psg.io/kcm.provider: route53 . route53: the domain must be hosted within the ACP route53 account, namely to allow kube-cert-manager to add the service record. If you are unsure if this is the case please check with the ACP team. DNS is optional for external sites but a requirement for sites seated behind the VPN. http: indicates a callback url for authentication. The domain can either be controlled externally via yourself or via the ACP team. Either way the dns record must be a CNAME to the external ingress hostname (please check with the ACP team if you dont know) . Obviously this challenge type can only be used on an external site. Note any IP white-listing on the ingress can still be used. As a developer I want to grab a certificate from Letsencrypt Below is an example of an ingress resource for a HTTPS host reachable over the internet. Note, your ingress resource using IP whitelisting is irrelivant here in regard to Letsencrypt i.e you can still protect the site with and ACL. The host my-app.my-project.homeoffice.gov.uk has a CNAME record to ingress-external.prod.acp.homeoffice.gov.uk (or relevant ingress-external address for the cluster you are using) apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated. ingress.kubernetes.io/secure-backends: true # This replaces the old annotation secure-backends ingress.kubernetes.io/backend-protocol: HTTPS kubernetes.io/ingress.class: nginx-external # ensure kube-cert-manager uses a http01 challenge stable.k8s.psg.io/kcm.provider: http labels: # this is a toggle to indicate kube-cert-manager should handle this resource stable.k8s.psg.io/kcm.class: default name: my-app spec: rules: - host: my-app.my-project.homeoffice.gov.uk http: paths: - backend: serviceName: my-app servicePort: 443 path: / tls: - hosts: - my-app.my-project.homeoffice.gov.uk secretName: my-app-external-tls As a developer I want a certificate from a site behind the vpn Using Letsencrypt Below is an example of an ingress resource for a HTTPS host reachable via VPN or services on the private network. The host my-app.my-project.homeoffice.gov.uk has a CNAME record to ingress-internal.prod.acp.homeoffice.gov.uk (or relevant ingress-internal address for the cluster you are using) The host my-app.my-project.homeoffice.gov.uk is managed via Route53 within the same account that the cluster is running within (done by ACP team) The host my-app.my-project.homeoffice.gov.uk (or TLD) is listed as a Hosted Domain within the custom policy admission controller (done by ACP team) apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated. ingress.kubernetes.io/secure-backends: true # This replaces the old annotation secure-backends ingress.kubernetes.io/backend-protocol: HTTPS kubernetes.io/ingress.class: nginx-internal labels: stable.k8s.psg.io/kcm.class: default name: my-app spec: rules: - host: my-app.my-project.homeoffice.gov.uk http: paths: - backend: serviceName: my-app servicePort: 443 path: / tls: - hosts: - my-app.my-project.homeoffice.gov.uk secretName: my-app-internal-tls Using LetsEncrypt with Ingress Assuming you are not bringing your own certificates, LetsEncrypt can be used to acquire certificates for both internal (behind vpn NOT cluster TLS certs) and external certificates. Simply place the annotation stable.k8s.psg.io/kcm.class: default into the ingress resource; A full list of the supported features can be found here . Note at present we ONLY allow you to request certificates via the ingress resource, not by the third party resource. As a developer I want a certificate for my pod / service The CA bundle By default, in all namespaces a CA bundle has been added which can been mounted into the /etc/ssl/certs of the container and which contains the root CA used to verify authenticity of the certificates. An example of using it is given below. Below is example of how to acquire a certificate from CloudflareSSL. apiVersion: extensions/v1beta1 kind: Deployment metadata: name: example spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: example spec: volumes: - name: bundle configMap: name: bundle - name: certs emptyDir: {} initContainers: - name: certs # PLEASE do not use latest, but check for the latest tag in the releases page of https://github.com/UKHomeOffice/cfssl-sidekick image: quay.io/ukhomeofficedigital/cfssl-sidekick:latest securityContext: runAsNonRoot: true args: - --certs=/certs - --domain=myservice.${KUBE_NAMESPACE}.svc.cluster.local - --domain=another_domain_name - --expiry=8760h env: - name: KUBE_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: # an emptyDir which the sidekicks writes the certificates - name: certs mountPath: /certs # The platform CA Bundle hold the root ca used to verify the certificate chain - name: bundle mountPath: /etc/ssl/certs readOnly: true containers: - name: your_application image: quay.io/ukhomeofficedigital/some_image ... ports: - name: https port: 443 targetPort: 443 volumeMounts: # You can configure your application to pick up the certificates from here (tls.pem and tls-key.pem) - name: certs mountPath: /certs readOnly: true To break down what is happening. Firstly we are adding two volumes bundle and certs . the bundle volume is mapped to a configmap which as indicated above is published by us into every namespace and contains the a certificate bundle. This is mounted into the default PKI directory of the container /etc/ssl/certs and permits the container to trust the service. the certs is a emptyDir which is a tmpfs volume and used to share the cecertificates between the sidekick and your application. We then inject into the initContainers the sidekick service. The sidekick is responsible for locally generating a private key (the private key itself never crosses the wire) generating a CSR for the certificate and requesting a signing from cloudflare service. once the certificate has been signed its placed into --certs=dir directory which in this case is the emptyDir shared across the containers. Note, if you wish to trust certificates generated by this service simply mounted the bundle into the certificates dorectory. Getting a Kubernetes Robot Token # Users Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab. Any robot tokens that have been created for that service will be listed. You can see the full token by clicking on the eye icon next to the token. If there are no robot tokens for that service, or the required one is not there, you will need to ask your project admin(s) to create a robot token. Project Admins (Creating a robot token) Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab and click the Create a Kubernetes robot token for this service button. Select the required cluster, RBAC group(s), robot name and description for the robot token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Users who are part of the project will be able to view the token in the same place you created it (Project - Service - Kube Robot Tokens). Getting a Kubernetes Token # Users Log into the Platform Hub . Go to the Projects section and find your project. On the Overview People tab, you should see a list of team members and the project admin (who will have the admin tag next to their name). Talk to your project admin and ask them to generate a user token for you. Once your token has been created, you will be able to find it in the Connected Identities section. You will need to expand the Kubernetes identity and show your full token by clicking the eye icon next to it. Project Admins (Creating a user token) Log into the Platform Hub . Go to the Projects section and find your project. Click on the Kube User Tokens tab, click Select a project team member and select the requesters name from the list. Click CREATE A NEW KUBERNETES USER TOKEN FOR THIS USER . Select the required cluster and RBAC group(s) needed for the token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Once the token is created the requester should be able to see it in their Connected Identities section for use in their Kube config. Note: Tokens can take a while to propagate so you may have to wait for up to 10 minutes before using a new token. Network Policies # By default a deny-all policy is applied to every namespace in each cluster. You can however add network policies to your own projects to allow for certain connections and these will be applied on top of the default deny-all policy. Here is an example network policy for allowing a connection from the ingress-internal namespace: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-network-policy namespace: your-namespace-here spec: podSelector: matchLabels: role: artifactory ingress: - from: - namespaceSelector: matchLabels: name: ingress-internal ports: - protocol: TCP port: 443 The port number should be the same as the one that your service is listening on. Controlling Egress Traffic Kubernetes v1.8 with Calico v2.6 adds support to limit egress traffic via the use of Kubernetes Network Policies. An example of a policy document blocking ALL egress traffic for a given namespace is below: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-egress namespace: your-namespace-here spec: podSelector: matchLabels: {} policyTypes: - Egress NOTE: The above document will also prevent DNS access for all pods in the namespace. To allow DNS egress traffic via the kube-system namespace, you can apply the following Network Policy document within your namespace (which takes precedence over deny-all-egress ): apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns-access namespace: your-namespace-here spec: podSelector: matchLabels: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: name: kube-system ports: - protocol: UDP port: 53 For more information, please see the following: - Kubernetes documentation on network policies - Kubernetes advanced network policy examples Run Performance Tests on a service hosted on ACP # As a Service, I should: Always have a baseline set of metrics of my isolated service Understand what those metrics need to be for each functionality i.e. how long file uploads should take vs a generic GET request Make sure the baseline does not include any other components i.e. networks, infrastructure etc. Expose a set of metrics, see Metrics Make performance testing part of my Continuous Integration workflow Have a history of performance over time Assessed tools summary: An example usage of Blazemeter's Taurus in a drone pipeline can be seen in the taurus-project-x repo . Artillery ( npm ) was also tested w/ the statsd plugin , visualising data in grafana. SonarQube plugin jmeter-sonar is now deprecated. The latest version of sonarqube does not to have plugin support for jmeter another option is k6 - tool is written in go and tests are written in javascript. To visualise the only option is InfluxDB and Grafana. Pod Security Policies # By default all user deployments will inherit a default PodSecurityPolicy applied in the Kubernetes Clusters, which define a set of conditions that a pod must be configured with in order to run successfully. The specification for the default policy is as follows: apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: default spec: privileged: false fsGroup: rule: RunAsAny hostPID: false hostIPC: false hostNetwork: false runAsUser: rule: MustRunAsNonRoot seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - downwardAPI - emptyDir - gitRepo - persistentVolumeClaim - projected - secret runAsUser This condition requires that the pod specification deploys an image with a non-root user. The user defined in the specification (image spec OR pod spec) must be numeric, so that Kubernetes will be able to verify that it is a non-root user. If this is not done, you may receive any of the following errors in your event log and your pod will be prevented from starting up successfully: - container's runAsUser breaks non-root policy - container has runAsNonRoot and image will run as root - container has runAsNonRoot and image has non-numeric user username , cannot verify user is non-root Note: You can view all recent events in your namespace by running the following command: kubectl -n my-namespace get events --sort-by=.metadata.creationTimestamp . To update your deployment accordingly for the above condition, there are multiple ways to achieve this: Dockerfile Within the Dockerfile for the image you are attempting to run, ensure the USER specified references the User ID rather than the username itself. For example: FROM quay.io/gambol99/keycloak-proxy:v2.1.1 LABEL maintainer= rohith.jayawardene@digital.homeoffice.gov.uk RUN adduser -D -u 1000 keycloak USER 1000 Note: The following common images have been updated to reference the UID within their respective Dockerfiles. If you use any of these images, updating your deployments to use these versions (or any newer versions) will meet the MustRunAsNonRoot requirement for this particular container: quay.io/ukhomeofficedigital/cfssl-sidekick:v0.0.6 quay.io/ukhomeofficedigital/elasticsearch:v1.5.3 quay.io/ukhomeofficedigital/jira:v7.9.1 quay.io/ukhomeofficedigital/keycloak:v3.4.3-2 quay.io/ukhomeofficedigital/kibana:v0.4.4 quay.io/ukhomeofficedigital/go-keycloak-proxy:v2.1.1 quay.io/ukhomeofficedigital/nginx-proxy:v3.2.9 quay.io/ukhomeofficedigital/nginx-proxy-govuk:v3.2.9.0 quay.io/ukhomeofficedigital/redis:v0.1.2 quay.io/ukhomeofficedigital/squidproxy:v0.0.5 Deployment Spec In the securityContext section of your deployment spec, the runAsUser field can be used to set a UID that the image should be run as. An example spec would include: spec: securityContext: fsGroup: 1000 runAsNonRoot: true runAsUser: 1000 containers: - name: {{ .IMAGE_NAME }} image: {{ .IMAGE }}:{{ .VERSION }} ... Using artifactory as a private npm registry # A step-by-step guide. This guide makes the following assumptions: you have drone ci set up for your project already you are using node@8 and npm@5 or later you are connected to ACP VPN Setting up a local environment Get your username and API key from artifactory Visit https://artifactory.digital.homeoffice.gov.uk/artifactory/webapp/#/profile, make a note of your username, and if you don't already have an API key then generate one. base64 encode your API key echo -n api key | base64 Set local environment variables Copy your encoded password, and set the following environment variables in your bash profile: export NPM_AUTH_USERNAME= username export NPM_AUTH_TOKEN= base64 encoded api key You might then need to source your profile to load these environment variables. Setting up CI in drone Request a bot token for artifactory You can do this through the ACP Hub . You'll need to provide a username for the bot when you create it. One of the ACP team will create a token and send it to you as an encrypted gpg file via email. Decrypt the token gpg --decrypt ./path/to/file.gpg Add the token to drone as a secret First, base64 encode the token: echo -n token | base64 Then add this token to drone as a secret: drone secret add UKHomeOffice/ repo NPM_AUTH_TOKEN base64-encoded-token --event pull_request Note: You will need to make sure the event types are lowercase. If an event is capitalised, it won't match the standard events inside of drone Note: you will need to make the secret available to pull request builds to be able to run npm commands in pull request steps Expose secret to build steps You will need to configure any steps which use npm to be able to access the secret. Do this by adding a secret property to those steps as follows: my_step: image: node:8 secrets: - npm_auth_token commands: - npm install - npm test Expose username to build steps In addition, you will need to add the username (as you provided when creating your token) as an environment variable. The easiest way to do this is as a \"matrix\" variable, which makes the username available to all steps without needing to configure them all individually. matrix: NPM_AUTH_USERNAME: - username Publishing modules to artifactory It is generally recommended to use a common namespace to publish your modules under. npm allows namespace specific configuration, which makes it easier to ensure that modules are always installed from artifactory, and will not accidentally try to install a public module with the same name. Setting publish registry Add publishConfig to package.json. This ensures that the module can only ever be published to the private registry, and misconfiguration won't accidentally make it public publishConfig : { registry : https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/ } Add auth settings In your project's .npmrc file (create one if it does not already exist) add the following lines: //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:email=test@example.com The email address can be anything, it just needs to be set. Add publish step to drone Add the following step to your .drone.yml file to publish a new version whenever you release a tag. publish: image: node:8 secrets: - npm_auth_token commands: - npm publish when: event: tag Now, when you push new tags to github then drone should publish them to the artifactory npm registry automatically. Using modules from artifactory as dependencies Configure your project to use artifactory In the project which is has private modules as dependencies, add the following line to .npmrc in the root of the project (create this file if it does not exist). @ namespace :registry = https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/ This will ensure that any module under that namespace will only ever install from artifactory, and never from the public registry If using multiple namespaces then add a line for each namespace. If the modules you are installing are not namespaced in artifactory, you can add the line with the namespace removed (i.e. registry = ... ) but this will have a negative impact on install speed. You should then add the following line to your project's .npmrc if they are not already there: //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN} You should now be able to install modules from artifactory into your local development environment. Installing dependencies in docker If you build a docker image as part of your CI pipeline, you will need to copy the .npmrc file into your image before installing there. Example Dockerfile : FROM quay.io/ukhomeofficedigital/nodejs-base:v8 ARG NPM_AUTH_USERNAME ARG NPM_AUTH_TOKEN COPY .npmrc /app/.npmrc COPY package.json /app/package.json COPY package-lock.json /app/package-lock.json RUN npm install --production --no-optional COPY . /app USER nodejs CMD node index.js When building the image, you will then need to pass the username and token variables into docker with the --build-arg flag. docker build --build-arg NPM_AUTH_USERNAME=$${NPM_AUTH_USERNAME} --build-arg NPM_AUTH_TOKEN=$${NPM_AUTH_TOKEN} . Provisioned Volumes and Storage Classes # In order to use volumes with your pod, we use kubernetes provisioned volume claims and storage classes, to read more about this please see Kubernetes Dynamic Provisioning . On each cluster in ACP, we have the the following storage classes for you to use: gp2-encrypted gp2-encrypted-eu-west-2a gp2-encrypted-eu-west-2b gp2-encrypted-eu-west-2c io1-encrypted-eu-west-2 io1-encrypted-eu-west-2a io1-encrypted-eu-west-2b io1-encrypted-eu-west-2c st1-encrypted-eu-west-2 st1-encrypted-eu-west-2a st1-encrypted-eu-west-2b st1-encrypted-eu-west-2c The io1-* (provisioned iops) storage classes have iopsPerGB: \"50\" Backups for EBS Once the ebs has been created, if you'd like to enable EBS snapshots for backups, please raise a ticket via the BAU support so that we can add AWS tags to the volume, which will be picked up by ebs-snapshot . Please remember to specify the retention policy in days to keep the snapshots for. TLS Passthrough # There are occasions when you don't want the TLS to be terminated on the ingress and prefer to terminate in the pod instead. Note, by terminating in the pod the ingress will no longer be able to perform any L7 actions, so all of the feature set is lost (effectively it's become a TLS proxy using SNI to route the traffic) Example steps First create a kubernetes secret containing the certificate you wish to use. $ kubectl create secret tls tls --cert=cert.pem --key=cert-key.pem Create the deployment and service. --- apiVersion: v1 kind: Service metadata: labels: name: tls-passthrough name: tls-passthrough spec: type: ClusterIP ports: - name: https port: 443 protocol: TCP targetPort: 10443 selector: name: tls-passthrough --- --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: tls-passthrough spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: tls-passthrough spec: volumes: - name: certs secret: secretName: tls containers: - name: proxy image: quay.io/ukhomeofficedigital/nginx-proxy:v3.2.0 ports: - name: https containerPort: 10443 protocol: TCP env: - name: PROXY_SERVICE_HOST value: 127.0.0.1 - name: PROXY_SERVICE_PORT value: 8080 - name: SERVER_CERT value: /certs/tls.crt - name: SERVER_KEY value: /certs/tls.key - name: ENABLE_UUID_PARAM value: FALSE - name: NAXSI_USE_DEFAULT_RULES value: FALSE - name: PORT_IN_HOST_HEADER value: FALSE volumeMounts: - name: certs mountPath: /certs readOnly: true - name: fake-application image: kennethreitz/httpbin:latest Push out the ingress resource indicating you want ssl-passthrough enabled. --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/ssl-passthrough: true kubernetes.io/ingress.class: nginx-external name: tls-passthrough spec: rules: - host: tls-passthrough.notprod.homeoffice.gov.uk http: paths: - backend: serviceName: tls-passthrough servicePort: 443 path: / Writing Dockerfiles # Dockerfile best practice We recommend using dockers excellent guidance for this! https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/ Docker images to build from This document lists all of our docker base images that you can build from: Technology specific images NodeJS onbuild image - recommended by default NodeJS base image - if you need more flexibility Scala image JDK image Maven image with Java 8 Ruby image Home Office CentOS base image If none of the technology specific images work for you you can either build on top of them or build from the base centos image: https://github.com/UKHomeOffice/docker-centos-base If you build an image that will be of use to other teams then please add it to the list of technology specific images above! And please make sure it adheres to the below guidance on building new base images. Guidance on building new base images All base images should be built with a set of onbuild commands in them to make sure anything built on top of them will automatically update the base OS, for example: yum install -y curl yum clean all rpm --rebuilddb The nodejs base image is a good example of this.","title":"How To's"},{"location":"how-to-docs/index.html#how-tos","text":"This tree contains a collection of how-to guides for Developers.","title":"How To's"},{"location":"how-to-docs/index.html#create-an-artifactory-access-token","text":"Note: These instructions are intended for the ACP team. If you would like to request an Artifactory token, please use the relevant support request on the Platform Hub . The requester should state the name of the token, how they would like to receive the token and post their GPG key. Create an Artifactory access token using the following command: curl -u username : api-key -XPOST https://artifactory.digital.homeoffice.gov.uk/artifactory/api/security/token -d username= robot-username -d scope=member-of-groups: appropriate-groups -d expires_in=0 where robot-username is the name of the access token and appropriate-groups is a comma separated list of the groups the token should be in (normally this will only be ci ). Note: If you set the expires_in time higher than 0, you will not be able to revoke the token via the UI. Once the token has been created, JSON data should be returned which will include the access key. The JSON data you receive will be the only time you will be able to see the access key as it is not shown on Artifactory. You should, however, be able to see the name and expiry date (if you set an expiry time) of the access key in the \"Access Keys\" section.","title":"Create an Artifactory access token"},{"location":"how-to-docs/index.html#kubernetes-pod-autoscaling","text":"For full documentation on kubernetes autoscaling feature please go here . As of writing the ACP cluster supports standard autoscaling based on a CPU metric, there are however plans to support custom-metrics in the near future. Assuming you have a deployment 'web' and you wish to autoscale the deployment when it hit's a 40% CPU usage with min/max of 5/10 pods. apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: web spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: web metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 Sysdig Metrics - Experimental The autoscaler can also consume and make scaling decisions from sysdig metrics. Note, this feature is currently experimental but tested as working. An example of sysdig would be scaling on http_request apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: autoscaler spec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: myapplication minReplicas: 3 maxReplicas: 10 metrics: - type: Object object: target: kind: Service name: myservice metricName: net.http.request.count targetValue: 100","title":"Kubernetes Pod Autoscaling"},{"location":"how-to-docs/index.html#certificates","text":"","title":"Certificates"},{"location":"how-to-docs/index.html#chisel","text":"The Problem : we want to provide services running in ACP access to the third party services as well as the ability to have user-based access controls. At present network access in ACP is provided via Calico, but this becomes redundant when the traffic egresses the cluster. Simply peering networks together either through VPC peering or VPN connections doesn't provide the controls we want. We could rely on user-authentication on third-party service but not all services are authenticated (take POISE) and beyond that peering networks provides no means of auditing traffic that is traversing the bridged networks. One pattern we are exploring is the use of a proxy cluster with an authenticated side-kick to route traffic and provide end-to-end encryption. Both ACP Notprod and Prod are peered to an respective proxy cluster that is running a Chisel server. Below is rough idea of how the chisel service works. The workflow for this is as follows, note the following example is assuming we have peered with a network in the proxy cluster which is exposing x services. A request via BAU the provisioning of a service on the Chisel server. Once done user is provided credentials for service. You add into your deployment a chisel container running in client mode and add the configuration as described to route the traffic. In regard to DNS and hostnames, kubernetes pods permit the user to add host entries into the container DNS, enabling you to override. The traffic is picked up, encrypted over an ssh tunnel and pushed to the Chisel server where the user credentials are evaluated. Assuming everything is ok the traffic is then proxied on to destination.","title":"Chisel"},{"location":"how-to-docs/index.html#debug-issues-with-your-deployments","text":"","title":"Debug Issues with your deployments"},{"location":"how-to-docs/index.html#dms-migration","text":"","title":"DMS Migration"},{"location":"how-to-docs/index.html#downscaling-services-out-of-hours","text":"In an effort to reduce costs on running the platform, we've enabled to capability to scale down specific resources Out Of Hours (OOH) for Non-Production environments.","title":"Downscaling Services Out Of Hours"},{"location":"how-to-docs/index.html#drone-how-to","text":"","title":"Drone How To"},{"location":"how-to-docs/index.html#scanning-images-in-drone","text":"ACP provides Anchore as scanning solution to images built into the Drone pipeline, allowing users to scan both ephemeral (built within the context of the drone, but not pushed to a repository yet) as well and well any public images. pipeline: build: image: docker:17.09.0-ce environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t docker.digital.homeoffice.gov.uk/myimage:$${DRONE_BUILD_NUMBER} . scan: # The location of the drone plugin image: quay.io/ukhomeofficedigital/anchore-submission:latest # The optional path of a Dockerfile dockerfile: Dockerfile # Note the lack of double $ here (due to the way drone injects variables image_name: docker.digital.homeoffice.gov.uk/myimage:${DRONE_BUILD_NUMBER} # This indicates we are willing tolerate any vulnerabilities which are below medium tolarates: medium # An optional whitelist (comman separated list of CVE's) whitelist: CVE_SOMENAME_1,CVE_SOMENAME_2 # An optional whitelist file containing a list of CSV relative to the repo path whitelist_file: PATH # By default the pligin will exit will fail if any vulnerabilities are discovered which are not tolarated, # you change this behaviour by setting the bellow fail_on_detection: false","title":"Scanning Images in Drone"},{"location":"how-to-docs/index.html#aws-ecr-for-private-docker-images","text":"AWS ECR (Elastic Container Registry) is now available as a self-service feature via the Platform Hub. Each project has the capability to create their own Docker Repositories and define individual access to each via the use of IAM Credentials.","title":"AWS ECR for Private Docker Images"},{"location":"how-to-docs/index.html#using-ingress","text":"An Ingress is a type of Kubernetes resource that allows you to expose your services outside the cluster. It gets deployed and managed exactly like other Kube resources. Our ingress setup offers two different ingresses based on how restrictively you want to expose your services: - internal - only people within the VPN can access services - external - anyone with internet access can access services The annotation kubernetes.io/ingress.class: \"nginx-internal\" is used to specify whether the ingress is internal. ( kubernetes.io/ingress.class: \"nginx-external\" is used for an external ingress.) In the following example the terms \"myapp\" and \"myproject\" have been used, these will need to be changed to the relevant names for your project. Where internal is used, this can be changed for an external ingress - everything else stays the same. apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # used to select which ingress this resource should be configured on kubernetes.io/ingress.class: nginx-internal # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated. ingress.kubernetes.io/secure-backends: true # This replaces the old annotation secure-backends ingress.kubernetes.io/backend-protocol: HTTPS name: myapp-server-internal spec: rules: - host: myapp.myproject.homeoffice.gov.uk http: paths: - backend: serviceName: myapp servicePort: 8000 path: / tls: - hosts: - myapp.myproject.homeoffice.gov.uk # the name of the kubernetes secret in your namespace with tls.crt and tls.key secretName: myapp-github-internal-tls Always ensure you are using TLS between the ingress controller and your pods by placing the annotation: ingress.kubernetes.io/backend-protocol: \"HTTPS\". At the moment; this is not enforced though there are plans to enforce by policy at a later date.","title":"Using Ingress"},{"location":"how-to-docs/index.html#kube-cert-managaer-cloudflare-ssl","text":"Services: - kube-cert-manager is used to retrieve Letencrypt certificates. - cfssl is an internal certificate service used to provide internal tls between pods / services.","title":"Kube-Cert-Managaer &amp; Cloudflare SSL"},{"location":"how-to-docs/index.html#getting-a-kubernetes-robot-token","text":"","title":"Getting a Kubernetes Robot Token"},{"location":"how-to-docs/index.html#getting-a-kubernetes-token","text":"","title":"Getting a Kubernetes Token"},{"location":"how-to-docs/index.html#network-policies","text":"By default a deny-all policy is applied to every namespace in each cluster. You can however add network policies to your own projects to allow for certain connections and these will be applied on top of the default deny-all policy. Here is an example network policy for allowing a connection from the ingress-internal namespace: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-network-policy namespace: your-namespace-here spec: podSelector: matchLabels: role: artifactory ingress: - from: - namespaceSelector: matchLabels: name: ingress-internal ports: - protocol: TCP port: 443 The port number should be the same as the one that your service is listening on.","title":"Network Policies"},{"location":"how-to-docs/index.html#run-performance-tests-on-a-service-hosted-on-acp","text":"","title":"Run Performance Tests on a service hosted on ACP"},{"location":"how-to-docs/index.html#pod-security-policies","text":"By default all user deployments will inherit a default PodSecurityPolicy applied in the Kubernetes Clusters, which define a set of conditions that a pod must be configured with in order to run successfully. The specification for the default policy is as follows: apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: default spec: privileged: false fsGroup: rule: RunAsAny hostPID: false hostIPC: false hostNetwork: false runAsUser: rule: MustRunAsNonRoot seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - downwardAPI - emptyDir - gitRepo - persistentVolumeClaim - projected - secret","title":"Pod Security Policies"},{"location":"how-to-docs/index.html#using-artifactory-as-a-private-npm-registry","text":"A step-by-step guide. This guide makes the following assumptions: you have drone ci set up for your project already you are using node@8 and npm@5 or later you are connected to ACP VPN","title":"Using artifactory as a private npm registry"},{"location":"how-to-docs/index.html#provisioned-volumes-and-storage-classes","text":"In order to use volumes with your pod, we use kubernetes provisioned volume claims and storage classes, to read more about this please see Kubernetes Dynamic Provisioning . On each cluster in ACP, we have the the following storage classes for you to use: gp2-encrypted gp2-encrypted-eu-west-2a gp2-encrypted-eu-west-2b gp2-encrypted-eu-west-2c io1-encrypted-eu-west-2 io1-encrypted-eu-west-2a io1-encrypted-eu-west-2b io1-encrypted-eu-west-2c st1-encrypted-eu-west-2 st1-encrypted-eu-west-2a st1-encrypted-eu-west-2b st1-encrypted-eu-west-2c The io1-* (provisioned iops) storage classes have iopsPerGB: \"50\"","title":"Provisioned Volumes and Storage Classes"},{"location":"how-to-docs/index.html#tls-passthrough","text":"There are occasions when you don't want the TLS to be terminated on the ingress and prefer to terminate in the pod instead. Note, by terminating in the pod the ingress will no longer be able to perform any L7 actions, so all of the feature set is lost (effectively it's become a TLS proxy using SNI to route the traffic)","title":"TLS Passthrough"},{"location":"how-to-docs/index.html#writing-dockerfiles","text":"","title":"Writing Dockerfiles"},{"location":"how-to-docs/artifactory-token.html","text":"Create an Artifactory access token # Note: These instructions are intended for the ACP team. If you would like to request an Artifactory token, please use the relevant support request on the Platform Hub . The requester should state the name of the token, how they would like to receive the token and post their GPG key. Create an Artifactory access token using the following command: curl -u username : api-key -XPOST https://artifactory.digital.homeoffice.gov.uk/artifactory/api/security/token -d username= robot-username -d scope=member-of-groups: appropriate-groups -d expires_in=0 where robot-username is the name of the access token and appropriate-groups is a comma separated list of the groups the token should be in (normally this will only be ci ). Note: If you set the expires_in time higher than 0, you will not be able to revoke the token via the UI. Once the token has been created, JSON data should be returned which will include the access key. The JSON data you receive will be the only time you will be able to see the access key as it is not shown on Artifactory. You should, however, be able to see the name and expiry date (if you set an expiry time) of the access key in the \"Access Keys\" section.","title":"Artifactory token"},{"location":"how-to-docs/artifactory-token.html#create-an-artifactory-access-token","text":"Note: These instructions are intended for the ACP team. If you would like to request an Artifactory token, please use the relevant support request on the Platform Hub . The requester should state the name of the token, how they would like to receive the token and post their GPG key. Create an Artifactory access token using the following command: curl -u username : api-key -XPOST https://artifactory.digital.homeoffice.gov.uk/artifactory/api/security/token -d username= robot-username -d scope=member-of-groups: appropriate-groups -d expires_in=0 where robot-username is the name of the access token and appropriate-groups is a comma separated list of the groups the token should be in (normally this will only be ci ). Note: If you set the expires_in time higher than 0, you will not be able to revoke the token via the UI. Once the token has been created, JSON data should be returned which will include the access key. The JSON data you receive will be the only time you will be able to see the access key as it is not shown on Artifactory. You should, however, be able to see the name and expiry date (if you set an expiry time) of the access key in the \"Access Keys\" section.","title":"Create an Artifactory access token"},{"location":"how-to-docs/autoscaling.html","text":"Kubernetes Pod Autoscaling # For full documentation on kubernetes autoscaling feature please go here . As of writing the ACP cluster supports standard autoscaling based on a CPU metric, there are however plans to support custom-metrics in the near future. Assuming you have a deployment 'web' and you wish to autoscale the deployment when it hit's a 40% CPU usage with min/max of 5/10 pods. apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: web spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: web metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 Sysdig Metrics - Experimental The autoscaler can also consume and make scaling decisions from sysdig metrics. Note, this feature is currently experimental but tested as working. An example of sysdig would be scaling on http_request apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: autoscaler spec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: myapplication minReplicas: 3 maxReplicas: 10 metrics: - type: Object object: target: kind: Service name: myservice metricName: net.http.request.count targetValue: 100","title":"Autoscaling"},{"location":"how-to-docs/autoscaling.html#kubernetes-pod-autoscaling","text":"For full documentation on kubernetes autoscaling feature please go here . As of writing the ACP cluster supports standard autoscaling based on a CPU metric, there are however plans to support custom-metrics in the near future. Assuming you have a deployment 'web' and you wish to autoscale the deployment when it hit's a 40% CPU usage with min/max of 5/10 pods. apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: web spec: maxReplicas: 10 minReplicas: 5 scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: web metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 Sysdig Metrics - Experimental The autoscaler can also consume and make scaling decisions from sysdig metrics. Note, this feature is currently experimental but tested as working. An example of sysdig would be scaling on http_request apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: autoscaler spec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: myapplication minReplicas: 3 maxReplicas: 10 metrics: - type: Object object: target: kind: Service name: myservice metricName: net.http.request.count targetValue: 100","title":"Kubernetes Pod Autoscaling"},{"location":"how-to-docs/certificates.html","text":"Certificates # Application Certificates Before reading about certificates and how you can create and manage them. Please familiarise yourself with our DNS naming convention first The platform provides two ways of managing HTTPS certificates : - Internal based certificates i.e. hostname.namespace.svc.cluster.local using CFSSL - External based certificates for external services i.e. service.homeoffice.gov.uk using kube-cert-manager and letsencrypt In most systems, it's likely that your service will have a user facing service, that will be served through an external endpoint i.e. it can be routed to externally by users as well as having non-external facing services i.e. internal services. You would want all communication between the user, through to the service and service dependencies to be encrypted, so that the traffic flow has encryption and that all endpoints trust who they are speaking to. Certificates kube-cert-manager and cfssl LetsEncrypt Limits Note Letsencrypt while a free service does come with a number of service limits detailed here . Probably one of the most crucial for projects is the max certificate requests per week; currently standing at 20. In addition, there is a max 5 failures for per hostname with a freeze of 1 hour, so if you accidently mess up configuration you might hit this.","title":"Certificates"},{"location":"how-to-docs/certificates.html#certificates","text":"","title":"Certificates"},{"location":"how-to-docs/chisel.html","text":"Chisel # The Problem : we want to provide services running in ACP access to the third party services as well as the ability to have user-based access controls. At present network access in ACP is provided via Calico, but this becomes redundant when the traffic egresses the cluster. Simply peering networks together either through VPC peering or VPN connections doesn't provide the controls we want. We could rely on user-authentication on third-party service but not all services are authenticated (take POISE) and beyond that peering networks provides no means of auditing traffic that is traversing the bridged networks. One pattern we are exploring is the use of a proxy cluster with an authenticated side-kick to route traffic and provide end-to-end encryption. Both ACP Notprod and Prod are peered to an respective proxy cluster that is running a Chisel server. Below is rough idea of how the chisel service works. The workflow for this is as follows, note the following example is assuming we have peered with a network in the proxy cluster which is exposing x services. A request via BAU the provisioning of a service on the Chisel server. Once done user is provided credentials for service. You add into your deployment a chisel container running in client mode and add the configuration as described to route the traffic. In regard to DNS and hostnames, kubernetes pods permit the user to add host entries into the container DNS, enabling you to override. The traffic is picked up, encrypted over an ssh tunnel and pushed to the Chisel server where the user credentials are evaluated. Assuming everything is ok the traffic is then proxied on to destination. A Working Example We have a two services called example-api.internal.homeoffice.gov.uk and another-service.example.com and we wish to consume the API from the pods. Lets assume the service has already been provisioned on the Chisel server and we have the credentials at hand. kind: Deployment metadata: name: consumer spec: replicas: 1 template: metadata: labels: name: consumer spec: hostAliases: - hostnames: - another-service.example.com - example-api.internal.homeoffice.gov.uk ip: 127.0.0.1 securityContext: fsGroup: 1000 volumes: - name: bundle configMap: name: bundle containers: - name: consumer image: quay.io/ukhomeofficedigital/someimage:someversion - name: chisel image: quay.io/ukhomeofficedigital/chisel:latest securityContext: runAsNonRoot: true env: # essentially user:password - name: AUTH valueFrom: secretKeyRef: name: chisel key: chisel.auth # this optional BUT recommended this is fingerprint for the SSH service - name: CHISEL_KEY valueFrom: secretKeyRef: name: chisel key: chisel.key args: - client - -v # this the chisel endpoint service hostname - gateway-internal.px.notprod.acp.homeoffice.gov.uk:443 # this is saying listen on port 10443 and route all traffic to another-service.example.com:443 endpoint - 127.0.0.1:10443:another-service.example.com:443 - 127.0.0.1:10444:example-api.internal.homeoffice.gov.uk:443 volumeMounts: - name: bundle mountPath: /etc/ssl/certs readOnly: true The above embeds the sidekick into the Pod and requests the client to listen on localhost:10443 and 10444 to redirect traffic via the Chisel service. The one annoying point here is the port requirements, placing things on different ports, but unfortunately this is required. You should be able to call the service via curl https://another-service.example.com:10443 at this point.","title":"Chisel"},{"location":"how-to-docs/chisel.html#chisel","text":"The Problem : we want to provide services running in ACP access to the third party services as well as the ability to have user-based access controls. At present network access in ACP is provided via Calico, but this becomes redundant when the traffic egresses the cluster. Simply peering networks together either through VPC peering or VPN connections doesn't provide the controls we want. We could rely on user-authentication on third-party service but not all services are authenticated (take POISE) and beyond that peering networks provides no means of auditing traffic that is traversing the bridged networks. One pattern we are exploring is the use of a proxy cluster with an authenticated side-kick to route traffic and provide end-to-end encryption. Both ACP Notprod and Prod are peered to an respective proxy cluster that is running a Chisel server. Below is rough idea of how the chisel service works. The workflow for this is as follows, note the following example is assuming we have peered with a network in the proxy cluster which is exposing x services. A request via BAU the provisioning of a service on the Chisel server. Once done user is provided credentials for service. You add into your deployment a chisel container running in client mode and add the configuration as described to route the traffic. In regard to DNS and hostnames, kubernetes pods permit the user to add host entries into the container DNS, enabling you to override. The traffic is picked up, encrypted over an ssh tunnel and pushed to the Chisel server where the user credentials are evaluated. Assuming everything is ok the traffic is then proxied on to destination.","title":"Chisel"},{"location":"how-to-docs/debug-issues.html","text":"Debug Issues with your deployments # Debug with secrets Sometimes your app doesn't want to talk to an API or a DB and you've stored the credentials or just the details of that in secret. The following approaches can be used to validate that your secret is set correctly $ kubectl exec -ti my-pod -c my-container -- mysql -h\\$DBHOST -u\\$DBUSER -p\\$DBPASS ## or $ kubectl exec -ti my-pod -c my-container -- openssl verify /secrets/certificate.pem ## or $ kubectl exec -ti my-pod -c my-container bash ## and you'll naturally have all the environment variables set and volumes mounted. ## however we recommend against outputing them to the console e.g. echo $DBHOST ## instead if you want to assert a variable is set correctly use $ [[ -z $DBHOST ]]; echo $? ## if it returns 1 then the variable is set. Debugging issues with your deployments to the platform If you get to the end of the above guide but can't access your application there are a number of places something could be going wrong. This section of the guide aims to give you some basic starting points for how to debug your application. Debugging deployments We suggest the following steps: 1. Check your deployment, replicaset and pods created properly $ kubectl get deployments $ kubectl get rs $ kubectl get pods 2. Investigate potential issues with your pods (this is most likely) If the get pods command shows that your pods aren't all running then this is likely where the issue is. You can then try curling your application to see if it is alive and responding as expected. e.g. $ curl localhost:4000 You can get further details on why the pods couldn't be deployed by running: $ kubectl describe pods *pods_name_here* If your pods are running you can check they are operating as expected by exec ing into them (this gets you a shell on one of your containers). $ kubectl exec -ti *pods_name_here* -c *container_name_here* /bin/sh Please note that the -c argument isn't needed if there is only one container in the pod.* 3. Investigate potential issues with your service A good way to do this is to run a container in your namespace with a bash terminal: $ kubectl run -ti --image quay.io/ukhomeofficedigital/centos-base debugger bash From this container you can then try curling your service. Your service will have a nice DNS name by default, so you can for example run: $ curl my-service-name 4. Investigate potential issues with ingress Minikube runs an ingress service using nginx. It's possible to ssh into the nginx container and cat the nginx.conf to inspect the configuration for nginx. In order to attach to the nginx container, you need to know the name of the container: $ kubectl get pods NAME READY STATUS RESTARTS AGE default-http-backend-2kodr 1/1 Running 1 5d acp-hello-world-3757754181-x1kdu 1/1 Running 2 6d ingress-3879072234-5f4uq 1/1 Running 2 5d You can attach to the running container with: $ kubectl exec -ti ingress-3879072234-5f4uq -c proxy bash where proxy is the container name of the nginx proxy inside the pod. You can find the name by describing the pod. You're inside the container. You can cat the nginx.conf with: $ cat /etc/nginx/nginx.conf You can also inspect the logs with: $ kubectl logs ingress-3879072234-5f4uq","title":"Debug issues"},{"location":"how-to-docs/debug-issues.html#debug-issues-with-your-deployments","text":"","title":"Debug Issues with your deployments"},{"location":"how-to-docs/dms-migration.html","text":"DMS Migration # Prerequisite The following need to be true before you follow this guide: AWS console logon Access to the DMS service from console * A region where STS has been activated DMS Setup Login to the AWS console using your auth, switch to a role with the correct access policies and verify you're in the right region. Next, select DMS from the services on the main dashboard to access the data migration home screen. Under the \"Get started\" section click on the \"create migration\" button then next to the Replication instance. You should see the following screen: The following are the options and example answers for the replication instance: Option Example answer Description Name dev-team-dms A name for the replication image. This name should be unique. Description DMS instance for migration Brief description of the instance Instance class dms.t2.medium The class of replication resource with the configuration you need for your migration. VPC vpc-* The virtual private cloud resource where you wish to add your dms instance. This should be as close to both the source and target instance as possible. Multi-AZ No Optional parameter to create a standby replica of your replication instance in another Availability Zone. Used for failover. Publicly Accessible False Option to access your instance from the internet You won't need to set any of the advanced settings. To create the instance click on the next button. You should now see a screen like this: The following are the options and example answers for the endpoints instances: Option Example answer Description Endpoint identifer database-source/target This is the name you use to identify the endpoint. Source/target engine postgres Choose the type of database engine that for this endpoint. Server name mysqlsrvinst.abcd123456789.us-west-1.rds.amazonaws.com Type of server name. For an on-premises database, this can be the IP address or the public hostname. For an Amazon RDS DB instance, this can be the endpoint for the DB instance. Port 5432 The port used by the database. SSL mode None SSL mode for encryption for your endpoints. Username root The user name with the permissions required to allow data migration. Password * * The password for the account with the required permissions. Database Name (target) dev-db The name of the attached database to the selected endpoint. Repeat these options for both source and target and make sure to test connection before clicking next. You might need to append security group rules to allow the replication instance access, for example: Replication instance has internal ip address 10.20.0.0 and the RDS is on port 5432 and uses TCP. Append rule Type Procol Port Range Source Custom TCP rule TCP 5432 Custom 10.20.0.0/32 Once this has fully been setup click next and you should be able to view the tasks page: The following are the options and example answers for these tasks: Option Example answer Description Task name Migration-task A name for the task. Task Description Task for migrating A description for the task. Source endpoint source-instance The source endpoint for migration. Target endpoint target-instance The target endpoint for migration. Replication instance replication-instance The replication instance to be used. Migration type Migrate existing data Migration method you want to use. Start task on create True When selected the task begins as soon as it is created. Target table preparation Drop table on target Migration strategy on target. Include LOB columns in replication Limited LOB mode Migration of large objects on target. Max LOB size 32 kb Maximum size of large objects. Enable logging False When selected migration events are logged. After completion the job will automatically run if \"start task on create\" has been selected. If not, the job can be started in the tasks section by selecting it and clicking on the \"Start/Resume\" button.","title":"Dms migration"},{"location":"how-to-docs/dms-migration.html#dms-migration","text":"","title":"DMS Migration"},{"location":"how-to-docs/downscaling.html","text":"Downscaling Services Out Of Hours # In an effort to reduce costs on running the platform, we've enabled to capability to scale down specific resources Out Of Hours (OOH) for Non-Production environments. AWS RDS (Relational Database Service) Non-Production RDS resources can be transitioned to a stopped state OOH to save on resource utilisation costs. This is currently managed with the use of tags on the RDS instance defining a cronjob schedule to stop and start the instance. To set a schedule for your RDS instances, please use the related Platform Hub support request template titled \"Shutdown RDS Instance(s) Out Of Hours\" . Note: Shutting down an RDS instance will have cost savings based on the instance size, however you will still be charged for the allocated storage. Kubernetes Pods Automatically scale down Kubernetes Deployments Statefulsets to 0 replicas during non-working hours for Non-Production or Production Environments. Downscaling for Deployments Statefulsets are managed by an annotation set within the manifest, and are processed every 30 seconds for changes, by a service running within the Kubernetes Clusters. Usage Set ONE of the following annotations on your Deployment / Statefulset: - downscaler/uptime : A time schedule in which the Deployment should be scaled up - downscaler/downtime : A time schedule in which the Deployment should be scaled down to 0 replicas The annotation values for the timeframe must have the following format to be processed correctly: WEEKDAY-FROM - WEEKDAY-TO-INCLUSIVE HH : MM - HH : MM TIMEZONE For example, to schedule a Deployment to only run on weekdays during working hours, the following annotation would be set: downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London Note: When the deployment is downscaled, an additional annotation downscaler/original-replicas is automatically set to retain a history of the desired replicas prior to the downscale action. If this annotation has been deleted before the service is automatically scaled back up, the downscaler service will not know what to set the replicas back to, and so it won't attempt to scale up the resource. Example Spec: apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London labels: name: example-app name: example-app namespace: acp-example spec: replicas: 2 template: spec: containers: image: docker.digital.homeoffice.gov.uk/acp-example-app:v0.0.1@sha256:07397c41ac25c4b19e0485006849201f04168703f0016fad75b8ba5d9885d6d4 ...","title":"Downscaling"},{"location":"how-to-docs/downscaling.html#downscaling-services-out-of-hours","text":"In an effort to reduce costs on running the platform, we've enabled to capability to scale down specific resources Out Of Hours (OOH) for Non-Production environments.","title":"Downscaling Services Out Of Hours"},{"location":"how-to-docs/drone-how-to.html","text":"Drone How To # Install Drone CLI Github drone instance: https://drone.acp.homeoffice.gov.uk/ Gitlab drone instance: https://drone-gitlab.acp.homeoffice.gov.uk/ Download and install the Drone CLI . At the time of writing, we are using version 0.8 of Drone. You can also install a release from Drone CLI's GitHub repo . Once you have downloaded the relevant file, extract it and move it to the /usr/local/bin directory. Verify it works as expected: $ drone --version drone version 0.8.0 Export the DRONE_SERVER and DRONE_TOKEN variables. You can find your token on Drone by clicking the icon in the top right corner and going to Token . export DRONE_SERVER=https://drone.acp.homeoffice.gov.uk export DRONE_TOKEN= your_drone_token If your installation is successful, you should be able to query the current Drone instance: $ drone info User: youruser Email: youremail@gmail.com If the output is bash Error: you must provide the Drone server address. or Error: you must provide your Drone access token. Please make sure that you have exported the DRONE_SERVER and DRONE_TOKEN variables properly. Activate your pipeline Once you are logged in to Drone, you will find a list of repos by clicking the icon in the top right corner and going to Repositories . Select the repo you want to activate. Navigate to your repository's settings in Github (or Gitlab) and you will see a webhook has been created. You need to update the url for the newly created web hook so that it matches this pattern: https://drone-external.acp.homeoffice.gov.uk/hook?access_token=some_token If it is already in that format there is no need to change anything. The token in the payload url will not be the same as the personal token that you exported and it should be left unchanged. Please note that this does not apply to Gitlab. When you activate the repo in Drone, you should not change anything for a Gitlab repo. Configure your pipeline In the root folder of your project, create a .drone.yml file with the following content: pipeline: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t image_name . when: branch: master event: push Commit and push your changes: $ git add .drone.yml $ git commit $ git push origin master Please note you should replace the name ... with the name of your app. You should be able to watch your build succeed in the Drone UI. Publishing Docker images Publishing to Quay If your repository is hosted on Gitlab, you don't want to publish your images to Quay. Images published to Quay are public and can be inspected and downloaded by anyone. You should publish your private images to Artifactory . Register for a free Quay account using your Github account linked to the Home Office organisation. Once you've logged into Quay check that you have ukhomeofficedigital under Users and Organisations. If you do not, submit a support request on the platform hub for access to the ukhomeoffice organisation . Once you have access to view the ukhomeofficedigital repositories, click repositories and click the + Create New Repositories that is: public empty - no need to create a repo from a Dockerfile or link it to an existing repository Add your project to the UKHomeOffice Quay account and submit a support request on the platform hub for a new Quay robot . Add the step to publish the docker image to Quay in your Drone pipeline: image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+ your_robot_username registry: quay.io repo: quay.io/ukhomeofficedigital/ your_quay_repo tags: - ${DRONE_COMMIT_SHA} - latest when: branch: master event: push Where your_quay_repo in: quay.io/ukhomeofficedigital/ your_quay_repo is the name of the Quay repo you (should) have already created. Note: ${DRONE_COMMIT_SHA} is a Drone environment variable that is passed to the container at runtime. The build should fail with the following error: Error response from daemon: Get https://quay.io/v2/: unauthorized: Could not find robot with username: ukhomeofficedigital+ your_robot_username and supplied password. The error points to the missing password for the Quay robot. You will need to add this as a drone secret. You can do this through the Drone UI by going to your repo, clicking the menu icon in the top right and then clicking Secrets . You should be presented with a list of the secrets for that repo (if there are any) and you should be able to add secrets giving them a name and value. Add a secret with the name DOCKER_PASSWORD and with the value being the robot token that was supplied to you. Alternatively, you can use the Drone CLI to add the secret: $ drone secret add --repository ukhomeoffice/ your_github_repo --name DOCKER_PASSWORD --value your_robot_token Restarting the build should be enough to make it pass. The Drone CLI allows for more control over the secret as opposed to the UI. For example, the CLI allows you to specify the image and the events that the secret will be allowed to be used with. Also note that the secret was specified in the secrets section of the pipeline to give it access to the secret. Without this, the pipeline would not be able to use the secret and it would fail. Secrets in this section are automatically uppercased at runtime so it is important that the secret is uppercased in your commands. You can also push specifically tagged images by using the DRONE_TAG Drone environment variable and by using the tag event: tagged_image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+ your_robot_username registry: quay.io repo: quay.io/ukhomeofficedigital/ your_quay_repo tags: - ${DRONE_TAG} when: event: tag Tag using git tag v1.0 and push your tag with git push origin v1.0 (replace v1.0 with the tag you actually want to use). Note: These pipeline configurations are using the Docker plugin for Drone. For more information, see http://plugins.drone.io/drone-plugins/drone-docker/ Publishing to Artifactory Images hosted on Artifactory are private. If your repository is hosted publicly on GitHub, you shouldn't publish your images to Artifactory. Artifactory is only used to publish private images. You should use Quay to publish your public images . Submit a support request for a new Artifactory access token . You should be supplied an access token in response. You can inject the token that has been supplied to you with: $ drone secret add --repository gitlab_repo_group / your_gitlab_repo --name DOCKER_PASSWORD --value your_robot_token You can add the following step in your .drone.yml : image_to_artifactory: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME= your_robots_username registry: docker.digital.homeoffice.gov.uk repo: docker.digital.homeoffice.gov.uk/ your_artifactory_repo tags: - ${DRONE_COMMIT_SHA} - latest when: branch: master event: push Where the image_name in: docker tag image_name docker.digital.homeoffice.gov.uk/ukhomeofficedigital/ your_artifactory_repo :$${DRONE_COMMIT_SHA} is the name of the image you tagged previously in the build step. The image should now be published on Artifactory. Deployments Deployments and promotions Create a step that runs only on deployments: deploy-to-preprod: image: busybox commands: - /bin/echo hello preprod when: environment: preprod event: deployment Push the changes to your remote repository. You can deploy the build you just pushed with the following command: $ drone deploy ukhomeoffice/ your_repo 16 preprod Where 16 is the successful build number on drone that you wish to deploy to the preprod environment. You can pass additional parameters to your deployment as environment variables: $ drone deploy ukhomeoffice/ your_repo 16 preprod -p DEBUG=1 -p NAME=Dan and use them in the step like this: deploy-to-preprod: image: busybox commands: - /bin/echo hello $${NAME} when: environment: preprod event: deployment Environments are strings and can be set to any value. When you wish to deploy to several environments you can create a step for each one of them: deploy-to-preprod: image: busybox commands: - /bin/echo hello preprod when: environment: preprod event: deployment deploy-to-prod: image: busybox commands: - /bin/echo hello prod when: environment: prod event: deployment And deploy them accordingly: $ drone deploy ukhomeoffice/ your_repo 16 preprod $ drone deploy ukhomeoffice/ your_repo 16 prod Read more on environments . Drone as a Pull Request builder Drone pipelines are triggered when events occurs. Event triggers can be as simple as a push , a tagged commit , a pull request or as granular as only for pull requests for a branch named test . You can limit the execution of build steps at runtime using the when block. As an example, this block executes only on pull requests: pr-builder: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t image_name . when: event: pull_request Drone will only execute that step when a new pull request is raised (and when pushes are made to the branch while a pull request is open). Read more about Drone conditions . Deploying to ACP Please note that this section assumes that you already have kube files to work with (specifically, deployment, service and ingress files). Examples of these files can be found in the kube-signed-commit-check project. Add a deployment script with the following: #!/bin/bash export KUBE_NAMESPACE= dev-induction export KUBE_SERVER=${KUBE_SERVER} export KUBE_TOKEN=${KUBE_TOKEN} kd -f deployment.yaml \\ -f service.yaml \\ -f ingress.yaml Please note that this is only an example script and it will need to be changed to fit your particular application's needs. If you deployed this now you would likely receive an error similar to this: error: You must be logged in to the server (the server has asked for the client to provide credentials) This error appears because kd needs 3 environment variables to be set before deploying: KUBE_NAMESPACE - The kubernetes namespace you wish to deploy to. You need to provide the kubernetes namespace as part of the deployment job . KUBE_TOKEN - This is the token used to authenticate against the kubernetes cluster. If you do not already have a kube token, here are docs explaining how to get one . KUBE_SERVER - This is the address of the kubernetes cluster that you want to deploy to. You will need to add KUBE_TOKEN and KUBE_SERVER as drone secrets. Information about how to add Drone secrets can be found in the publishing to Quay section . You can verify that the secrets for your repo are present with: $ drone secret ls --repository ukhomeoffice/ your-repo Once the secrets have been added, add a new step to your drone pipeline that will execute the deployment script: deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - ./deploy.sh when: environment: uat event: deployment Using Another Repo It is possible to access files or deployment scripts from another repo, there are two ways of doing this. The recommended method is to clone another repo in the current repo (since this only requires maintaining one .drone.yml) using the following step: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/ your_repo .git when: environment: uat event: deployment Your repository is saved in the workspace, which in turn is shared among all steps in the pipeline. However, if you decide that you want to trigger a completely different pipeline on a separate repository, you can leverage the drone-trigger plugin. If you have a secondary repository, you can setup Drone on that repository like so: pipeline: deploy_to_uat: image: busybox commands: - echo ${SHA} when: event: deployment environment: uat Once you are ready, you can push the changes to the remote repository. In your main repository you can add the following step: trigger_deploy: image: quay.io/ukhomeofficedigital/drone-trigger:latest drone_server: https://drone.acp.homeoffice.gov.uk repo: UKHomeOffice/ deployment_repo branch: master deploy_to: uat params: SHA=${DRONE_COMMIT_SHA} when: event: deployment environment: uat The settings are very similar to the drone deploy command: deploy_to is the environment constraint params is a list of comma separated list of arguments. In the command line tool, this is equivalent to -p PARAM1=ONE -p PARAM2=TWO repo the repository where the deployment scripts are located The next time you trigger a deployment on the main repository with: $ drone deploy UKHomeOffice/ your_repo 16 uat This will trigger a new deployment on the second repository. Please note that in this scenario you need to inspect 2 builds on 2 separate repositories if you just want to inspect the logs. Versioned deployments When you restart your build, Drone will automatically use the latest version of the code. However always using the latest version of the deployment configuration can cause major issues and isn't recommended. For example when promoting from preprod to prod you want to use the preprod version of the deployment configuration. If you use the latest it could potentially break your production environment, especially as it won't necessarily have been tested. To counteract this you should use a specific version of your deployment scripts. In fact, you should git checkout the tag or sha as part of your deployment step. Here is an example of this: predeploy_to_uat: image: plugins/git commands: - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/ your_repo .git when: environment: uat event: deployment deploy_to_uat: image: quay.io/ukhomeofficedigital/kd:v0.11.0 secrets: - kube_server - kube_token commands: - apk update apk add git - git checkout v1.1 - ./deploy.sh when: environment: uat event: deployment Migrating your pipeline Secrets and Signing It is no longer necessary to sign your .drone.yml so the .drone.yml.sig can be deleted. Secrets can be defined in the Drone UI or using the CLI. Secrets created using the UI will be available to push, tag and deployment events. To restrict to selected events, or to allow pull request builds to access secrets you must use the CLI. Pipelines by default do not have access to any Drone secrets that you have added. You must now define which secrets a pipeline is allowed access to in a secrets section in your pipeline. Here is an example of a pipeline that has access to the DOCKER_PASSWORD secret which will be used to push an image to Quay: image_to_quay: image: quay.io/ukhomeofficedigital/drone-docker secrets: - docker_password environment: - DOCKER_USERNAME=ukhomeofficedigital+ your_robot_username registry: quay.io repo: quay.io/ukhomeofficedigital/ your_quay_repo tags: - latest when: branch: master event: push Note: Secrets names in the secrets section will have their names uppercased at runtime. Organisation secrets are no longer available. This means that if you are using any organisation secrets such as KUBE_TOKEN_DEV , you will need to add a secret in Drone to replace it. Docker-in-Docker The Docker-in-Docker (dind) service is no longer required. Instead, change the Docker host to DOCKER_HOST=tcp://172.17.0.1:2375 in the environment section of your pipline, and you will be able to access the shared Docker server on the drone agent. Note that it is only possible to run one Docker build at a time per Drone agent. Since privileged mode was primarily used for docker in docker, you should remove the privileged: true line from your .drone.yml . You can also use your freshly built image directly and run commands as part of your pipeline. Example: pipeline: build_image: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t hello_world . when: branch: master event: push test_image: image: hello_world commands: - ./run-hello-world.sh when: branch: master event: push Services If you use the services section of your .drone.yml it is possible to reference them using the DNS name of the service. For example, if using the following section: services: database: image: mysql The mysql server would be available on tcp://database:3306 Variable Escaping Any Drone variables (secrets and environment variables) must now be escaped by having two $$ instead of one. Examples: ${DOCKER_PASSWORD} -- $${DOCKER_PASSWORD} ${DRONE_TAG} -- $${DRONE_TAG} ${DRONE_COMMIT_SHA} -- $${DRONE_COMMIT_SHA} Scanning Images in Drone # ACP provides Anchore as scanning solution to images built into the Drone pipeline, allowing users to scan both ephemeral (built within the context of the drone, but not pushed to a repository yet) as well and well any public images. pipeline: build: image: docker:17.09.0-ce environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t docker.digital.homeoffice.gov.uk/myimage:$${DRONE_BUILD_NUMBER} . scan: # The location of the drone plugin image: quay.io/ukhomeofficedigital/anchore-submission:latest # The optional path of a Dockerfile dockerfile: Dockerfile # Note the lack of double $ here (due to the way drone injects variables image_name: docker.digital.homeoffice.gov.uk/myimage:${DRONE_BUILD_NUMBER} # This indicates we are willing tolerate any vulnerabilities which are below medium tolarates: medium # An optional whitelist (comman separated list of CVE's) whitelist: CVE_SOMENAME_1,CVE_SOMENAME_2 # An optional whitelist file containing a list of CSV relative to the repo path whitelist_file: PATH # By default the pligin will exit will fail if any vulnerabilities are discovered which are not tolarated, # you change this behaviour by setting the bellow fail_on_detection: false Q As Q: The build fails with \"ERROR: Insufficient privileges to use privileged mode\" A: Remove privileged: true from your .drone.yml . As explained in the migrating your pipeline section , the primary use of this was for Docker-in-Docker which is not required. Q: The build fails with \"Cannot connect to the Docker daemon. Is the docker daemon running on this host?\" A: Make sure that your steps contain the environment variable DOCKER_HOST=tcp://172.17.0.1:2375 like in this case: my-build: image: docker:18.03 environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t image_name . when: branch: master event: push Q: The build fails when uploading to Quay with the error \"Error response from daemon: Get https://quay.io/v2/: unauthorized:...\" A: This is likely because the secret wasn't added correctly or the password is incorrect. Check that the secret has been added to Drone and that you have added the secrets section in your .drone.yaml it to the pipeline that requires it. Q: As part of my build process I have two Dockerfiles to produce a Docker image. How can I share files between builds in the same step? A: When the pipeline starts, Drone creates a Docker data volume that is passed along all active steps in the pipeline. If the first step creates a test.txt file, the second step can use that file. As an example, this pipeline uses a two step build process: pipeline: first-step: image: busybox commands: - echo hello test.txt when: branch: master event: push second-step: image: busybox commands: - cat test.txt when: branch: master event: push Q: Should I use Gitlab with Quay? A: Please don't. If your repository is hosted in Gitlab then use Artifactory to publish your images. Images published to Artifactory are kept private. If you still want to use Quay, you should consider hosting your repository on the open (Github). Q: Can I create a token that has permission to create ephemeral/temporary namespaces? A: No. This is because there is currently no way to give access to namespaces via regex. I.e. There is no way to give access to any namespace with the format: my-temp-namespace-* (where * would be build number or something similar). Alternatively, you can be given a named namespace in the CI cluster. Please create an issue on our BAU board if you require this.","title":"Drone how to"},{"location":"how-to-docs/drone-how-to.html#drone-how-to","text":"","title":"Drone How To"},{"location":"how-to-docs/drone-how-to.html#scanning-images-in-drone","text":"ACP provides Anchore as scanning solution to images built into the Drone pipeline, allowing users to scan both ephemeral (built within the context of the drone, but not pushed to a repository yet) as well and well any public images. pipeline: build: image: docker:17.09.0-ce environment: - DOCKER_HOST=tcp://172.17.0.1:2375 commands: - docker build -t docker.digital.homeoffice.gov.uk/myimage:$${DRONE_BUILD_NUMBER} . scan: # The location of the drone plugin image: quay.io/ukhomeofficedigital/anchore-submission:latest # The optional path of a Dockerfile dockerfile: Dockerfile # Note the lack of double $ here (due to the way drone injects variables image_name: docker.digital.homeoffice.gov.uk/myimage:${DRONE_BUILD_NUMBER} # This indicates we are willing tolerate any vulnerabilities which are below medium tolarates: medium # An optional whitelist (comman separated list of CVE's) whitelist: CVE_SOMENAME_1,CVE_SOMENAME_2 # An optional whitelist file containing a list of CSV relative to the repo path whitelist_file: PATH # By default the pligin will exit will fail if any vulnerabilities are discovered which are not tolarated, # you change this behaviour by setting the bellow fail_on_detection: false","title":"Scanning Images in Drone"},{"location":"how-to-docs/elastic-container-registry.html","text":"AWS ECR for Private Docker Images # AWS ECR (Elastic Container Registry) is now available as a self-service feature via the Platform Hub. Each project has the capability to create their own Docker Repositories and define individual access to each via the use of IAM Credentials. Creating a Docker Repository Anybody that is part of a Project within the Platform Hub will have the ability to create a new Docker Repository. Login to the Platform Hub via https://hub.acp.homeoffice.gov.uk Navigate to the Projects list: https://hub.acp.homeoffice.gov.uk/projects/list Select your Project from the list to go to the detail page (e.g. https://hub.acp.homeoffice.gov.uk/projects/detail/acp) Ensure you have a Service defined within your Project for the Docker Repository to be associated with (check under the SERVICES tab) Select the ALL DOCKER REPOS tab Select the REQUEST NEW DOCKER REPO button Choose the Service to associate this Repository with and provide the name of the Repository to be created (e.g. hello-world-app ) The request to create a new Docker Repository can take a few seconds to complete. You can view the status of a Repository by navigating to the ALL DOCKER REPOS tab and viewing the list. Once the request has completed, your Repository should have the Active label associated with it. This repository won't automatically refresh, but you can hit the REFRESH button above the Repository list or just manually refresh your browser window for updates. Generating Access Credentials Access to ECR Repositories is managed via AWS IAM. These IAM credentials are generated via the Platform Hub and access can be managed per user, per Docker Repository. Navigate to the ALL DOCKER REPOS tab for your Project within the Platform Hub For the Repository you have created, select the MANAGE ACCESS button At this stage, you can: Create a Robot Account(s), which can be used in deployment pipelines in Drone CI for publishing new images to AWS ECR Select which Project Members have the ability to pull images, and additionally push updates using their own IAM credentials (separate to the Robot Account(s) and CI builds) For this example, select your own User and press Save . Note: Generally users should never be granted write access, as any write actions should be performed via CI (using the Robot Accounts). Press the REFRESH button at the top of the page and check the User Access has a status of active Robot Accounts are visible under the Docker Repository, and once they reveal an active status the IAM Credentials are displayed alongside it. Accessing a Docker Repository Accessing the AWS Container Registry to Pull Push images is currently a two-step process: 1. Use IAM Credentials to generate a temporary authorisation token 1. Use the temporary authorisation token to authenticate your docker client with ECR Note: The authorisation token generated for docker login is only valid for 12 hours, and so the process above will need to be repeated. Pre-Requisites To follow the below steps you must have: AWS CLI (version 1.11.91 or above, check with aws --version ) * Install Guides: Linux , OSX , Windows Docker (version 17.06 or above, check with docker --version ) Step 1: Retrieve an authorisation token Navigate to the Connected Identities page: https://hub.acp.homeoffice.gov.uk/identities Under Amazon ECR you will have access to your own personal IAM Credentials. These credentials will work across multiple projects whose Repositories you have been granted access to. With the AWS IAM Credentials retrieved from the Connected Identities page, setup a local IAM Profile via the Terminal: $ aws configure --profile acp-ecr AWS Access Key ID [None]: XXXXXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Default region name [None]: eu-west-2 Default output format [None]: json $ export AWS_PROFILE=acp-ecr Now, using the aws-cli you can request an authorisation token to perform a docker login: $ aws ecr get-login --no-include-email docker login -u AWS -p long-auth-token https://340268328991.dkr.ecr.eu-west-2.amazonaws.com Step 2: Login with Authorisation Token Following a successful ecr get-login , a full docker login command should be returned. Copy and paste the command exactly, to login to the ECR endpoint: $ docker login -u AWS -p long-auth-token https://340268328991.dkr.ecr.eu-west-2.amazonaws.com WARNING! Using --password via the CLI is insecure. Use --password-stdin. Login Succeeded Note: If you get an error from Step 1 such as Unknown options: --no-include-email , your aws-cli client needs updating. You can omit --no-include-email rather than updating your aws-cli client, but the resulting docker login command will include a deprecated -e none flag (needs to be removed prior to running the command). Pulling Pushing Images Within the ACP Kubernetes Clusters, you do not need to provide an imagePullSecret as was previously required for images in Artifactory. The ACP Clusters will authenticate behind-the-scenes and be able to successfully pull images from any Docker Repositories you create via the Platform Hub. The Docker Repositories section of the Platform Hub will provide a URL such as follows for the Repository you have created: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app Now that you have locally authenticated with AWS ECR, you can pull and push (if write access was granted) images as normal: $ docker build . -t 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 Sending build context to Docker daemon 32.78MB ... Successfully built 882e2cadb649 Successfully tagged 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 $ docker push 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1 The push refers to repository [340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app] afbe4b47c182: Pushed 78147c906fce: Pushed 86177d14466d: Pushed f55514f6bd18: Pushed ce74984572d7: Pushed 67d7e5db87ee: Pushed 12d012372115: Pushed b0bb54920d03: Pushed 835c2760f26b: Pushed e9bcacee1741: Pushed cd7100a72410: Pushed v0.0.1: digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f size: 2628 $ docker pull 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f: Pulling from acp/hello-world-app Digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Status: Image is up to date for 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f Managing Image Deployments via Drone CI The Docker Authorisation Token generated via the aws-cli command is only valid for 12 hours, and so this can't be used as a Drone Secret for Docker Image builds. Instead, you would need to store the IAM Credentials for a Robot Account as Drone Secrets and perform the aws ecr get-login + docker login .. step on each build. To simplify this process you can use a custom Drone ECR plugin, which: - Builds a docker image in the root repository directory, with custom build arguments passed in (optional) - Authenticates to ECR using your AWS IAM credentials (stored as Drone Secrets) - Pushes the image to ECR with the given tags in the list (latest and commit sha) Example Pipeline: pipeline: build_push_to_ecr: image: quay.io/ukhomeofficedigital/ecr:latest secrets: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY repo: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app build_args: - APP_BUILD=${DRONE_COMMIT_SHA} tags: - latest - ${DRONE_COMMIT_SHA} The UKHomeOffice ECR image above is based off the official Docker ECR Plugin , with amendments to run in ACP Drone CI.","title":"Elastic container registry"},{"location":"how-to-docs/elastic-container-registry.html#aws-ecr-for-private-docker-images","text":"AWS ECR (Elastic Container Registry) is now available as a self-service feature via the Platform Hub. Each project has the capability to create their own Docker Repositories and define individual access to each via the use of IAM Credentials.","title":"AWS ECR for Private Docker Images"},{"location":"how-to-docs/ingress.html","text":"Using Ingress # An Ingress is a type of Kubernetes resource that allows you to expose your services outside the cluster. It gets deployed and managed exactly like other Kube resources. Our ingress setup offers two different ingresses based on how restrictively you want to expose your services: - internal - only people within the VPN can access services - external - anyone with internet access can access services The annotation kubernetes.io/ingress.class: \"nginx-internal\" is used to specify whether the ingress is internal. ( kubernetes.io/ingress.class: \"nginx-external\" is used for an external ingress.) In the following example the terms \"myapp\" and \"myproject\" have been used, these will need to be changed to the relevant names for your project. Where internal is used, this can be changed for an external ingress - everything else stays the same. apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # used to select which ingress this resource should be configured on kubernetes.io/ingress.class: nginx-internal # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated. ingress.kubernetes.io/secure-backends: true # This replaces the old annotation secure-backends ingress.kubernetes.io/backend-protocol: HTTPS name: myapp-server-internal spec: rules: - host: myapp.myproject.homeoffice.gov.uk http: paths: - backend: serviceName: myapp servicePort: 8000 path: / tls: - hosts: - myapp.myproject.homeoffice.gov.uk # the name of the kubernetes secret in your namespace with tls.crt and tls.key secretName: myapp-github-internal-tls Always ensure you are using TLS between the ingress controller and your pods by placing the annotation: ingress.kubernetes.io/backend-protocol: \"HTTPS\". At the moment; this is not enforced though there are plans to enforce by policy at a later date.","title":"Ingress"},{"location":"how-to-docs/ingress.html#using-ingress","text":"An Ingress is a type of Kubernetes resource that allows you to expose your services outside the cluster. It gets deployed and managed exactly like other Kube resources. Our ingress setup offers two different ingresses based on how restrictively you want to expose your services: - internal - only people within the VPN can access services - external - anyone with internet access can access services The annotation kubernetes.io/ingress.class: \"nginx-internal\" is used to specify whether the ingress is internal. ( kubernetes.io/ingress.class: \"nginx-external\" is used for an external ingress.) In the following example the terms \"myapp\" and \"myproject\" have been used, these will need to be changed to the relevant names for your project. Where internal is used, this can be changed for an external ingress - everything else stays the same. apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # used to select which ingress this resource should be configured on kubernetes.io/ingress.class: nginx-internal # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated. ingress.kubernetes.io/secure-backends: true # This replaces the old annotation secure-backends ingress.kubernetes.io/backend-protocol: HTTPS name: myapp-server-internal spec: rules: - host: myapp.myproject.homeoffice.gov.uk http: paths: - backend: serviceName: myapp servicePort: 8000 path: / tls: - hosts: - myapp.myproject.homeoffice.gov.uk # the name of the kubernetes secret in your namespace with tls.crt and tls.key secretName: myapp-github-internal-tls Always ensure you are using TLS between the ingress controller and your pods by placing the annotation: ingress.kubernetes.io/backend-protocol: \"HTTPS\". At the moment; this is not enforced though there are plans to enforce by policy at a later date.","title":"Using Ingress"},{"location":"how-to-docs/kube-cert-manager.html","text":"Kube-Cert-Managaer Cloudflare SSL # Services: - kube-cert-manager is used to retrieve Letencrypt certificates. - cfssl is an internal certificate service used to provide internal tls between pods / services. Domains and Challenge types At present two Let's Encrypt challenge types are supported for certificates which is controlled via the stable.k8s.psg.io/kcm.provider annotation on the Ingress resource; note if no annotation is present the default is stable.k8s.psg.io/kcm.provider: route53 . route53: the domain must be hosted within the ACP route53 account, namely to allow kube-cert-manager to add the service record. If you are unsure if this is the case please check with the ACP team. DNS is optional for external sites but a requirement for sites seated behind the VPN. http: indicates a callback url for authentication. The domain can either be controlled externally via yourself or via the ACP team. Either way the dns record must be a CNAME to the external ingress hostname (please check with the ACP team if you dont know) . Obviously this challenge type can only be used on an external site. Note any IP white-listing on the ingress can still be used. As a developer I want to grab a certificate from Letsencrypt Below is an example of an ingress resource for a HTTPS host reachable over the internet. Note, your ingress resource using IP whitelisting is irrelivant here in regard to Letsencrypt i.e you can still protect the site with and ACL. The host my-app.my-project.homeoffice.gov.uk has a CNAME record to ingress-external.prod.acp.homeoffice.gov.uk (or relevant ingress-external address for the cluster you are using) apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated. ingress.kubernetes.io/secure-backends: true # This replaces the old annotation secure-backends ingress.kubernetes.io/backend-protocol: HTTPS kubernetes.io/ingress.class: nginx-external # ensure kube-cert-manager uses a http01 challenge stable.k8s.psg.io/kcm.provider: http labels: # this is a toggle to indicate kube-cert-manager should handle this resource stable.k8s.psg.io/kcm.class: default name: my-app spec: rules: - host: my-app.my-project.homeoffice.gov.uk http: paths: - backend: serviceName: my-app servicePort: 443 path: / tls: - hosts: - my-app.my-project.homeoffice.gov.uk secretName: my-app-external-tls As a developer I want a certificate from a site behind the vpn Using Letsencrypt Below is an example of an ingress resource for a HTTPS host reachable via VPN or services on the private network. The host my-app.my-project.homeoffice.gov.uk has a CNAME record to ingress-internal.prod.acp.homeoffice.gov.uk (or relevant ingress-internal address for the cluster you are using) The host my-app.my-project.homeoffice.gov.uk is managed via Route53 within the same account that the cluster is running within (done by ACP team) The host my-app.my-project.homeoffice.gov.uk (or TLD) is listed as a Hosted Domain within the custom policy admission controller (done by ACP team) apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated. ingress.kubernetes.io/secure-backends: true # This replaces the old annotation secure-backends ingress.kubernetes.io/backend-protocol: HTTPS kubernetes.io/ingress.class: nginx-internal labels: stable.k8s.psg.io/kcm.class: default name: my-app spec: rules: - host: my-app.my-project.homeoffice.gov.uk http: paths: - backend: serviceName: my-app servicePort: 443 path: / tls: - hosts: - my-app.my-project.homeoffice.gov.uk secretName: my-app-internal-tls Using LetsEncrypt with Ingress Assuming you are not bringing your own certificates, LetsEncrypt can be used to acquire certificates for both internal (behind vpn NOT cluster TLS certs) and external certificates. Simply place the annotation stable.k8s.psg.io/kcm.class: default into the ingress resource; A full list of the supported features can be found here . Note at present we ONLY allow you to request certificates via the ingress resource, not by the third party resource. As a developer I want a certificate for my pod / service The CA bundle By default, in all namespaces a CA bundle has been added which can been mounted into the /etc/ssl/certs of the container and which contains the root CA used to verify authenticity of the certificates. An example of using it is given below. Below is example of how to acquire a certificate from CloudflareSSL. apiVersion: extensions/v1beta1 kind: Deployment metadata: name: example spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: example spec: volumes: - name: bundle configMap: name: bundle - name: certs emptyDir: {} initContainers: - name: certs # PLEASE do not use latest, but check for the latest tag in the releases page of https://github.com/UKHomeOffice/cfssl-sidekick image: quay.io/ukhomeofficedigital/cfssl-sidekick:latest securityContext: runAsNonRoot: true args: - --certs=/certs - --domain=myservice.${KUBE_NAMESPACE}.svc.cluster.local - --domain=another_domain_name - --expiry=8760h env: - name: KUBE_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: # an emptyDir which the sidekicks writes the certificates - name: certs mountPath: /certs # The platform CA Bundle hold the root ca used to verify the certificate chain - name: bundle mountPath: /etc/ssl/certs readOnly: true containers: - name: your_application image: quay.io/ukhomeofficedigital/some_image ... ports: - name: https port: 443 targetPort: 443 volumeMounts: # You can configure your application to pick up the certificates from here (tls.pem and tls-key.pem) - name: certs mountPath: /certs readOnly: true To break down what is happening. Firstly we are adding two volumes bundle and certs . the bundle volume is mapped to a configmap which as indicated above is published by us into every namespace and contains the a certificate bundle. This is mounted into the default PKI directory of the container /etc/ssl/certs and permits the container to trust the service. the certs is a emptyDir which is a tmpfs volume and used to share the cecertificates between the sidekick and your application. We then inject into the initContainers the sidekick service. The sidekick is responsible for locally generating a private key (the private key itself never crosses the wire) generating a CSR for the certificate and requesting a signing from cloudflare service. once the certificate has been signed its placed into --certs=dir directory which in this case is the emptyDir shared across the containers. Note, if you wish to trust certificates generated by this service simply mounted the bundle into the certificates dorectory.","title":"Kube cert manager"},{"location":"how-to-docs/kube-cert-manager.html#kube-cert-managaer-cloudflare-ssl","text":"Services: - kube-cert-manager is used to retrieve Letencrypt certificates. - cfssl is an internal certificate service used to provide internal tls between pods / services.","title":"Kube-Cert-Managaer &amp; Cloudflare SSL"},{"location":"how-to-docs/kubernetes-robot-token.html","text":"Getting a Kubernetes Robot Token # Users Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab. Any robot tokens that have been created for that service will be listed. You can see the full token by clicking on the eye icon next to the token. If there are no robot tokens for that service, or the required one is not there, you will need to ask your project admin(s) to create a robot token. Project Admins (Creating a robot token) Log into the Platform Hub . Go to the Projects section and find your project. Click on the Services tab and find the service that requires a robot token. Go to the Kube Robot Tokens tab and click the Create a Kubernetes robot token for this service button. Select the required cluster, RBAC group(s), robot name and description for the robot token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Users who are part of the project will be able to view the token in the same place you created it (Project - Service - Kube Robot Tokens).","title":"Kubernetes robot token"},{"location":"how-to-docs/kubernetes-robot-token.html#getting-a-kubernetes-robot-token","text":"","title":"Getting a Kubernetes Robot Token"},{"location":"how-to-docs/kubernetes-user-token.html","text":"Getting a Kubernetes Token # Users Log into the Platform Hub . Go to the Projects section and find your project. On the Overview People tab, you should see a list of team members and the project admin (who will have the admin tag next to their name). Talk to your project admin and ask them to generate a user token for you. Once your token has been created, you will be able to find it in the Connected Identities section. You will need to expand the Kubernetes identity and show your full token by clicking the eye icon next to it. Project Admins (Creating a user token) Log into the Platform Hub . Go to the Projects section and find your project. Click on the Kube User Tokens tab, click Select a project team member and select the requesters name from the list. Click CREATE A NEW KUBERNETES USER TOKEN FOR THIS USER . Select the required cluster and RBAC group(s) needed for the token and click Create . An explanation of RBAC groups can be found here: RBAC Groups Once the token is created the requester should be able to see it in their Connected Identities section for use in their Kube config. Note: Tokens can take a while to propagate so you may have to wait for up to 10 minutes before using a new token.","title":"Kubernetes user token"},{"location":"how-to-docs/kubernetes-user-token.html#getting-a-kubernetes-token","text":"","title":"Getting a Kubernetes Token"},{"location":"how-to-docs/network-policies.html","text":"Network Policies # By default a deny-all policy is applied to every namespace in each cluster. You can however add network policies to your own projects to allow for certain connections and these will be applied on top of the default deny-all policy. Here is an example network policy for allowing a connection from the ingress-internal namespace: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-network-policy namespace: your-namespace-here spec: podSelector: matchLabels: role: artifactory ingress: - from: - namespaceSelector: matchLabels: name: ingress-internal ports: - protocol: TCP port: 443 The port number should be the same as the one that your service is listening on. Controlling Egress Traffic Kubernetes v1.8 with Calico v2.6 adds support to limit egress traffic via the use of Kubernetes Network Policies. An example of a policy document blocking ALL egress traffic for a given namespace is below: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-egress namespace: your-namespace-here spec: podSelector: matchLabels: {} policyTypes: - Egress NOTE: The above document will also prevent DNS access for all pods in the namespace. To allow DNS egress traffic via the kube-system namespace, you can apply the following Network Policy document within your namespace (which takes precedence over deny-all-egress ): apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns-access namespace: your-namespace-here spec: podSelector: matchLabels: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: name: kube-system ports: - protocol: UDP port: 53 For more information, please see the following: - Kubernetes documentation on network policies - Kubernetes advanced network policy examples","title":"Network policies"},{"location":"how-to-docs/network-policies.html#network-policies","text":"By default a deny-all policy is applied to every namespace in each cluster. You can however add network policies to your own projects to allow for certain connections and these will be applied on top of the default deny-all policy. Here is an example network policy for allowing a connection from the ingress-internal namespace: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-network-policy namespace: your-namespace-here spec: podSelector: matchLabels: role: artifactory ingress: - from: - namespaceSelector: matchLabels: name: ingress-internal ports: - protocol: TCP port: 443 The port number should be the same as the one that your service is listening on.","title":"Network Policies"},{"location":"how-to-docs/performance-testing.html","text":"Run Performance Tests on a service hosted on ACP # As a Service, I should: Always have a baseline set of metrics of my isolated service Understand what those metrics need to be for each functionality i.e. how long file uploads should take vs a generic GET request Make sure the baseline does not include any other components i.e. networks, infrastructure etc. Expose a set of metrics, see Metrics Make performance testing part of my Continuous Integration workflow Have a history of performance over time Assessed tools summary: An example usage of Blazemeter's Taurus in a drone pipeline can be seen in the taurus-project-x repo . Artillery ( npm ) was also tested w/ the statsd plugin , visualising data in grafana. SonarQube plugin jmeter-sonar is now deprecated. The latest version of sonarqube does not to have plugin support for jmeter another option is k6 - tool is written in go and tests are written in javascript. To visualise the only option is InfluxDB and Grafana.","title":"Performance testing"},{"location":"how-to-docs/performance-testing.html#run-performance-tests-on-a-service-hosted-on-acp","text":"","title":"Run Performance Tests on a service hosted on ACP"},{"location":"how-to-docs/pod-security-policies.html","text":"Pod Security Policies # By default all user deployments will inherit a default PodSecurityPolicy applied in the Kubernetes Clusters, which define a set of conditions that a pod must be configured with in order to run successfully. The specification for the default policy is as follows: apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: default spec: privileged: false fsGroup: rule: RunAsAny hostPID: false hostIPC: false hostNetwork: false runAsUser: rule: MustRunAsNonRoot seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - downwardAPI - emptyDir - gitRepo - persistentVolumeClaim - projected - secret runAsUser This condition requires that the pod specification deploys an image with a non-root user. The user defined in the specification (image spec OR pod spec) must be numeric, so that Kubernetes will be able to verify that it is a non-root user. If this is not done, you may receive any of the following errors in your event log and your pod will be prevented from starting up successfully: - container's runAsUser breaks non-root policy - container has runAsNonRoot and image will run as root - container has runAsNonRoot and image has non-numeric user username , cannot verify user is non-root Note: You can view all recent events in your namespace by running the following command: kubectl -n my-namespace get events --sort-by=.metadata.creationTimestamp . To update your deployment accordingly for the above condition, there are multiple ways to achieve this: Dockerfile Within the Dockerfile for the image you are attempting to run, ensure the USER specified references the User ID rather than the username itself. For example: FROM quay.io/gambol99/keycloak-proxy:v2.1.1 LABEL maintainer= rohith.jayawardene@digital.homeoffice.gov.uk RUN adduser -D -u 1000 keycloak USER 1000 Note: The following common images have been updated to reference the UID within their respective Dockerfiles. If you use any of these images, updating your deployments to use these versions (or any newer versions) will meet the MustRunAsNonRoot requirement for this particular container: quay.io/ukhomeofficedigital/cfssl-sidekick:v0.0.6 quay.io/ukhomeofficedigital/elasticsearch:v1.5.3 quay.io/ukhomeofficedigital/jira:v7.9.1 quay.io/ukhomeofficedigital/keycloak:v3.4.3-2 quay.io/ukhomeofficedigital/kibana:v0.4.4 quay.io/ukhomeofficedigital/go-keycloak-proxy:v2.1.1 quay.io/ukhomeofficedigital/nginx-proxy:v3.2.9 quay.io/ukhomeofficedigital/nginx-proxy-govuk:v3.2.9.0 quay.io/ukhomeofficedigital/redis:v0.1.2 quay.io/ukhomeofficedigital/squidproxy:v0.0.5 Deployment Spec In the securityContext section of your deployment spec, the runAsUser field can be used to set a UID that the image should be run as. An example spec would include: spec: securityContext: fsGroup: 1000 runAsNonRoot: true runAsUser: 1000 containers: - name: {{ .IMAGE_NAME }} image: {{ .IMAGE }}:{{ .VERSION }} ...","title":"Pod security policies"},{"location":"how-to-docs/pod-security-policies.html#pod-security-policies","text":"By default all user deployments will inherit a default PodSecurityPolicy applied in the Kubernetes Clusters, which define a set of conditions that a pod must be configured with in order to run successfully. The specification for the default policy is as follows: apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: default spec: privileged: false fsGroup: rule: RunAsAny hostPID: false hostIPC: false hostNetwork: false runAsUser: rule: MustRunAsNonRoot seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - downwardAPI - emptyDir - gitRepo - persistentVolumeClaim - projected - secret","title":"Pod Security Policies"},{"location":"how-to-docs/private-npm-registry.html","text":"Using artifactory as a private npm registry # A step-by-step guide. This guide makes the following assumptions: you have drone ci set up for your project already you are using node@8 and npm@5 or later you are connected to ACP VPN Setting up a local environment Get your username and API key from artifactory Visit https://artifactory.digital.homeoffice.gov.uk/artifactory/webapp/#/profile, make a note of your username, and if you don't already have an API key then generate one. base64 encode your API key echo -n api key | base64 Set local environment variables Copy your encoded password, and set the following environment variables in your bash profile: export NPM_AUTH_USERNAME= username export NPM_AUTH_TOKEN= base64 encoded api key You might then need to source your profile to load these environment variables. Setting up CI in drone Request a bot token for artifactory You can do this through the ACP Hub . You'll need to provide a username for the bot when you create it. One of the ACP team will create a token and send it to you as an encrypted gpg file via email. Decrypt the token gpg --decrypt ./path/to/file.gpg Add the token to drone as a secret First, base64 encode the token: echo -n token | base64 Then add this token to drone as a secret: drone secret add UKHomeOffice/ repo NPM_AUTH_TOKEN base64-encoded-token --event pull_request Note: You will need to make sure the event types are lowercase. If an event is capitalised, it won't match the standard events inside of drone Note: you will need to make the secret available to pull request builds to be able to run npm commands in pull request steps Expose secret to build steps You will need to configure any steps which use npm to be able to access the secret. Do this by adding a secret property to those steps as follows: my_step: image: node:8 secrets: - npm_auth_token commands: - npm install - npm test Expose username to build steps In addition, you will need to add the username (as you provided when creating your token) as an environment variable. The easiest way to do this is as a \"matrix\" variable, which makes the username available to all steps without needing to configure them all individually. matrix: NPM_AUTH_USERNAME: - username Publishing modules to artifactory It is generally recommended to use a common namespace to publish your modules under. npm allows namespace specific configuration, which makes it easier to ensure that modules are always installed from artifactory, and will not accidentally try to install a public module with the same name. Setting publish registry Add publishConfig to package.json. This ensures that the module can only ever be published to the private registry, and misconfiguration won't accidentally make it public publishConfig : { registry : https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/ } Add auth settings In your project's .npmrc file (create one if it does not already exist) add the following lines: //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:email=test@example.com The email address can be anything, it just needs to be set. Add publish step to drone Add the following step to your .drone.yml file to publish a new version whenever you release a tag. publish: image: node:8 secrets: - npm_auth_token commands: - npm publish when: event: tag Now, when you push new tags to github then drone should publish them to the artifactory npm registry automatically. Using modules from artifactory as dependencies Configure your project to use artifactory In the project which is has private modules as dependencies, add the following line to .npmrc in the root of the project (create this file if it does not exist). @ namespace :registry = https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/ This will ensure that any module under that namespace will only ever install from artifactory, and never from the public registry If using multiple namespaces then add a line for each namespace. If the modules you are installing are not namespaced in artifactory, you can add the line with the namespace removed (i.e. registry = ... ) but this will have a negative impact on install speed. You should then add the following line to your project's .npmrc if they are not already there: //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME} //artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN} You should now be able to install modules from artifactory into your local development environment. Installing dependencies in docker If you build a docker image as part of your CI pipeline, you will need to copy the .npmrc file into your image before installing there. Example Dockerfile : FROM quay.io/ukhomeofficedigital/nodejs-base:v8 ARG NPM_AUTH_USERNAME ARG NPM_AUTH_TOKEN COPY .npmrc /app/.npmrc COPY package.json /app/package.json COPY package-lock.json /app/package-lock.json RUN npm install --production --no-optional COPY . /app USER nodejs CMD node index.js When building the image, you will then need to pass the username and token variables into docker with the --build-arg flag. docker build --build-arg NPM_AUTH_USERNAME=$${NPM_AUTH_USERNAME} --build-arg NPM_AUTH_TOKEN=$${NPM_AUTH_TOKEN} .","title":"Private npm registry"},{"location":"how-to-docs/private-npm-registry.html#using-artifactory-as-a-private-npm-registry","text":"A step-by-step guide. This guide makes the following assumptions: you have drone ci set up for your project already you are using node@8 and npm@5 or later you are connected to ACP VPN","title":"Using artifactory as a private npm registry"},{"location":"how-to-docs/pv-and-storage-classes.html","text":"Provisioned Volumes and Storage Classes # In order to use volumes with your pod, we use kubernetes provisioned volume claims and storage classes, to read more about this please see Kubernetes Dynamic Provisioning . On each cluster in ACP, we have the the following storage classes for you to use: gp2-encrypted gp2-encrypted-eu-west-2a gp2-encrypted-eu-west-2b gp2-encrypted-eu-west-2c io1-encrypted-eu-west-2 io1-encrypted-eu-west-2a io1-encrypted-eu-west-2b io1-encrypted-eu-west-2c st1-encrypted-eu-west-2 st1-encrypted-eu-west-2a st1-encrypted-eu-west-2b st1-encrypted-eu-west-2c The io1-* (provisioned iops) storage classes have iopsPerGB: \"50\" Backups for EBS Once the ebs has been created, if you'd like to enable EBS snapshots for backups, please raise a ticket via the BAU support so that we can add AWS tags to the volume, which will be picked up by ebs-snapshot . Please remember to specify the retention policy in days to keep the snapshots for.","title":"Pv and storage classes"},{"location":"how-to-docs/pv-and-storage-classes.html#provisioned-volumes-and-storage-classes","text":"In order to use volumes with your pod, we use kubernetes provisioned volume claims and storage classes, to read more about this please see Kubernetes Dynamic Provisioning . On each cluster in ACP, we have the the following storage classes for you to use: gp2-encrypted gp2-encrypted-eu-west-2a gp2-encrypted-eu-west-2b gp2-encrypted-eu-west-2c io1-encrypted-eu-west-2 io1-encrypted-eu-west-2a io1-encrypted-eu-west-2b io1-encrypted-eu-west-2c st1-encrypted-eu-west-2 st1-encrypted-eu-west-2a st1-encrypted-eu-west-2b st1-encrypted-eu-west-2c The io1-* (provisioned iops) storage classes have iopsPerGB: \"50\"","title":"Provisioned Volumes and Storage Classes"},{"location":"how-to-docs/ssl-passthrough.html","text":"TLS Passthrough # There are occasions when you don't want the TLS to be terminated on the ingress and prefer to terminate in the pod instead. Note, by terminating in the pod the ingress will no longer be able to perform any L7 actions, so all of the feature set is lost (effectively it's become a TLS proxy using SNI to route the traffic) Example steps First create a kubernetes secret containing the certificate you wish to use. $ kubectl create secret tls tls --cert=cert.pem --key=cert-key.pem Create the deployment and service. --- apiVersion: v1 kind: Service metadata: labels: name: tls-passthrough name: tls-passthrough spec: type: ClusterIP ports: - name: https port: 443 protocol: TCP targetPort: 10443 selector: name: tls-passthrough --- --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: tls-passthrough spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: tls-passthrough spec: volumes: - name: certs secret: secretName: tls containers: - name: proxy image: quay.io/ukhomeofficedigital/nginx-proxy:v3.2.0 ports: - name: https containerPort: 10443 protocol: TCP env: - name: PROXY_SERVICE_HOST value: 127.0.0.1 - name: PROXY_SERVICE_PORT value: 8080 - name: SERVER_CERT value: /certs/tls.crt - name: SERVER_KEY value: /certs/tls.key - name: ENABLE_UUID_PARAM value: FALSE - name: NAXSI_USE_DEFAULT_RULES value: FALSE - name: PORT_IN_HOST_HEADER value: FALSE volumeMounts: - name: certs mountPath: /certs readOnly: true - name: fake-application image: kennethreitz/httpbin:latest Push out the ingress resource indicating you want ssl-passthrough enabled. --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/ssl-passthrough: true kubernetes.io/ingress.class: nginx-external name: tls-passthrough spec: rules: - host: tls-passthrough.notprod.homeoffice.gov.uk http: paths: - backend: serviceName: tls-passthrough servicePort: 443 path: /","title":"Ssl passthrough"},{"location":"how-to-docs/ssl-passthrough.html#tls-passthrough","text":"There are occasions when you don't want the TLS to be terminated on the ingress and prefer to terminate in the pod instead. Note, by terminating in the pod the ingress will no longer be able to perform any L7 actions, so all of the feature set is lost (effectively it's become a TLS proxy using SNI to route the traffic)","title":"TLS Passthrough"},{"location":"how-to-docs/write-dockerfiles.html","text":"Writing Dockerfiles # Dockerfile best practice We recommend using dockers excellent guidance for this! https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/ Docker images to build from This document lists all of our docker base images that you can build from: Technology specific images NodeJS onbuild image - recommended by default NodeJS base image - if you need more flexibility Scala image JDK image Maven image with Java 8 Ruby image Home Office CentOS base image If none of the technology specific images work for you you can either build on top of them or build from the base centos image: https://github.com/UKHomeOffice/docker-centos-base If you build an image that will be of use to other teams then please add it to the list of technology specific images above! And please make sure it adheres to the below guidance on building new base images. Guidance on building new base images All base images should be built with a set of onbuild commands in them to make sure anything built on top of them will automatically update the base OS, for example: yum install -y curl yum clean all rpm --rebuilddb The nodejs base image is a good example of this.","title":"Write dockerfiles"},{"location":"how-to-docs/write-dockerfiles.html#writing-dockerfiles","text":"","title":"Writing Dockerfiles"},{"location":"releases-notes/index.html","text":"This page contains release notes of various components on ACP. Artifactory ELK Stack GitLab Ingress Keycloak Kubernetes Platform SonarQube Sysdig Artifactory # v6.5.13 Update to Artifactory v6.5.13: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory6.5.13 v6.5.9 Update to Artifactory v6.5.9: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory6.5.9 v6.2.0 Update to Artifactory v6.2.0: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory6.2.0 v5.9.3 Update to Artifactory v5.9.3: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.9.3 v5.9.0 Update to Artifactory v5.9.0: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.9.0 v5.8.4 Update to Artifactory v5.8.4: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.8.4 v5.7.2 Update to Artifactory v5.7.2: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.7.2 v5.4.6 Update to Artifactory v5.4.6: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.4.6 Use the Artifactory docker image supplied by JFrog Change configmap to secrets v5.4.4 Configuration update to cycle the certificates used by artifactory ELK Stack # v0.4.3 Enables Watcher SMTP Enables PDF export for reporting Improvements to Elasticsearch monitoring ELK components run with set UID v0.4.1 Enables X-Pack RBAC v0.3.3 Enabled X-Pack Enabled Kibana user authentication Enabled Elasticsearch cluster TLS Enabled secure logs shipping Enabled security audit logging GitLab # You can find full Gitlab Release changelog here . v11.4.7 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1147-2018-11-20 v11.1.4 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1114-2018-07-30 v10.8.3 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1083-2018-05-30 v10.4.2 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1042-2018-01-30 v10.2.5 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1025-2017-12-15 v9.4.5 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#945-2017-08-14 v9.4.3 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#943-2017-07-31 Ingress # 0.21.0 Update to Ingress-nginx 0.21.0: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#0210 This includes a breaking change to the ingress.kubernetes.io/secure-backends: \"true\" annotation, it has been deprecated and ingress.kubernetes.io/backend-protocol: \"HTTPS\" introduced. Whilst we upgrade, we recommend having both in place. 0.15.0 Update to Ingress-nginx 0.15.0: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#0150 0.13.0 Update to Ingress-nginx 0.13.0: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#0130 Add kube namespace to log messages 0.11.0 Update to Ingress-nginx 0.11.0: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#0110 0.9-beta-2 Update to Ingress-nginx 0.9-beta-2: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#09-beta2 Keycloak # Keycloak v4.5.0 Enhancement [KEYCLOAK-4622] - Use HS256 for refresh tokens [KEYCLOAK-4623] - Use HS256 for client registration tokens [KEYCLOAK-7270] - First Broker Login Link Without Authentication [KEYCLOAK-7604] - SAML test clients have confusing names/entityIDs [KEYCLOAK-8010] - Improve handling of SAML tag [KEYCLOAK-8072] - Investigate if we can drop server-openshift [KEYCLOAK-8265] - Document the default admin credentials when running Test Suite Feature Request [KEYCLOAK-6229] - Implement OpenShift Token Review Interface [KEYCLOAK-6285] - WWW-Authenticate challenge flow for 'oc login' [KEYCLOAK-6770] - JWS signatures using ES256 algorithms for signing [KEYCLOAK-7061] - Keycloak Docker image should provide a way to import realm files [KEYCLOAK-7435] - Add logout specific session to session endpoints in account service [KEYCLOAK-7560] - Token Signature SPI [KEYCLOAK-8146] - Add LocaleSelectorSPI to allow custom locale selection [KEYCLOAK-8263] - Allow overriding token expiration time on a client [KEYCLOAK-8264] - Updates to Token Review endpoint [KEYCLOAK-8273] - Failed to evaluate permissions when in permissive mode and using UMA tickets [KEYCLOAK-8416] - Add support for configuring welcome theme in docker image Epic [KEYCLOAK-8121] - Add support for additional token algorithms Bug [KEYCLOAK-3058] - Validation of \"aud\" in adapters [KEYCLOAK-6468] - AnonymousAuthenticationToken cannot be cast to KeycloakAuthenticationToken [KEYCLOAK-6962] - No documentation for rpms upgrade [KEYCLOAK-6971] - Requesting a token for same user multiple times concurrently causes 401 errors [KEYCLOAK-7608] - FolderThemeProvider throws NPE when the themes are located on the classpath [KEYCLOAK-7661] - Keycloak does not build on windows environment due \u201ctestAllTheZipThings\u201d failure [KEYCLOAK-7987] - Can't set authorization enabled when using kcreg [KEYCLOAK-8016] - Broken new Account Console in Chrome [KEYCLOAK-8070] - wrong expose headers when enable cors and policyenforcer [KEYCLOAK-8133] - Can't Sucessfully inject a custom KeycloakSpringBootConfigResolver in the Keycloak Spring Boot Security Adapter [KEYCLOAK-8168] - PEP is resolving claims twice under certain circumstances [KEYCLOAK-8180] - Welcome Screen sporadically fails to load in Firefox [KEYCLOAK-8189] - Broken Google Social Login test [KEYCLOAK-8190] - Cannot compile Node.js adapter UI tests [KEYCLOAK-8222] - Performance issue when loading user list [KEYCLOAK-8270] - Cannot remove \"Not Before\" or \"Not On or After\" from a Time Policy [KEYCLOAK-8274] - Realm export \"uma_protection\" role not reproducable [KEYCLOAK-8281] - Deletion of client and users with token exchange policy leads to breaking errors [KEYCLOAK-8308] - Deprecate token_introspection_endpoint claim from OIDC discovery document [KEYCLOAK-8315] - Completely broken Node.js adapter [KEYCLOAK-8327] - Token Introspect Test for Refresh Token Mistake [KEYCLOAK-8378] - Infinispan version mismatch in prod-arguments [KEYCLOAK-8425] - AdapterExecutionDecider throws NPE for cluster tests [KEYCLOAK-8440] - QuickStart does not build straight forward from GitHub clone Story [KEYCLOAK-8122] - Long lived tokens for OpenShift Task [KEYCLOAK-6332] - Switch Firefox driver to GeckoDriver once Firefox ESR 52.x is not supported [KEYCLOAK-6638] - Audience support for Client Scopes [KEYCLOAK-7582] - Clustering support to Keycloak server image [KEYCLOAK-7742] - Investigate failing Admin Console UI tests [KEYCLOAK-7813] - Coordinate the transfer or keycloak-proxy repository to Keycloak organization [KEYCLOAK-7915] - Implement trigger of Test pipeline using UMB [KEYCLOAK-8024] - Install JMS Messaging plugin to keycloak-jenkins [KEYCLOAK-8057] - Investigate React Framework [KEYCLOAK-8206] - Use HS256 for action tokens [KEYCLOAK-8207] - Use HS256 for cookie tokens [KEYCLOAK-8211] - Use HS256 for authorization permission tickets [KEYCLOAK-8236] - Bot command for executing tests (test command) can be without additional paramters [KEYCLOAK-8238] - Create required clients in OpenShift integration script [KEYCLOAK-8285] - Remove user-storage-jpa and user-storage-simple from Keycloak repository [KEYCLOAK-8289] - Remove authorization services from product preview profile [KEYCLOAK-8297] - Docs for audience [KEYCLOAK-8319] - Initial pipeline for product test * Keycloak v4.4.0 Enhancement [KEYCLOAK-7161] - Refactor PhotozAuthz tests to not use hard waits [KEYCLOAK-7584] - Doublecheck admin console tests passing after clientScopes added [KEYCLOAK-7594] - Fix cross-dc integration and update Cross-dc integration to ISPN server 9.2.4 and JDG 7.2 [KEYCLOAK-7595] - Upgrade aesh to 1.4 [KEYCLOAK-7596] - Doublecheck migration scripts and all tests pass for Wildfly 13 [KEYCLOAK-7858] - OIDC servlet filter adapter OSGi support [KEYCLOAK-7991] - Role-Resource in the Admin-API-Client is missing pagination params [KEYCLOAK-8054] - Use BOMREST property in product config files [KEYCLOAK-8069] - Fixed hostname provider should read ports from request [KEYCLOAK-8075] - Provide a better error handling for Express when access is denied [KEYCLOAK-8076] - Remove deprecation warning messages from Node.js adapter [KEYCLOAK-8134] - uma-ticket rpt token endpoint does not evaluate policies without permission value(s) [KEYCLOAK-8241] - Use BOMREST property in nodejs-connect product config * Feature Request [KEYCLOAK-3334] - Enable authorization services to Node.js adapter [KEYCLOAK-5520] - Script based ProtocolMapper for SAML [KEYCLOAK-7222] - forgot password redirect to account client [KEYCLOAK-7751] - Auth welcome page - HTML code [KEYCLOAK-7891] - Configure SSL certificates in Docker image [KEYCLOAK-8056] - Configure hostname in Docker image [KEYCLOAK-8145] - Configure keystore when running KeycloakServer using TLS [KEYCLOAK-8147] - Support Content-Security-Policy-Report-Only security header [KEYCLOAK-8152] - Allow passing current locale to OAuth2 IdPs Epic [KEYCLOAK-6275] - Improve usability of xPaaS images [KEYCLOAK-6781] - Upgrade to WildFly 13 Final Bug [KEYCLOAK-5226] - Travis builds fail for default trusty image [KEYCLOAK-6086] - Spring boot app doesn't start with Keycloak and Jetty combination [KEYCLOAK-6411] - Unable to provision database on MariaDB 10.3.4+ [KEYCLOAK-6706] - E-mail verification won't let user back into the app [KEYCLOAK-6803] - Sign-up error with custom UserStorageProvider [KEYCLOAK-7087] - Calling admin routes without data crashes NodeJS app [KEYCLOAK-7160] - Completely broken AuthZ Configs in IE [KEYCLOAK-7243] - Link are broken in documentaion [KEYCLOAK-7394] - LDAP Connections can't be tested using UI [KEYCLOAK-7695] - Authorization response with implicit flow should contain token_type and expires_in [KEYCLOAK-7731] - KeycloakUriBuilder removes well known ports (80 / 443) even if the issuer url in the token includes it [KEYCLOAK-7752] - CLIENT_INITIATED_ACCOUNT_LINKING_ERROR with invalid_token [KEYCLOAK-7754] - UMA token introspection endpoint does not conform to specification [KEYCLOAK-7757] - Deleting scopes in account console doesn't work [KEYCLOAK-7782] - keycloak Groups click view all can't query out [KEYCLOAK-7797] - Misplaced question marks [KEYCLOAK-7911] - Wrong version of Node.js adapters - Final missing [KEYCLOAK-7943] - NullPointerException when SAML User Property mapper is empty/blank [KEYCLOAK-7944] - Fail to create a primary key on (USER_SESSION_ID, CLIENT_ID, OFFLINE_FLAG) when migrating to 3.4.3 [KEYCLOAK-7970] - Fresh authSession not started for correct client when handling action tokens [KEYCLOAK-7975] - updating execution with Oracle DB causes unique constraint (DBALLO00.CONSTRAINT_AUTH_CFG_PK) violated [KEYCLOAK-8012] - Unable to issue RPT token after server restart - NullPointerException [KEYCLOAK-8068] - IP access control broken in undertow current version [KEYCLOAK-8073] - cli unusable on FreeBSD [KEYCLOAK-8080] - Update Realm Events Config doesn't create an admin event [KEYCLOAK-8095] - Keycloak \"SSSD\" user federation option doesn't shows up on the drop down menu [KEYCLOAK-8101] - NPE in PathBasedKeycloakConfigResolver when cache is empty [KEYCLOAK-8109] - StaxParserUtil.getDOMElement can return incomplete tree [KEYCLOAK-8119] - Migration tests doesn't reflect if some features are not enabled [KEYCLOAK-8120] - NullPointerException in ClaimInformationPointProviderTest when product profile is used [KEYCLOAK-8132] - Missing type definitions for parsed tokens in Keycloak Javascript Typescript declarations. [KEYCLOAK-8138] - Several tests failing in product profile [KEYCLOAK-8142] - Regression in proper handling of public endpoints in AbstractPolicyEnforcer [KEYCLOAK-8150] - Performance regression between 3.4.3.Final and 4.3.0.Final [KEYCLOAK-8178] - AdminEventTest.testGetRepresentation failing with many databases (mysql, postresql, mariadb, oracle)) [KEYCLOAK-8183] - Database is repeatedly queried for resource server if client has authz not enabled [KEYCLOAK-8218] - BaseSAML2BindingBuilder cleans URL parameters in a SAML provider * Task [KEYCLOAK-6743] - Adapter tests - update the server maintanance according to e.g. test annotation (cluster, crossdc) [KEYCLOAK-6746] - Adapter tests - execute (or skip) particular tests based on app server-container [KEYCLOAK-6974] - Update to Infinispan 9 [KEYCLOAK-6975] - Update configuration files for WF 13 [KEYCLOAK-6976] - Refactor to accommodate removed jndi-name in cache-container [KEYCLOAK-7231] - Add Example adapter tests for app-server-undertow (without Photoz test class) [KEYCLOAK-7437] - Support for non-persistent consents through prompt=consent [KEYCLOAK-7470] - Client scope ordering capability [KEYCLOAK-7488] - Keycloak OpenShift template(s) [KEYCLOAK-7770] - Instabilities in the keycloak-pipeline [KEYCLOAK-7925] - Prepare test suite for New Acct Mgt Console Tests [KEYCLOAK-8052] - Change Travis job from Node.js adapter to always build the latest changes from Keycloak server [KEYCLOAK-8067] - Provide an up to date Docker image for the Keycloak server [KEYCLOAK-8093] - Add IDs to HTML elements for better testability [KEYCLOAK-8103] - Remove deprecated images [KEYCLOAK-8116] - Change Travis job from Node.js adapter to use Docker [KEYCLOAK-8127] - Make sure adapter tests are passing and correct adapters tested [KEYCLOAK-8139] - Align tests, documentation and downloads with supported Wildfly adapters [KEYCLOAK-8170] - Add build description to the pipeline [KEYCLOAK-8171] - Limit time execution of the keycloak test pipeline [KEYCLOAK-8174] - Admin console tests for the \"Client Scopes\" tab of the client [KEYCLOAK-8188] - Add ability to exclude failing or unstable tests from pipeline execution [KEYCLOAK-8246] - Disable Base UI tests by default Keycloak v4.3.0 Enhancement [KEYCLOAK-5289] - support hosted domain for Google logins Feature Request [KEYCLOAK-7635] - Authenticate clients with x509 certificate [KEYCLOAK-7967] - Hostname SPI Epic [KEYCLOAK-5522] - Add support for Fuse 7 Bug [KEYCLOAK-6818] - Keycloak creates an extra AUTH_SESSION_ID cookie with a path of \u201c/auth\u201d when logging in [KEYCLOAK-6832] - Destination Validation should ignore whether default port is explicitly specified [KEYCLOAK-7528] - Token endpoint doesn't set Cache-Control and Pragma header [KEYCLOAK-7562] - ClientInitiatedAccountLinkTest#testErrorConditions fails [KEYCLOAK-7946] - Variable rather than intended value showing in RH-SSO doc [KEYCLOAK-7954] - OIDC Provider doesn't skip token validation if URL is empty [KEYCLOAK-7974] - Fix typo in remove credentials alert [KEYCLOAK-7984] - RequiredActionProviderEntity priority migration issue [KEYCLOAK-7985] - Version migration the database table names do not match [KEYCLOAK-7986] - Migration test fails for migration from 3.4.3.Final [KEYCLOAK-7988] - New keycloak-bot commands convention is not mentioned in README.md [KEYCLOAK-7989] - Running server config migration fails due the Hostname SPI [KEYCLOAK-7994] - Move Fuse examples into test-apps [KEYCLOAK-8002] - Cannot build new Account Console [KEYCLOAK-8003] - Migration to 4.2.1 extracting RESOURCE_URIs fails with fine-grained admin permissions [KEYCLOAK-8007] - Cannot compile Console UI and Welcome Page tests [KEYCLOAK-8015] - Migration into 4.2.1.Final fails from version 3.4.3.Final [KEYCLOAK-8035] - Failing GitLab Social Login test [KEYCLOAK-8036] - Misplaced IdPs buttons on the Login Page [KEYCLOAK-8046] - X509 Client Authenticator sends Client entity twice [KEYCLOAK-8048] - Testsuite does not compile due to cross-PR interference Keycloak v4.2.1 Bug [KEYCLOAK-7984] - RequiredActionProviderEntity priority migration issue [KEYCLOAK-7985] - Version migration the database table names do not match [KEYCLOAK-7986] - Migration test fails for migration from 3.4.3.Final [KEYCLOAK-7998] - File based realm export breaks after migration from Keycloak 3.4.3 to 4.2.1 Keycloak v4.2.0 Enhancement2 [KEYCLOAK-4407] - Ability to restart arquillian containers from test [KEYCLOAK-5609] - An option to create claims with dots (.) in them [KEYCLOAK-6577] - Unable to map claim attributes with dots (.) in them [KEYCLOAK-7703] - PathBasedKeycloakConfigResolver - more generic behavior [KEYCLOAK-7792] - Always preserve URL fragment in redirectUri [KEYCLOAK-7876] - Improve stability of fuse7 hawtio test [KEYCLOAK-7924] - Speed up cross-dc tests [KEYCLOAK-7959] - OAuth 2.0 Certificate Bound Access Tokens in Reverse Proxy Deployed Environment [KEYCLOAK-7973] - Possibility to add classpath elements to KeycloakServer Feature Request [KEYCLOAK-1925] - SAML adapter multitenant support [KEYCLOAK-2606] - In-app browser tab support for Cordova [KEYCLOAK-5629] - Add credential endpoints to account service [KEYCLOAK-6313] - Changing execution order of required actions easily feature [KEYCLOAK-7105] - Notification needs to be lower [KEYCLOAK-7201] - OIDC Identity Brokering with Client parameter forward [KEYCLOAK-7294] - Password Page - Angular [KEYCLOAK-7846] - Turn off disallowed features * Epic [KEYCLOAK-6176] - Stable and reliable CI that can be used to test PRs, master and releases Bug [KEYCLOAK-2886] - Cluster tests fail when running from IDE [KEYCLOAK-4662] - Keycloak adapter missing configuration attribute \"proxy-url\" [KEYCLOAK-6308] - Disable 'secret question credentials' fails [KEYCLOAK-6314] - After a terms conditions rejection, an \"internal server error has ocurred\" happens [KEYCLOAK-6708] - NullPointerException when integrating with IDP that returns a SAML XML that does not contain the fields Keycloak expects by default [KEYCLOAK-6866] - Error 404 after changing locale while authenticating using X.509 [KEYCLOAK-7497] - Remove Babel transpiler [KEYCLOAK-7524] - Vertical Nav Doesn't close on secondary click [KEYCLOAK-7663] - Deleting identity provider does not delete it's mappers [KEYCLOAK-7795] - \"Back to \" missing from Welcome Page [KEYCLOAK-7802] - Broken HoKTest [KEYCLOAK-7805] - Broken PayPal and Bitbucket Social Login tests [KEYCLOAK-7816] - Tech preview features (like authz) are run by default [KEYCLOAK-7823] - Keycloak returns wrong HTTP status during SPNEGO authentication [KEYCLOAK-7840] - Secret in keycloak.json and client-import.json doesn't match [KEYCLOAK-7860] - proxy-address-forwarding option is not added to https listener in Docker image [KEYCLOAK-7872] - Doesn't remove Identity Provider Mapper after removing identity provider [KEYCLOAK-7881] - Docker image jboss/keycloak doesn't contain jq [KEYCLOAK-7913] - Invalid naming of JPA changelog files [KEYCLOAK-7934] - Dataset generator cannot create \"empty\" mappings [KEYCLOAK-7965] - Redundant div end tag in base theme login.ftl [KEYCLOAK-7977] - Release failing due the NPE during swagger2markup-maven-plugin execution Task [KEYCLOAK-7101] - Investigate failing Social Login Tests [KEYCLOAK-7269] - [SPIKE] - Investigate how to support resource-less permissions [KEYCLOAK-7310] - Add migration test from 3.4.x to 4.x [KEYCLOAK-7328] - Test the RH-SSO 7.2.z EAP 7 adapter with early builds of EAP 7.2.0 [KEYCLOAK-7329] - Test the RH-SSO 7.3.0 EAP 7 adapter with early builds of EAP 7.2.0 [KEYCLOAK-7400] - Update Camel / Fuse 7 adapter once CAMEL-12514 is merged to Fuse's Camel component [KEYCLOAK-7498] - Remove unused components. [KEYCLOAK-7599] - Improve handling of test datasets [KEYCLOAK-7620] - Generating performance datasets for authorization services [KEYCLOAK-7666] - Adapter tests - add dynamically loaded container - remove abstract classes - EAP6-fuse6 [KEYCLOAK-7817] - Update eap6.version in testsuite [KEYCLOAK-7857] - Fix Notifications - Switch to pf-ng notifications [KEYCLOAK-7888] - Update Fuse adapter examples/guide to new way of CXF servlet registration Keycloak v4.1.0 Enhancement [KEYCLOAK-3063] - Check if keycloak-osgi-thirdparty can be removed [KEYCLOAK-3370] - Choose Theme by Client [KEYCLOAK-4937] - Convert time units in emails into human-friendly format [KEYCLOAK-5166] - Setup CI jobs to test RH-SSO quickstarts [KEYCLOAK-5578] - The javascript adapter should use native Promises [KEYCLOAK-5791] - Add support for multi-valued attributes in ScriptBasedOIDCProtocolMapper [KEYCLOAK-5811] - OIDC Client Authentication by JWS Client Assertion in client_secret_jwt [KEYCLOAK-5857] - Support for PBKDF2 hashes with different key size [KEYCLOAK-5886] - Improvements to Photoz Examples [KEYCLOAK-6085] - Allow to customize DB dump download location through a Maven property [KEYCLOAK-6222] - Check for script syntax errors on ScriptBasedOIDCProtocolMapper validation [KEYCLOAK-6262] - Incorporate new visual design for login pages [KEYCLOAK-6298] - SAML adapter script should support offline installation of adapter [KEYCLOAK-6299] - Product profile should only include supported JavaDocs [KEYCLOAK-6302] - Add support for user defined networks and Docker compose [KEYCLOAK-6330] - GitHub social IdP: Use GitHub API for fetching user's private email if no public email is set [KEYCLOAK-6335] - Per client authentication flows [KEYCLOAK-6336] - Per client authentication flows configuration in admin console [KEYCLOAK-6339] - Show SAML IdP-initiated client URI [KEYCLOAK-6350] - Refactor SAML parsers [KEYCLOAK-6355] - Non-browser multi-request cookieless auth flow support [KEYCLOAK-6378] - Clean-up node_modules in themes [KEYCLOAK-6493] - [SPIKE] Investigate architecture of new account management console [KEYCLOAK-6561] - Add account management and update profile to js-console example [KEYCLOAK-6578] - Support other OIDC providers with keycloak.js [KEYCLOAK-6589] - Performance issues in Users REST API [KEYCLOAK-6618] - Update German Translation [KEYCLOAK-6664] - Fix performance testsuite shell scripts to run on macOS [KEYCLOAK-6700] - Financial API Read and Write API Security Profile : State hash value (s_hash) to protect state parameter [KEYCLOAK-6871] - Make sending a request object mandatory for certain clients [KEYCLOAK-4134] - Update paths in adapters when new resources are created on the server [KEYCLOAK-5457] - Context accessibility for JS base policies [KEYCLOAK-5830] - Automated stress test [KEYCLOAK-6448] - Instagram social broker [KEYCLOAK-6494] - Address load-time of new account management console [KEYCLOAK-6495] - Address number of requests for new account management console [KEYCLOAK-6496] - Cleanup and polish current code base for new account management console [KEYCLOAK-6699] - Add more recipes to Admin CLI documentation [KEYCLOAK-6815] - Use htmlunit browser in adapter tests [KEYCLOAK-6838] - Update RH-SSO logo style [KEYCLOAK-6857] - [RH-SSO] Remove support for RH-SSO 7.1 from the documentation [KEYCLOAK-6992] - Proxy: Configure Request Timeout [KEYCLOAK-7033] - Server rendered \"Login Success/Failure\" for kcinit/KeycloakInstalled browser [KEYCLOAK-7044] - kcadm --token support [KEYCLOAK-7147] - Support obtaining a buffered input stream in HttpFacade.Request [KEYCLOAK-7162] - Expose WWW-Authenticate Header when using CORS [KEYCLOAK-7204] - Make sure that sso.redhatkeynote.com route is used just for \"sso\" project [KEYCLOAK-7223] - Increase count of connections at datasource [KEYCLOAK-6655] - Javascript Adapter - Allow users to provide cordova-specific options to login and register [KEYCLOAK-6656] - Javascript Adapter - Reject 'login' promise when users close their cordova in-app-browser on purpose [KEYCLOAK-7274] - Hardcoded config in offline adapter installation scripts [KEYCLOAK-7354] - Split ticket management and permission endpoint [KEYCLOAK-4828] - LDAP: default groups are not automatically added during user registration [KEYCLOAK-6883] - Add \"scope\" as claim to the access token. [KEYCLOAK-7334] - Update vertical nav/Integrate patternfly-ng [KEYCLOAK-7356] - Code to Token flow fails if initial redirect_uri contains a session_state parameter [KEYCLOAK-7504] - Update Node.js adapter dependencies and deprecate support to Node 4 [KEYCLOAK-7523] - PathBasedKeycloakConfigResolver uses wrong context path [KEYCLOAK-7531] - Javascript Adapter - Typescript definition of login.cordovaOptions [KEYCLOAK-7593] - Add setter for httpContext to Fuse 7 adapters' PaxWebIntegrationService [KEYCLOAK-7633] - Improve support for DEBUG level logging, including runtime level change [KEYCLOAK-7701] - Refactor key providers to support additional algorithms [KEYCLOAK-7722] - Move configuration files specific to EAP6 to app-server-eap6 module Feature Request [KEYCLOAK-943] - Account Management REST api [KEYCLOAK-1942] - Magic link authenticator prototype [KEYCLOAK-3736] - Add display name to clients [KEYCLOAK-4547] - Allow deploying themes to deployments dir [KEYCLOAK-4721] - Consider Session Language of Realm Also In ReCaptcha [KEYCLOAK-4743] - Running keycloak behind web proxy [KEYCLOAK-5372] - Add warm-up, test-time, ramp-down times [KEYCLOAK-5574] - Add edit this page in Github links to docs [KEYCLOAK-6041] - SSSD Federation script to be idempotent [KEYCLOAK-6147] - Ability to add nonce attribute to idp request [KEYCLOAK-6228] - Client Storage SPI [KEYCLOAK-6289] - Add Theme Selector SPI [KEYCLOAK-6519] - Theme resource provider [KEYCLOAK-4102] - Allow policy enforcer to load paths on demand when no path is provider [KEYCLOAK-4538] - Possiblity to allow clock skew between client and server [KEYCLOAK-4903] - Pushed Claims [KEYCLOAK-5098] - Spring Boot 2 Adapter [KEYCLOAK-6305] - Slovak translation [KEYCLOAK-6497] - Profile page [KEYCLOAK-6498] - Welcome page [KEYCLOAK-6499] - Add password update - HTML [KEYCLOAK-6500] - Add device activity - HTML [KEYCLOAK-6501] - Applications page - wireframe [KEYCLOAK-6505] - Authenticator page - wireframe review [KEYCLOAK-6622] - admin console support for client storage SPI [KEYCLOAK-6798] - Keycloak.js - allow to provide custom adapters [KEYCLOAK-6813] - CLI SSO Utility kcinit [KEYCLOAK-7000] - kcinit whoami [KEYCLOAK-7004] - kcinit browser mode [KEYCLOAK-7039] - Add support for MariaDB to Docker image [KEYCLOAK-7072] - Extended user attributes on Profile Page [KEYCLOAK-7196] - Add kc_locale to keycloak.js [KEYCLOAK-7197] - Response design for welcome page [KEYCLOAK-7090] - Applications page - HTML [KEYCLOAK-7148] - Associate sub resources to a parent resource [KEYCLOAK-7206] - Search by user id on admin console [KEYCLOAK-349] - Scope query parameter support [KEYCLOAK-5579] - Change Client Templates to Client Scope [KEYCLOAK-6720] - Display promise error massage in GrantManager.prototype.validateToken [KEYCLOAK-6771] - Holder of Key mechanism: OAuth 2.0 Certificate Bound Access Tokens [KEYCLOAK-7382] - Application Response HTML Update [KEYCLOAK-7451] - OAuth Authorization Server Metadata for Proof Key for Code Exchange [KEYCLOAK-7500] - Upgrade MySQL driver to 5.1.46 [KEYCLOAK-6663] - Support OAuth2 installed / native apps using a custom redirect uri [KEYCLOAK-7384] - Federated Identity (linked accounts) HTML [KEYCLOAK-7641] - Introduce a profile to allow building only server [KEYCLOAK-7651] - Docker image support to build Keycloak from source [KEYCLOAK-7688] - Offline Session Max for Offline Token [KEYCLOAK-7689] - Authenticator - Mobile Setup HTML (including the responsive code) [KEYCLOAK-7690] - Authenticator - SMS Code Setup HTML (including the responsive code) [KEYCLOAK-7691] - Authenticator - Backup Code Setup HTML (including the responsive code) [KEYCLOAK-7705] - Added hardcoded-ldap-group-mapper for user federation Keycloak 3.4 Enhancement [KEYCLOAK-3303] - Add option to support last two refresh tokens [KEYCLOAK-4052] - Use PasswordPolicy for LDAP password updates [KEYCLOAK-4803] - Move support for MySQL and PostgreSQL to main Keycloak Docker image [KEYCLOAK-4858] - Slow query performance for client with large data volume [KEYCLOAK-4928] - Add Primary Key Constraints to all tables of the database [KEYCLOAK-5032] - Authorize endpoint, request parameters not transmitted to IDP [KEYCLOAK-5165] - Setup CI jobs to test Keycloak quickstarts [KEYCLOAK-5186] - create user - set federationLink if present [KEYCLOAK-5298] - Enable autoscaping in Freemarker template [KEYCLOAK-5439] - Remove unneeded subsystems/modules from distro [KEYCLOAK-5446] - Improve logging of URI mismatch in received vs expected in SAML adapter [KEYCLOAK-5510] - Allow import of LDAP groups with missing subgroups [KEYCLOAK-5576] - Performance testsuite should allow exporting the dump directly after data generation [KEYCLOAK-5577] - Support setting cpu/mem docker limits in performance tests [KEYCLOAK-5616] - Processing of claims parameter [KEYCLOAK-5624] - Rename import-data profile to generate-data [KEYCLOAK-5631] - A lot of 'Unrecognized attribute' warnings in log when WF is configured to run in crossdc [KEYCLOAK-5655] - Upgrade FreeMarker [KEYCLOAK-5661] - OIDC Financial API Read Only Profile : scope MUST be returned in the response from Token Endpoint [KEYCLOAK-5671] - Adding a \"policy provider attributes\" field to Permission [KEYCLOAK-5700] - Add support for jarless server distribution [KEYCLOAK-5703] - Improving exception handling and parsing server response [KEYCLOAK-5726] - Support define enforcement mode for scopes on the adapter configuration [KEYCLOAK-5728] - Allow policy providers to push permission claims [KEYCLOAK-5798] - Move keycloak-nodejs-auth-utils and keycloak-nodejs-connect repositories Feature Request [KEYCLOAK-2035] - Add ability to get users in role [KEYCLOAK-2671] - Allow additional attributes to be pushed into Freemarker templates (login and account themes) [KEYCLOAK-3135] - Support Remote Policy Management [KEYCLOAK-3599] - Script based ProtocolMapper for ODIC [KEYCLOAK-4169] - Document how to run testsuite [KEYCLOAK-4374] - Support SAML 2.0 AttributeValue AnyType [KEYCLOAK-4580] - Token exchange service [KEYCLOAK-4766] - Add Executor Service SPI [KEYCLOAK-4982] - Create a job to release Keycloak quickstarts [KEYCLOAK-5244] - Blacklist PasswordCredentialPolicy [KEYCLOAK-5448] - Login with PayPal [KEYCLOAK-5623] - Integrate Keycloak via Jboss Fuse Fabric Profile resource Keycloak 3.3 Enhancement [KEYCLOAK-4756] - unversioned keycloak.js cache life too long [KEYCLOAK-5067] - Allow refreshable context to have an optional adapter token store [KEYCLOAK-5138] - Use build time in resource versions for snapshots [KEYCLOAK-5143] - Use auth-server-wildfly on Travis [KEYCLOAK-5180] - Add title attribute for keycloak-session-iframe to suppress accessibility errors [KEYCLOAK-5190] - Login with BitBucket [KEYCLOAK-5194] - Disabling Group and Role Permissions doesn't delete anything [KEYCLOAK-5242] - Add means to run test Keycloak server with https [KEYCLOAK-5285] - Improve possibility to extend FreeMarkerEmailTemplateProvider Feature Request [KEYCLOAK-3877] - Expose adapter config for public clients through the management interface [KEYCLOAK-4253] - Functional tests for quickstarts [KEYCLOAK-4439] - WildFly/EAP Management UI SSO [KEYCLOAK-4477] - Upgrade server to WildFly 11 Alpha1 [KEYCLOAK-4663] - Elytron subsystem [KEYCLOAK-4900] - Passing login_hint up to Identity Provider [KEYCLOAK-5086] - Provide Chinese translation in keycloak themes [KEYCLOAK-5203] - Keycloak Proxy Docker image [KEYCLOAK-5249] - gitlab.com identity provider [KEYCLOAK-5269] - account service unlink REST API doesn't work [KEYCLOAK-5291] - token references feature [KEYCLOAK-5307] - Provide Dutch translation in keycloak themes [KEYCLOAK-5319] - Source maps for keycloak.js Keycloak 3.2.0 Enhancement * [KEYCLOAK-3056] - Option to verify signature on SAML assertion in SAML Identity broker [KEYCLOAK-3631] - Make user actions tokens independently from -user- +client+ sessions so they can live a long time [KEYCLOAK-3988] - Multiple missing indexes on FKs [KEYCLOAK-3990] - Too many autoFlush checks by Hibernate and explicit em.flush() [KEYCLOAK-4016] - Provide a Link to go Back to The Application on a Timeout [KEYCLOAK-4097] - Better session sharing / handling of multiple logins [KEYCLOAK-4119] - Allow debugging Keycloak when running tests [KEYCLOAK-4497] - Update French Translation [KEYCLOAK-4670] - Back/forward/refresh button issues [KEYCLOAK-4765] - QueryParamTokenRequestAuthenticator fails when access_token query param is not a valid bearer [KEYCLOAK-4770] - Development version of keycloak-connect should depend on GitHub version of auth-utils [KEYCLOAK-4814] - disable security via configuration (i.e. for testing) in Spring Boot Adapter [KEYCLOAK-4862] - Expose client description in ClientBean [KEYCLOAK-4888] - Change default hashing provider for realm [KEYCLOAK-4889] - Improve error messages for password policies [KEYCLOAK-4929] - Refactor Authz caching and jpa API [KEYCLOAK-4933] - Use server-provisioning to create WildFly adapter dist and server overlay [KEYCLOAK-4940] - Typo in German email verification body [KEYCLOAK-4961] - Group policy [KEYCLOAK-5033] - Quickstarts integration tests should be parallelized on Travis CI [KEYCLOAK-5051] - Invalidate authz cache when realm cache is cleared [KEYCLOAK-5064] - add configuration property option realmPublicKey [KEYCLOAK-5069] - Provide a way to add a custom KeycloakConfigResolver instance for initialization [KEYCLOAK-5072] - JSPolicyProvider should use ScriptingSPI Feature Request [KEYCLOAK-3168] - Group-Based Access Control [KEYCLOAK-3297] - Add option for setting CORS Access-Control-Expose-Headers header [KEYCLOAK-3316] - Remove the IDToken if scope=openid is not used [KEYCLOAK-3444] - Fine-grained permissions in admin console and endpoints [KEYCLOAK-3592] - Docker Auth V2 Protocol Support [KEYCLOAK-4007] - Add integration tests for the nodejs adapter \"admin\" endpoints [KEYCLOAK-4204] - Extend brute force protection with permanent lockout on failed attempts [KEYCLOAK-4444] - Allow sending test email [KEYCLOAK-4773] - Remove 'providers' directory [KEYCLOAK-4815] - Making Proxy address forwarding configurable in Docker container [KEYCLOAK-4826] - Docker image for OpenShift [KEYCLOAK-4886] - Support creation of multiple OpenShift v3 Identity Providers [KEYCLOAK-4951] - Update Spring Boot documentation to reflect usage of the starter [KEYCLOAK-4955] - Partial export through admin console [KEYCLOAK-4965] - Node.js testsuite occasionally stuck on Travis CI [KEYCLOAK-4967] - Setup Travis CI to build development version of the quickstarts [KEYCLOAK-4980] - SAML adapter should return 401 when unauthenticated Ajax client accesses [KEYCLOAK-5082] - Unable to access webapp which URL being rewritten Keycloak Release 3.1.0-3 An update to the govuk themes from v1.1.0 to v1.1.1 (https://github.com/UKHomeOffice/keycloak-theme-govuk) this will change the govuk, govuk-internal and social themes You can find the Keycloak Release pages here . Note a JBOSS Developer login is required (it's free) v3.1.0 Enhancement: [KEYCLOAK-2122] - Config of AssertionConsumerServiceUrl in Saml Adapter [KEYCLOAK-4361] - Remove auth-server-standalone from domain.xml [KEYCLOAK-4502] - Update Russian translation [KEYCLOAK-4528] - Identity Provider for Openshift [KEYCLOAK-4602] - Improve path matcher when using handling patterns and caching [KEYCLOAK-4614] - Fix tooltip reference for linkOnly field in identity providers section [KEYCLOAK-4644] - Have option to not synchronize a linked account on login [KEYCLOAK-4652] - PolicyEvaluationService.evaluate() handles Role Policy for composite role incorrectly [KEYCLOAK-4664] - linking doesn't update token [KEYCLOAK-4665] - Parent IDP not logged out if account was linked during session [KEYCLOAK-4671] - Add server-private-spi to dependency deployer [KEYCLOAK-4727] - move PolicyEvaluationService into admin client [KEYCLOAK-4728] - Update French Translation [KEYCLOAK-4729] - Update German Translation [KEYCLOAK-4734] - Update Italian Translations [KEYCLOAK-4751] - Send default access denied page when requests don't match any path config [KEYCLOAK-4762] - Improve French translations [KEYCLOAK-4792] - AuthzClient should support credential types other than secret Feature Request: [KEYCLOAK-2604] - Proof Key for Code Exchange by OAuth Public Clients [KEYCLOAK-3468] - Upgrade server to WildFly 10.1.0.Final [KEYCLOAK-3573] - Elytron adapters [KEYCLOAK-3999] - Add Key Rotation support for Spring Security Adapter [KEYCLOAK-4000] - Add Key Rotation support for Spring Boot Adapter [KEYCLOAK-4163] - Improve support for e-mail addresses [KEYCLOAK-4168] - Add proxy debug endpoint [KEYCLOAK-4335] - X509 Certificate user authentication [KEYCLOAK-4396] - TypeScript type definitions for keycloak.js [KEYCLOAK-4691] - Extract Initial Token Generator to a separate method [KEYCLOAK-4697] - [RHSSO] Upgrade to EAP 7.1.0 Alpha16 [KEYCLOAK-4736] - Extend security defenses with header X-XSS-Protection [KEYCLOAK-4804] - Add spring-boot-container-bundle v3.0.0 Enhancement: [KEYCLOAK-3964] - No-import LDAP option [KEYCLOAK-3989] - Realm creation/deletion drops all admin composite roles and re-inserts them. [KEYCLOAK-4224] - Allow hiding identity providers on login page [KEYCLOAK-4362] - Split migration-domain script into two separate scripts [KEYCLOAK-4363] - ComponentFactory.onUpdate needs old model too [KEYCLOAK-4381] - Merge ModelReadOnlyException with ReadOnlyException [KEYCLOAK-4382] - Merge ModelReadOnlyException with ReadOnlyException [KEYCLOAK-4385] - Simple KeycloakConfigResolver to easily find keycloak.json inside OSGI bundle [KEYCLOAK-4475] - Minor improvements in hawtio integration docs [KEYCLOAK-4505] - ScriptBasedAuthenticator should expose clientSession as script binding [KEYCLOAK-4515] - Make it possible to clean-up other DB types than mysql or postgres [KEYCLOAK-4520] - Enable testsuite logging when running test from IDE Feature Request: [KEYCLOAK-3621] - Node.js service quickstart [KEYCLOAK-3955] - Add module for testing domain dependant tests [KEYCLOAK-4008] - Add checksums to file downloads [KEYCLOAK-4195] - Keycloak adapter and SPI bom [KEYCLOAK-4360] - Add OneTimeUse condition to SAMLResponse [KEYCLOAK-4501] - Federated identity management [KEYCLOAK-4504] - SAML Broker: Support redirect logout, even when using POST for SAML response [KEYCLOAK-4537] - Adapter for Jetty 9.4 [KEYCLOAK-4565] - Javadocument the adapter properties and add the metadata generator [KEYCLOAK-4581] - Swedish translations v2.5.0 Enhancement: [KEYCLOAK-2654] - KC invokes UserInfo Endpoint call against external Identity Provider [KEYCLOAK-2962] - OAuthRequestAuthenticator shouldn't redirect on XHR / AJAX requests [KEYCLOAK-3124] - Have adapter tests running on embedded undertow during default build [KEYCLOAK-3474] - Provide a filter to enable authorizations [KEYCLOAK-3678] - [Fuse] Add example using Camel RestDSL [KEYCLOAK-3823] - Adapters should clear key caches when not before policy is sent [KEYCLOAK-3933] - Remove UserFederationProvidersResource.getUserFederationInstanceWithFallback() once UserFederation SPI is removed [KEYCLOAK-3973] - Migration strategy for deprecated custom UserFed [KEYCLOAK-3987] - Grant the new role from the saml token if it exist [KEYCLOAK-4002] - realmRevisions cache size needs to be configurable [KEYCLOAK-4003] - Slow role checks on Infinispan RoleAdapter with composite roles. [KEYCLOAK-4004] - Display client name in referrer link instead of id [KEYCLOAK-4040] - Unable to import SAML metadata with OrganizationUrl [KEYCLOAK-4046] - Setting the 'Credentials - Temporary' flag when creating a new user causes the user to be disabled in MSAD [KEYCLOAK-4062] - Provide GUI for KeyName format in identity broker and client [KEYCLOAK-4074] - Decoupling of default provider implementations Feature Request: [KEYCLOAK-912] - Admin CLI [KEYCLOAK-3339] - Enable authorization services to EAP6 adapter [KEYCLOAK-3479] - Providers in JEE deployment [KEYCLOAK-3648] - Support for importing SAML response multi-valued attributes [KEYCLOAK-3731] - Support broker initiated SSO [KEYCLOAK-3824] - Add expiration to keys cached by clients [KEYCLOAK-4005] - Add support for \"not before\" in the NodeJS adapter [KEYCLOAK-4009] - Compatibility with AD LDS [KEYCLOAK-4018] - Client-Based Policy [KEYCLOAK-4059] - Support for duplicate emails [KEYCLOAK-4087] - LDAP group mapping should be possible via uid in memberUid mode [KEYCLOAK-4092] - Add key provider for HMAC signatures [KEYCLOAK-4109] - Ability to disable impersonation Kubernetes Platform # v0.2.19 Added Kubernetes conformance tests, extended the integration tests and use spot instances for E2E runs Calculate and set systemReserved kubeReserved kubelet flags, based on instance type per IG Mount encrypted docker volume for Master Compute Nodes Update default instance types for IGs Update CoreOS to stable release v1967.3.0 (including CVE fixes) v0.2.18 Upgrading to kops v0.2.7 due to an issue with the rbac kubelet-api manifest v0.2.17 Updated to the latest custom kops release v0.2.6 Added the fix for the kubelet webhook authorization Added kubelet webhook authorization to the kubelet for granular perms v0.2.16 Updates to the Kubernetes APIServer Audit Policy v0.2.15 Updated CoreOS release to v1911.4.0 Reduced verbosity of api audit logs Increased default value for master nodes root volumes v0.2.14 Refined the kubelet pod eviction policies v0.2.13 Updated the kubelet pod eviction policies v0.2.12 Updated to kubernetes v1.12.3 v0.2.11 Updating to kubernetes v1.12.1 #PR96 Changing to the recommended ipvs mode for the kube-proxy. #PR98 Added a sample psp and cluster role / binding to ease ephermal testing. #PR96 Update the CoreOS version to 1855.4.0 #PR99 Updating to Canel v3.2.3 #PR101 v0.2.10 Updating the fixed version of kops Added the token controller for bootstrap cleanup v0.2.9 Added the node authorizer to the mix #PR87 Fixed the s3 bucket bug #PR92 Fixed up the image to use the fixed node authorizer image #PR93 v0.2.8 Fixed the iptables restore created by the previous release v0.2.7 Allowing the blocking off vpc ssh to be optional, defaulting to true and mainly as an exception for acp-vpn v0.2.6 Fixed the E2E tests. This was broken due to PSP policies which was blocking the kuberang test from running. v0.2.5 Use AWS TimeSync service v0.2.4 Updating the base image to alpine 3.7 v0.2.3 (kops release) Bumped to custom-v0.1.3 , including a minor fix to the api-server AdmissionController flag use in k8s v1.1 v0.2.3 (ACP Build) Kubernetes v1.10.3: https://github.com/kubernetes/kubernetes/releases/tag/v1.10.3 v0.2.0 (ACP Build) Kubernetes v1.10.1: https://github.com/kubernetes/kubernetes/releases/tag/v1.10.1 CoreOS v1688.5.3: https://github.com/coreos/manifest/releases/tag/v1688.5.3 Etcd v3.3.3: https://github.com/coreos/etcd/blob/master/CHANGELOG-3.3.md#v333-2018-03-29 v0.1.0 (ACP Build) Kubernetes v1.8.4: https://github.com/kubernetes/kubernetes/releases/tag/v1.8.4 CoreOS v1632.2.1: https://github.com/coreos/manifest/releases/tag/v1632.2.1 Etcd v3.3.1: https://github.com/coreos/etcd/blob/master/CHANGELOG-3.3.md#v331-2018-02-12 SonarQube # 6.7.3.1 Nginx - Remove limit on body size 6.7.3 Update to: https://www.sonarsource.com/resources/product-news/news.html#2018-04-06-sonarqube-673-released Update all existing plugins to latest Install new plugins: https://github.com/stevespringett/dependency-check-sonar-plugin https://github.com/racodond/sonar-gherkin-plugin 6.7.1 Update to: https://www.sonarsource.com/resources/product-news/news.html#2017-12-21-sonarqube-671-released Sysdig # 1629 Updated to hotfix to address the dashboards not showing current data issue, lagging x hours behind. 1586 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/v1586 1511 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/v1511 1472 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/v1472 987 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/987 925 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/925 893 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/893 886 Update to 886 (Release notes not available) Includes hotfix for AWS CloudWatch metrics 858 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/858 800 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/800 776 Update to Sysdig 776: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/776 760 Update to Sysdig 760: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/760 722 Update to Sysdig 722: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/722","title":"Release Notes"},{"location":"releases-notes/index.html#artifactory","text":"","title":"Artifactory"},{"location":"releases-notes/index.html#elk-stack","text":"","title":"ELK Stack"},{"location":"releases-notes/index.html#gitlab","text":"You can find full Gitlab Release changelog here .","title":"GitLab"},{"location":"releases-notes/index.html#ingress","text":"","title":"Ingress"},{"location":"releases-notes/index.html#keycloak","text":"","title":"Keycloak"},{"location":"releases-notes/index.html#kubernetes-platform","text":"","title":"Kubernetes Platform"},{"location":"releases-notes/index.html#sonarqube","text":"","title":"SonarQube"},{"location":"releases-notes/index.html#sysdig","text":"","title":"Sysdig"},{"location":"releases-notes/artifactory.html","text":"Artifactory # v6.5.13 Update to Artifactory v6.5.13: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory6.5.13 v6.5.9 Update to Artifactory v6.5.9: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory6.5.9 v6.2.0 Update to Artifactory v6.2.0: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory6.2.0 v5.9.3 Update to Artifactory v5.9.3: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.9.3 v5.9.0 Update to Artifactory v5.9.0: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.9.0 v5.8.4 Update to Artifactory v5.8.4: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.8.4 v5.7.2 Update to Artifactory v5.7.2: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.7.2 v5.4.6 Update to Artifactory v5.4.6: https://www.jfrog.com/confluence/display/RTF/Release+Notes#ReleaseNotes-Artifactory5.4.6 Use the Artifactory docker image supplied by JFrog Change configmap to secrets v5.4.4 Configuration update to cycle the certificates used by artifactory","title":"Artifactory"},{"location":"releases-notes/artifactory.html#artifactory","text":"","title":"Artifactory"},{"location":"releases-notes/elk.html","text":"ELK Stack # v0.4.3 Enables Watcher SMTP Enables PDF export for reporting Improvements to Elasticsearch monitoring ELK components run with set UID v0.4.1 Enables X-Pack RBAC v0.3.3 Enabled X-Pack Enabled Kibana user authentication Enabled Elasticsearch cluster TLS Enabled secure logs shipping Enabled security audit logging","title":"ELK Stack"},{"location":"releases-notes/elk.html#elk-stack","text":"","title":"ELK Stack"},{"location":"releases-notes/gitlab.html","text":"GitLab # You can find full Gitlab Release changelog here . v11.4.7 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1147-2018-11-20 v11.1.4 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1114-2018-07-30 v10.8.3 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1083-2018-05-30 v10.4.2 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1042-2018-01-30 v10.2.5 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#1025-2017-12-15 v9.4.5 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#945-2017-08-14 v9.4.3 https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md#943-2017-07-31","title":"GitLab"},{"location":"releases-notes/gitlab.html#gitlab","text":"You can find full Gitlab Release changelog here .","title":"GitLab"},{"location":"releases-notes/ingress.html","text":"Ingress # 0.21.0 Update to Ingress-nginx 0.21.0: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#0210 This includes a breaking change to the ingress.kubernetes.io/secure-backends: \"true\" annotation, it has been deprecated and ingress.kubernetes.io/backend-protocol: \"HTTPS\" introduced. Whilst we upgrade, we recommend having both in place. 0.15.0 Update to Ingress-nginx 0.15.0: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#0150 0.13.0 Update to Ingress-nginx 0.13.0: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#0130 Add kube namespace to log messages 0.11.0 Update to Ingress-nginx 0.11.0: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#0110 0.9-beta-2 Update to Ingress-nginx 0.9-beta-2: https://github.com/kubernetes/ingress-nginx/blob/master/Changelog.md#09-beta2","title":"Ingress"},{"location":"releases-notes/ingress.html#ingress","text":"","title":"Ingress"},{"location":"releases-notes/keycloak.html","text":"Keycloak # Keycloak v4.5.0 Enhancement [KEYCLOAK-4622] - Use HS256 for refresh tokens [KEYCLOAK-4623] - Use HS256 for client registration tokens [KEYCLOAK-7270] - First Broker Login Link Without Authentication [KEYCLOAK-7604] - SAML test clients have confusing names/entityIDs [KEYCLOAK-8010] - Improve handling of SAML tag [KEYCLOAK-8072] - Investigate if we can drop server-openshift [KEYCLOAK-8265] - Document the default admin credentials when running Test Suite Feature Request [KEYCLOAK-6229] - Implement OpenShift Token Review Interface [KEYCLOAK-6285] - WWW-Authenticate challenge flow for 'oc login' [KEYCLOAK-6770] - JWS signatures using ES256 algorithms for signing [KEYCLOAK-7061] - Keycloak Docker image should provide a way to import realm files [KEYCLOAK-7435] - Add logout specific session to session endpoints in account service [KEYCLOAK-7560] - Token Signature SPI [KEYCLOAK-8146] - Add LocaleSelectorSPI to allow custom locale selection [KEYCLOAK-8263] - Allow overriding token expiration time on a client [KEYCLOAK-8264] - Updates to Token Review endpoint [KEYCLOAK-8273] - Failed to evaluate permissions when in permissive mode and using UMA tickets [KEYCLOAK-8416] - Add support for configuring welcome theme in docker image Epic [KEYCLOAK-8121] - Add support for additional token algorithms Bug [KEYCLOAK-3058] - Validation of \"aud\" in adapters [KEYCLOAK-6468] - AnonymousAuthenticationToken cannot be cast to KeycloakAuthenticationToken [KEYCLOAK-6962] - No documentation for rpms upgrade [KEYCLOAK-6971] - Requesting a token for same user multiple times concurrently causes 401 errors [KEYCLOAK-7608] - FolderThemeProvider throws NPE when the themes are located on the classpath [KEYCLOAK-7661] - Keycloak does not build on windows environment due \u201ctestAllTheZipThings\u201d failure [KEYCLOAK-7987] - Can't set authorization enabled when using kcreg [KEYCLOAK-8016] - Broken new Account Console in Chrome [KEYCLOAK-8070] - wrong expose headers when enable cors and policyenforcer [KEYCLOAK-8133] - Can't Sucessfully inject a custom KeycloakSpringBootConfigResolver in the Keycloak Spring Boot Security Adapter [KEYCLOAK-8168] - PEP is resolving claims twice under certain circumstances [KEYCLOAK-8180] - Welcome Screen sporadically fails to load in Firefox [KEYCLOAK-8189] - Broken Google Social Login test [KEYCLOAK-8190] - Cannot compile Node.js adapter UI tests [KEYCLOAK-8222] - Performance issue when loading user list [KEYCLOAK-8270] - Cannot remove \"Not Before\" or \"Not On or After\" from a Time Policy [KEYCLOAK-8274] - Realm export \"uma_protection\" role not reproducable [KEYCLOAK-8281] - Deletion of client and users with token exchange policy leads to breaking errors [KEYCLOAK-8308] - Deprecate token_introspection_endpoint claim from OIDC discovery document [KEYCLOAK-8315] - Completely broken Node.js adapter [KEYCLOAK-8327] - Token Introspect Test for Refresh Token Mistake [KEYCLOAK-8378] - Infinispan version mismatch in prod-arguments [KEYCLOAK-8425] - AdapterExecutionDecider throws NPE for cluster tests [KEYCLOAK-8440] - QuickStart does not build straight forward from GitHub clone Story [KEYCLOAK-8122] - Long lived tokens for OpenShift Task [KEYCLOAK-6332] - Switch Firefox driver to GeckoDriver once Firefox ESR 52.x is not supported [KEYCLOAK-6638] - Audience support for Client Scopes [KEYCLOAK-7582] - Clustering support to Keycloak server image [KEYCLOAK-7742] - Investigate failing Admin Console UI tests [KEYCLOAK-7813] - Coordinate the transfer or keycloak-proxy repository to Keycloak organization [KEYCLOAK-7915] - Implement trigger of Test pipeline using UMB [KEYCLOAK-8024] - Install JMS Messaging plugin to keycloak-jenkins [KEYCLOAK-8057] - Investigate React Framework [KEYCLOAK-8206] - Use HS256 for action tokens [KEYCLOAK-8207] - Use HS256 for cookie tokens [KEYCLOAK-8211] - Use HS256 for authorization permission tickets [KEYCLOAK-8236] - Bot command for executing tests (test command) can be without additional paramters [KEYCLOAK-8238] - Create required clients in OpenShift integration script [KEYCLOAK-8285] - Remove user-storage-jpa and user-storage-simple from Keycloak repository [KEYCLOAK-8289] - Remove authorization services from product preview profile [KEYCLOAK-8297] - Docs for audience [KEYCLOAK-8319] - Initial pipeline for product test * Keycloak v4.4.0 Enhancement [KEYCLOAK-7161] - Refactor PhotozAuthz tests to not use hard waits [KEYCLOAK-7584] - Doublecheck admin console tests passing after clientScopes added [KEYCLOAK-7594] - Fix cross-dc integration and update Cross-dc integration to ISPN server 9.2.4 and JDG 7.2 [KEYCLOAK-7595] - Upgrade aesh to 1.4 [KEYCLOAK-7596] - Doublecheck migration scripts and all tests pass for Wildfly 13 [KEYCLOAK-7858] - OIDC servlet filter adapter OSGi support [KEYCLOAK-7991] - Role-Resource in the Admin-API-Client is missing pagination params [KEYCLOAK-8054] - Use BOMREST property in product config files [KEYCLOAK-8069] - Fixed hostname provider should read ports from request [KEYCLOAK-8075] - Provide a better error handling for Express when access is denied [KEYCLOAK-8076] - Remove deprecation warning messages from Node.js adapter [KEYCLOAK-8134] - uma-ticket rpt token endpoint does not evaluate policies without permission value(s) [KEYCLOAK-8241] - Use BOMREST property in nodejs-connect product config * Feature Request [KEYCLOAK-3334] - Enable authorization services to Node.js adapter [KEYCLOAK-5520] - Script based ProtocolMapper for SAML [KEYCLOAK-7222] - forgot password redirect to account client [KEYCLOAK-7751] - Auth welcome page - HTML code [KEYCLOAK-7891] - Configure SSL certificates in Docker image [KEYCLOAK-8056] - Configure hostname in Docker image [KEYCLOAK-8145] - Configure keystore when running KeycloakServer using TLS [KEYCLOAK-8147] - Support Content-Security-Policy-Report-Only security header [KEYCLOAK-8152] - Allow passing current locale to OAuth2 IdPs Epic [KEYCLOAK-6275] - Improve usability of xPaaS images [KEYCLOAK-6781] - Upgrade to WildFly 13 Final Bug [KEYCLOAK-5226] - Travis builds fail for default trusty image [KEYCLOAK-6086] - Spring boot app doesn't start with Keycloak and Jetty combination [KEYCLOAK-6411] - Unable to provision database on MariaDB 10.3.4+ [KEYCLOAK-6706] - E-mail verification won't let user back into the app [KEYCLOAK-6803] - Sign-up error with custom UserStorageProvider [KEYCLOAK-7087] - Calling admin routes without data crashes NodeJS app [KEYCLOAK-7160] - Completely broken AuthZ Configs in IE [KEYCLOAK-7243] - Link are broken in documentaion [KEYCLOAK-7394] - LDAP Connections can't be tested using UI [KEYCLOAK-7695] - Authorization response with implicit flow should contain token_type and expires_in [KEYCLOAK-7731] - KeycloakUriBuilder removes well known ports (80 / 443) even if the issuer url in the token includes it [KEYCLOAK-7752] - CLIENT_INITIATED_ACCOUNT_LINKING_ERROR with invalid_token [KEYCLOAK-7754] - UMA token introspection endpoint does not conform to specification [KEYCLOAK-7757] - Deleting scopes in account console doesn't work [KEYCLOAK-7782] - keycloak Groups click view all can't query out [KEYCLOAK-7797] - Misplaced question marks [KEYCLOAK-7911] - Wrong version of Node.js adapters - Final missing [KEYCLOAK-7943] - NullPointerException when SAML User Property mapper is empty/blank [KEYCLOAK-7944] - Fail to create a primary key on (USER_SESSION_ID, CLIENT_ID, OFFLINE_FLAG) when migrating to 3.4.3 [KEYCLOAK-7970] - Fresh authSession not started for correct client when handling action tokens [KEYCLOAK-7975] - updating execution with Oracle DB causes unique constraint (DBALLO00.CONSTRAINT_AUTH_CFG_PK) violated [KEYCLOAK-8012] - Unable to issue RPT token after server restart - NullPointerException [KEYCLOAK-8068] - IP access control broken in undertow current version [KEYCLOAK-8073] - cli unusable on FreeBSD [KEYCLOAK-8080] - Update Realm Events Config doesn't create an admin event [KEYCLOAK-8095] - Keycloak \"SSSD\" user federation option doesn't shows up on the drop down menu [KEYCLOAK-8101] - NPE in PathBasedKeycloakConfigResolver when cache is empty [KEYCLOAK-8109] - StaxParserUtil.getDOMElement can return incomplete tree [KEYCLOAK-8119] - Migration tests doesn't reflect if some features are not enabled [KEYCLOAK-8120] - NullPointerException in ClaimInformationPointProviderTest when product profile is used [KEYCLOAK-8132] - Missing type definitions for parsed tokens in Keycloak Javascript Typescript declarations. [KEYCLOAK-8138] - Several tests failing in product profile [KEYCLOAK-8142] - Regression in proper handling of public endpoints in AbstractPolicyEnforcer [KEYCLOAK-8150] - Performance regression between 3.4.3.Final and 4.3.0.Final [KEYCLOAK-8178] - AdminEventTest.testGetRepresentation failing with many databases (mysql, postresql, mariadb, oracle)) [KEYCLOAK-8183] - Database is repeatedly queried for resource server if client has authz not enabled [KEYCLOAK-8218] - BaseSAML2BindingBuilder cleans URL parameters in a SAML provider * Task [KEYCLOAK-6743] - Adapter tests - update the server maintanance according to e.g. test annotation (cluster, crossdc) [KEYCLOAK-6746] - Adapter tests - execute (or skip) particular tests based on app server-container [KEYCLOAK-6974] - Update to Infinispan 9 [KEYCLOAK-6975] - Update configuration files for WF 13 [KEYCLOAK-6976] - Refactor to accommodate removed jndi-name in cache-container [KEYCLOAK-7231] - Add Example adapter tests for app-server-undertow (without Photoz test class) [KEYCLOAK-7437] - Support for non-persistent consents through prompt=consent [KEYCLOAK-7470] - Client scope ordering capability [KEYCLOAK-7488] - Keycloak OpenShift template(s) [KEYCLOAK-7770] - Instabilities in the keycloak-pipeline [KEYCLOAK-7925] - Prepare test suite for New Acct Mgt Console Tests [KEYCLOAK-8052] - Change Travis job from Node.js adapter to always build the latest changes from Keycloak server [KEYCLOAK-8067] - Provide an up to date Docker image for the Keycloak server [KEYCLOAK-8093] - Add IDs to HTML elements for better testability [KEYCLOAK-8103] - Remove deprecated images [KEYCLOAK-8116] - Change Travis job from Node.js adapter to use Docker [KEYCLOAK-8127] - Make sure adapter tests are passing and correct adapters tested [KEYCLOAK-8139] - Align tests, documentation and downloads with supported Wildfly adapters [KEYCLOAK-8170] - Add build description to the pipeline [KEYCLOAK-8171] - Limit time execution of the keycloak test pipeline [KEYCLOAK-8174] - Admin console tests for the \"Client Scopes\" tab of the client [KEYCLOAK-8188] - Add ability to exclude failing or unstable tests from pipeline execution [KEYCLOAK-8246] - Disable Base UI tests by default Keycloak v4.3.0 Enhancement [KEYCLOAK-5289] - support hosted domain for Google logins Feature Request [KEYCLOAK-7635] - Authenticate clients with x509 certificate [KEYCLOAK-7967] - Hostname SPI Epic [KEYCLOAK-5522] - Add support for Fuse 7 Bug [KEYCLOAK-6818] - Keycloak creates an extra AUTH_SESSION_ID cookie with a path of \u201c/auth\u201d when logging in [KEYCLOAK-6832] - Destination Validation should ignore whether default port is explicitly specified [KEYCLOAK-7528] - Token endpoint doesn't set Cache-Control and Pragma header [KEYCLOAK-7562] - ClientInitiatedAccountLinkTest#testErrorConditions fails [KEYCLOAK-7946] - Variable rather than intended value showing in RH-SSO doc [KEYCLOAK-7954] - OIDC Provider doesn't skip token validation if URL is empty [KEYCLOAK-7974] - Fix typo in remove credentials alert [KEYCLOAK-7984] - RequiredActionProviderEntity priority migration issue [KEYCLOAK-7985] - Version migration the database table names do not match [KEYCLOAK-7986] - Migration test fails for migration from 3.4.3.Final [KEYCLOAK-7988] - New keycloak-bot commands convention is not mentioned in README.md [KEYCLOAK-7989] - Running server config migration fails due the Hostname SPI [KEYCLOAK-7994] - Move Fuse examples into test-apps [KEYCLOAK-8002] - Cannot build new Account Console [KEYCLOAK-8003] - Migration to 4.2.1 extracting RESOURCE_URIs fails with fine-grained admin permissions [KEYCLOAK-8007] - Cannot compile Console UI and Welcome Page tests [KEYCLOAK-8015] - Migration into 4.2.1.Final fails from version 3.4.3.Final [KEYCLOAK-8035] - Failing GitLab Social Login test [KEYCLOAK-8036] - Misplaced IdPs buttons on the Login Page [KEYCLOAK-8046] - X509 Client Authenticator sends Client entity twice [KEYCLOAK-8048] - Testsuite does not compile due to cross-PR interference Keycloak v4.2.1 Bug [KEYCLOAK-7984] - RequiredActionProviderEntity priority migration issue [KEYCLOAK-7985] - Version migration the database table names do not match [KEYCLOAK-7986] - Migration test fails for migration from 3.4.3.Final [KEYCLOAK-7998] - File based realm export breaks after migration from Keycloak 3.4.3 to 4.2.1 Keycloak v4.2.0 Enhancement2 [KEYCLOAK-4407] - Ability to restart arquillian containers from test [KEYCLOAK-5609] - An option to create claims with dots (.) in them [KEYCLOAK-6577] - Unable to map claim attributes with dots (.) in them [KEYCLOAK-7703] - PathBasedKeycloakConfigResolver - more generic behavior [KEYCLOAK-7792] - Always preserve URL fragment in redirectUri [KEYCLOAK-7876] - Improve stability of fuse7 hawtio test [KEYCLOAK-7924] - Speed up cross-dc tests [KEYCLOAK-7959] - OAuth 2.0 Certificate Bound Access Tokens in Reverse Proxy Deployed Environment [KEYCLOAK-7973] - Possibility to add classpath elements to KeycloakServer Feature Request [KEYCLOAK-1925] - SAML adapter multitenant support [KEYCLOAK-2606] - In-app browser tab support for Cordova [KEYCLOAK-5629] - Add credential endpoints to account service [KEYCLOAK-6313] - Changing execution order of required actions easily feature [KEYCLOAK-7105] - Notification needs to be lower [KEYCLOAK-7201] - OIDC Identity Brokering with Client parameter forward [KEYCLOAK-7294] - Password Page - Angular [KEYCLOAK-7846] - Turn off disallowed features * Epic [KEYCLOAK-6176] - Stable and reliable CI that can be used to test PRs, master and releases Bug [KEYCLOAK-2886] - Cluster tests fail when running from IDE [KEYCLOAK-4662] - Keycloak adapter missing configuration attribute \"proxy-url\" [KEYCLOAK-6308] - Disable 'secret question credentials' fails [KEYCLOAK-6314] - After a terms conditions rejection, an \"internal server error has ocurred\" happens [KEYCLOAK-6708] - NullPointerException when integrating with IDP that returns a SAML XML that does not contain the fields Keycloak expects by default [KEYCLOAK-6866] - Error 404 after changing locale while authenticating using X.509 [KEYCLOAK-7497] - Remove Babel transpiler [KEYCLOAK-7524] - Vertical Nav Doesn't close on secondary click [KEYCLOAK-7663] - Deleting identity provider does not delete it's mappers [KEYCLOAK-7795] - \"Back to \" missing from Welcome Page [KEYCLOAK-7802] - Broken HoKTest [KEYCLOAK-7805] - Broken PayPal and Bitbucket Social Login tests [KEYCLOAK-7816] - Tech preview features (like authz) are run by default [KEYCLOAK-7823] - Keycloak returns wrong HTTP status during SPNEGO authentication [KEYCLOAK-7840] - Secret in keycloak.json and client-import.json doesn't match [KEYCLOAK-7860] - proxy-address-forwarding option is not added to https listener in Docker image [KEYCLOAK-7872] - Doesn't remove Identity Provider Mapper after removing identity provider [KEYCLOAK-7881] - Docker image jboss/keycloak doesn't contain jq [KEYCLOAK-7913] - Invalid naming of JPA changelog files [KEYCLOAK-7934] - Dataset generator cannot create \"empty\" mappings [KEYCLOAK-7965] - Redundant div end tag in base theme login.ftl [KEYCLOAK-7977] - Release failing due the NPE during swagger2markup-maven-plugin execution Task [KEYCLOAK-7101] - Investigate failing Social Login Tests [KEYCLOAK-7269] - [SPIKE] - Investigate how to support resource-less permissions [KEYCLOAK-7310] - Add migration test from 3.4.x to 4.x [KEYCLOAK-7328] - Test the RH-SSO 7.2.z EAP 7 adapter with early builds of EAP 7.2.0 [KEYCLOAK-7329] - Test the RH-SSO 7.3.0 EAP 7 adapter with early builds of EAP 7.2.0 [KEYCLOAK-7400] - Update Camel / Fuse 7 adapter once CAMEL-12514 is merged to Fuse's Camel component [KEYCLOAK-7498] - Remove unused components. [KEYCLOAK-7599] - Improve handling of test datasets [KEYCLOAK-7620] - Generating performance datasets for authorization services [KEYCLOAK-7666] - Adapter tests - add dynamically loaded container - remove abstract classes - EAP6-fuse6 [KEYCLOAK-7817] - Update eap6.version in testsuite [KEYCLOAK-7857] - Fix Notifications - Switch to pf-ng notifications [KEYCLOAK-7888] - Update Fuse adapter examples/guide to new way of CXF servlet registration Keycloak v4.1.0 Enhancement [KEYCLOAK-3063] - Check if keycloak-osgi-thirdparty can be removed [KEYCLOAK-3370] - Choose Theme by Client [KEYCLOAK-4937] - Convert time units in emails into human-friendly format [KEYCLOAK-5166] - Setup CI jobs to test RH-SSO quickstarts [KEYCLOAK-5578] - The javascript adapter should use native Promises [KEYCLOAK-5791] - Add support for multi-valued attributes in ScriptBasedOIDCProtocolMapper [KEYCLOAK-5811] - OIDC Client Authentication by JWS Client Assertion in client_secret_jwt [KEYCLOAK-5857] - Support for PBKDF2 hashes with different key size [KEYCLOAK-5886] - Improvements to Photoz Examples [KEYCLOAK-6085] - Allow to customize DB dump download location through a Maven property [KEYCLOAK-6222] - Check for script syntax errors on ScriptBasedOIDCProtocolMapper validation [KEYCLOAK-6262] - Incorporate new visual design for login pages [KEYCLOAK-6298] - SAML adapter script should support offline installation of adapter [KEYCLOAK-6299] - Product profile should only include supported JavaDocs [KEYCLOAK-6302] - Add support for user defined networks and Docker compose [KEYCLOAK-6330] - GitHub social IdP: Use GitHub API for fetching user's private email if no public email is set [KEYCLOAK-6335] - Per client authentication flows [KEYCLOAK-6336] - Per client authentication flows configuration in admin console [KEYCLOAK-6339] - Show SAML IdP-initiated client URI [KEYCLOAK-6350] - Refactor SAML parsers [KEYCLOAK-6355] - Non-browser multi-request cookieless auth flow support [KEYCLOAK-6378] - Clean-up node_modules in themes [KEYCLOAK-6493] - [SPIKE] Investigate architecture of new account management console [KEYCLOAK-6561] - Add account management and update profile to js-console example [KEYCLOAK-6578] - Support other OIDC providers with keycloak.js [KEYCLOAK-6589] - Performance issues in Users REST API [KEYCLOAK-6618] - Update German Translation [KEYCLOAK-6664] - Fix performance testsuite shell scripts to run on macOS [KEYCLOAK-6700] - Financial API Read and Write API Security Profile : State hash value (s_hash) to protect state parameter [KEYCLOAK-6871] - Make sending a request object mandatory for certain clients [KEYCLOAK-4134] - Update paths in adapters when new resources are created on the server [KEYCLOAK-5457] - Context accessibility for JS base policies [KEYCLOAK-5830] - Automated stress test [KEYCLOAK-6448] - Instagram social broker [KEYCLOAK-6494] - Address load-time of new account management console [KEYCLOAK-6495] - Address number of requests for new account management console [KEYCLOAK-6496] - Cleanup and polish current code base for new account management console [KEYCLOAK-6699] - Add more recipes to Admin CLI documentation [KEYCLOAK-6815] - Use htmlunit browser in adapter tests [KEYCLOAK-6838] - Update RH-SSO logo style [KEYCLOAK-6857] - [RH-SSO] Remove support for RH-SSO 7.1 from the documentation [KEYCLOAK-6992] - Proxy: Configure Request Timeout [KEYCLOAK-7033] - Server rendered \"Login Success/Failure\" for kcinit/KeycloakInstalled browser [KEYCLOAK-7044] - kcadm --token support [KEYCLOAK-7147] - Support obtaining a buffered input stream in HttpFacade.Request [KEYCLOAK-7162] - Expose WWW-Authenticate Header when using CORS [KEYCLOAK-7204] - Make sure that sso.redhatkeynote.com route is used just for \"sso\" project [KEYCLOAK-7223] - Increase count of connections at datasource [KEYCLOAK-6655] - Javascript Adapter - Allow users to provide cordova-specific options to login and register [KEYCLOAK-6656] - Javascript Adapter - Reject 'login' promise when users close their cordova in-app-browser on purpose [KEYCLOAK-7274] - Hardcoded config in offline adapter installation scripts [KEYCLOAK-7354] - Split ticket management and permission endpoint [KEYCLOAK-4828] - LDAP: default groups are not automatically added during user registration [KEYCLOAK-6883] - Add \"scope\" as claim to the access token. [KEYCLOAK-7334] - Update vertical nav/Integrate patternfly-ng [KEYCLOAK-7356] - Code to Token flow fails if initial redirect_uri contains a session_state parameter [KEYCLOAK-7504] - Update Node.js adapter dependencies and deprecate support to Node 4 [KEYCLOAK-7523] - PathBasedKeycloakConfigResolver uses wrong context path [KEYCLOAK-7531] - Javascript Adapter - Typescript definition of login.cordovaOptions [KEYCLOAK-7593] - Add setter for httpContext to Fuse 7 adapters' PaxWebIntegrationService [KEYCLOAK-7633] - Improve support for DEBUG level logging, including runtime level change [KEYCLOAK-7701] - Refactor key providers to support additional algorithms [KEYCLOAK-7722] - Move configuration files specific to EAP6 to app-server-eap6 module Feature Request [KEYCLOAK-943] - Account Management REST api [KEYCLOAK-1942] - Magic link authenticator prototype [KEYCLOAK-3736] - Add display name to clients [KEYCLOAK-4547] - Allow deploying themes to deployments dir [KEYCLOAK-4721] - Consider Session Language of Realm Also In ReCaptcha [KEYCLOAK-4743] - Running keycloak behind web proxy [KEYCLOAK-5372] - Add warm-up, test-time, ramp-down times [KEYCLOAK-5574] - Add edit this page in Github links to docs [KEYCLOAK-6041] - SSSD Federation script to be idempotent [KEYCLOAK-6147] - Ability to add nonce attribute to idp request [KEYCLOAK-6228] - Client Storage SPI [KEYCLOAK-6289] - Add Theme Selector SPI [KEYCLOAK-6519] - Theme resource provider [KEYCLOAK-4102] - Allow policy enforcer to load paths on demand when no path is provider [KEYCLOAK-4538] - Possiblity to allow clock skew between client and server [KEYCLOAK-4903] - Pushed Claims [KEYCLOAK-5098] - Spring Boot 2 Adapter [KEYCLOAK-6305] - Slovak translation [KEYCLOAK-6497] - Profile page [KEYCLOAK-6498] - Welcome page [KEYCLOAK-6499] - Add password update - HTML [KEYCLOAK-6500] - Add device activity - HTML [KEYCLOAK-6501] - Applications page - wireframe [KEYCLOAK-6505] - Authenticator page - wireframe review [KEYCLOAK-6622] - admin console support for client storage SPI [KEYCLOAK-6798] - Keycloak.js - allow to provide custom adapters [KEYCLOAK-6813] - CLI SSO Utility kcinit [KEYCLOAK-7000] - kcinit whoami [KEYCLOAK-7004] - kcinit browser mode [KEYCLOAK-7039] - Add support for MariaDB to Docker image [KEYCLOAK-7072] - Extended user attributes on Profile Page [KEYCLOAK-7196] - Add kc_locale to keycloak.js [KEYCLOAK-7197] - Response design for welcome page [KEYCLOAK-7090] - Applications page - HTML [KEYCLOAK-7148] - Associate sub resources to a parent resource [KEYCLOAK-7206] - Search by user id on admin console [KEYCLOAK-349] - Scope query parameter support [KEYCLOAK-5579] - Change Client Templates to Client Scope [KEYCLOAK-6720] - Display promise error massage in GrantManager.prototype.validateToken [KEYCLOAK-6771] - Holder of Key mechanism: OAuth 2.0 Certificate Bound Access Tokens [KEYCLOAK-7382] - Application Response HTML Update [KEYCLOAK-7451] - OAuth Authorization Server Metadata for Proof Key for Code Exchange [KEYCLOAK-7500] - Upgrade MySQL driver to 5.1.46 [KEYCLOAK-6663] - Support OAuth2 installed / native apps using a custom redirect uri [KEYCLOAK-7384] - Federated Identity (linked accounts) HTML [KEYCLOAK-7641] - Introduce a profile to allow building only server [KEYCLOAK-7651] - Docker image support to build Keycloak from source [KEYCLOAK-7688] - Offline Session Max for Offline Token [KEYCLOAK-7689] - Authenticator - Mobile Setup HTML (including the responsive code) [KEYCLOAK-7690] - Authenticator - SMS Code Setup HTML (including the responsive code) [KEYCLOAK-7691] - Authenticator - Backup Code Setup HTML (including the responsive code) [KEYCLOAK-7705] - Added hardcoded-ldap-group-mapper for user federation Keycloak 3.4 Enhancement [KEYCLOAK-3303] - Add option to support last two refresh tokens [KEYCLOAK-4052] - Use PasswordPolicy for LDAP password updates [KEYCLOAK-4803] - Move support for MySQL and PostgreSQL to main Keycloak Docker image [KEYCLOAK-4858] - Slow query performance for client with large data volume [KEYCLOAK-4928] - Add Primary Key Constraints to all tables of the database [KEYCLOAK-5032] - Authorize endpoint, request parameters not transmitted to IDP [KEYCLOAK-5165] - Setup CI jobs to test Keycloak quickstarts [KEYCLOAK-5186] - create user - set federationLink if present [KEYCLOAK-5298] - Enable autoscaping in Freemarker template [KEYCLOAK-5439] - Remove unneeded subsystems/modules from distro [KEYCLOAK-5446] - Improve logging of URI mismatch in received vs expected in SAML adapter [KEYCLOAK-5510] - Allow import of LDAP groups with missing subgroups [KEYCLOAK-5576] - Performance testsuite should allow exporting the dump directly after data generation [KEYCLOAK-5577] - Support setting cpu/mem docker limits in performance tests [KEYCLOAK-5616] - Processing of claims parameter [KEYCLOAK-5624] - Rename import-data profile to generate-data [KEYCLOAK-5631] - A lot of 'Unrecognized attribute' warnings in log when WF is configured to run in crossdc [KEYCLOAK-5655] - Upgrade FreeMarker [KEYCLOAK-5661] - OIDC Financial API Read Only Profile : scope MUST be returned in the response from Token Endpoint [KEYCLOAK-5671] - Adding a \"policy provider attributes\" field to Permission [KEYCLOAK-5700] - Add support for jarless server distribution [KEYCLOAK-5703] - Improving exception handling and parsing server response [KEYCLOAK-5726] - Support define enforcement mode for scopes on the adapter configuration [KEYCLOAK-5728] - Allow policy providers to push permission claims [KEYCLOAK-5798] - Move keycloak-nodejs-auth-utils and keycloak-nodejs-connect repositories Feature Request [KEYCLOAK-2035] - Add ability to get users in role [KEYCLOAK-2671] - Allow additional attributes to be pushed into Freemarker templates (login and account themes) [KEYCLOAK-3135] - Support Remote Policy Management [KEYCLOAK-3599] - Script based ProtocolMapper for ODIC [KEYCLOAK-4169] - Document how to run testsuite [KEYCLOAK-4374] - Support SAML 2.0 AttributeValue AnyType [KEYCLOAK-4580] - Token exchange service [KEYCLOAK-4766] - Add Executor Service SPI [KEYCLOAK-4982] - Create a job to release Keycloak quickstarts [KEYCLOAK-5244] - Blacklist PasswordCredentialPolicy [KEYCLOAK-5448] - Login with PayPal [KEYCLOAK-5623] - Integrate Keycloak via Jboss Fuse Fabric Profile resource Keycloak 3.3 Enhancement [KEYCLOAK-4756] - unversioned keycloak.js cache life too long [KEYCLOAK-5067] - Allow refreshable context to have an optional adapter token store [KEYCLOAK-5138] - Use build time in resource versions for snapshots [KEYCLOAK-5143] - Use auth-server-wildfly on Travis [KEYCLOAK-5180] - Add title attribute for keycloak-session-iframe to suppress accessibility errors [KEYCLOAK-5190] - Login with BitBucket [KEYCLOAK-5194] - Disabling Group and Role Permissions doesn't delete anything [KEYCLOAK-5242] - Add means to run test Keycloak server with https [KEYCLOAK-5285] - Improve possibility to extend FreeMarkerEmailTemplateProvider Feature Request [KEYCLOAK-3877] - Expose adapter config for public clients through the management interface [KEYCLOAK-4253] - Functional tests for quickstarts [KEYCLOAK-4439] - WildFly/EAP Management UI SSO [KEYCLOAK-4477] - Upgrade server to WildFly 11 Alpha1 [KEYCLOAK-4663] - Elytron subsystem [KEYCLOAK-4900] - Passing login_hint up to Identity Provider [KEYCLOAK-5086] - Provide Chinese translation in keycloak themes [KEYCLOAK-5203] - Keycloak Proxy Docker image [KEYCLOAK-5249] - gitlab.com identity provider [KEYCLOAK-5269] - account service unlink REST API doesn't work [KEYCLOAK-5291] - token references feature [KEYCLOAK-5307] - Provide Dutch translation in keycloak themes [KEYCLOAK-5319] - Source maps for keycloak.js Keycloak 3.2.0 Enhancement * [KEYCLOAK-3056] - Option to verify signature on SAML assertion in SAML Identity broker [KEYCLOAK-3631] - Make user actions tokens independently from -user- +client+ sessions so they can live a long time [KEYCLOAK-3988] - Multiple missing indexes on FKs [KEYCLOAK-3990] - Too many autoFlush checks by Hibernate and explicit em.flush() [KEYCLOAK-4016] - Provide a Link to go Back to The Application on a Timeout [KEYCLOAK-4097] - Better session sharing / handling of multiple logins [KEYCLOAK-4119] - Allow debugging Keycloak when running tests [KEYCLOAK-4497] - Update French Translation [KEYCLOAK-4670] - Back/forward/refresh button issues [KEYCLOAK-4765] - QueryParamTokenRequestAuthenticator fails when access_token query param is not a valid bearer [KEYCLOAK-4770] - Development version of keycloak-connect should depend on GitHub version of auth-utils [KEYCLOAK-4814] - disable security via configuration (i.e. for testing) in Spring Boot Adapter [KEYCLOAK-4862] - Expose client description in ClientBean [KEYCLOAK-4888] - Change default hashing provider for realm [KEYCLOAK-4889] - Improve error messages for password policies [KEYCLOAK-4929] - Refactor Authz caching and jpa API [KEYCLOAK-4933] - Use server-provisioning to create WildFly adapter dist and server overlay [KEYCLOAK-4940] - Typo in German email verification body [KEYCLOAK-4961] - Group policy [KEYCLOAK-5033] - Quickstarts integration tests should be parallelized on Travis CI [KEYCLOAK-5051] - Invalidate authz cache when realm cache is cleared [KEYCLOAK-5064] - add configuration property option realmPublicKey [KEYCLOAK-5069] - Provide a way to add a custom KeycloakConfigResolver instance for initialization [KEYCLOAK-5072] - JSPolicyProvider should use ScriptingSPI Feature Request [KEYCLOAK-3168] - Group-Based Access Control [KEYCLOAK-3297] - Add option for setting CORS Access-Control-Expose-Headers header [KEYCLOAK-3316] - Remove the IDToken if scope=openid is not used [KEYCLOAK-3444] - Fine-grained permissions in admin console and endpoints [KEYCLOAK-3592] - Docker Auth V2 Protocol Support [KEYCLOAK-4007] - Add integration tests for the nodejs adapter \"admin\" endpoints [KEYCLOAK-4204] - Extend brute force protection with permanent lockout on failed attempts [KEYCLOAK-4444] - Allow sending test email [KEYCLOAK-4773] - Remove 'providers' directory [KEYCLOAK-4815] - Making Proxy address forwarding configurable in Docker container [KEYCLOAK-4826] - Docker image for OpenShift [KEYCLOAK-4886] - Support creation of multiple OpenShift v3 Identity Providers [KEYCLOAK-4951] - Update Spring Boot documentation to reflect usage of the starter [KEYCLOAK-4955] - Partial export through admin console [KEYCLOAK-4965] - Node.js testsuite occasionally stuck on Travis CI [KEYCLOAK-4967] - Setup Travis CI to build development version of the quickstarts [KEYCLOAK-4980] - SAML adapter should return 401 when unauthenticated Ajax client accesses [KEYCLOAK-5082] - Unable to access webapp which URL being rewritten Keycloak Release 3.1.0-3 An update to the govuk themes from v1.1.0 to v1.1.1 (https://github.com/UKHomeOffice/keycloak-theme-govuk) this will change the govuk, govuk-internal and social themes You can find the Keycloak Release pages here . Note a JBOSS Developer login is required (it's free) v3.1.0 Enhancement: [KEYCLOAK-2122] - Config of AssertionConsumerServiceUrl in Saml Adapter [KEYCLOAK-4361] - Remove auth-server-standalone from domain.xml [KEYCLOAK-4502] - Update Russian translation [KEYCLOAK-4528] - Identity Provider for Openshift [KEYCLOAK-4602] - Improve path matcher when using handling patterns and caching [KEYCLOAK-4614] - Fix tooltip reference for linkOnly field in identity providers section [KEYCLOAK-4644] - Have option to not synchronize a linked account on login [KEYCLOAK-4652] - PolicyEvaluationService.evaluate() handles Role Policy for composite role incorrectly [KEYCLOAK-4664] - linking doesn't update token [KEYCLOAK-4665] - Parent IDP not logged out if account was linked during session [KEYCLOAK-4671] - Add server-private-spi to dependency deployer [KEYCLOAK-4727] - move PolicyEvaluationService into admin client [KEYCLOAK-4728] - Update French Translation [KEYCLOAK-4729] - Update German Translation [KEYCLOAK-4734] - Update Italian Translations [KEYCLOAK-4751] - Send default access denied page when requests don't match any path config [KEYCLOAK-4762] - Improve French translations [KEYCLOAK-4792] - AuthzClient should support credential types other than secret Feature Request: [KEYCLOAK-2604] - Proof Key for Code Exchange by OAuth Public Clients [KEYCLOAK-3468] - Upgrade server to WildFly 10.1.0.Final [KEYCLOAK-3573] - Elytron adapters [KEYCLOAK-3999] - Add Key Rotation support for Spring Security Adapter [KEYCLOAK-4000] - Add Key Rotation support for Spring Boot Adapter [KEYCLOAK-4163] - Improve support for e-mail addresses [KEYCLOAK-4168] - Add proxy debug endpoint [KEYCLOAK-4335] - X509 Certificate user authentication [KEYCLOAK-4396] - TypeScript type definitions for keycloak.js [KEYCLOAK-4691] - Extract Initial Token Generator to a separate method [KEYCLOAK-4697] - [RHSSO] Upgrade to EAP 7.1.0 Alpha16 [KEYCLOAK-4736] - Extend security defenses with header X-XSS-Protection [KEYCLOAK-4804] - Add spring-boot-container-bundle v3.0.0 Enhancement: [KEYCLOAK-3964] - No-import LDAP option [KEYCLOAK-3989] - Realm creation/deletion drops all admin composite roles and re-inserts them. [KEYCLOAK-4224] - Allow hiding identity providers on login page [KEYCLOAK-4362] - Split migration-domain script into two separate scripts [KEYCLOAK-4363] - ComponentFactory.onUpdate needs old model too [KEYCLOAK-4381] - Merge ModelReadOnlyException with ReadOnlyException [KEYCLOAK-4382] - Merge ModelReadOnlyException with ReadOnlyException [KEYCLOAK-4385] - Simple KeycloakConfigResolver to easily find keycloak.json inside OSGI bundle [KEYCLOAK-4475] - Minor improvements in hawtio integration docs [KEYCLOAK-4505] - ScriptBasedAuthenticator should expose clientSession as script binding [KEYCLOAK-4515] - Make it possible to clean-up other DB types than mysql or postgres [KEYCLOAK-4520] - Enable testsuite logging when running test from IDE Feature Request: [KEYCLOAK-3621] - Node.js service quickstart [KEYCLOAK-3955] - Add module for testing domain dependant tests [KEYCLOAK-4008] - Add checksums to file downloads [KEYCLOAK-4195] - Keycloak adapter and SPI bom [KEYCLOAK-4360] - Add OneTimeUse condition to SAMLResponse [KEYCLOAK-4501] - Federated identity management [KEYCLOAK-4504] - SAML Broker: Support redirect logout, even when using POST for SAML response [KEYCLOAK-4537] - Adapter for Jetty 9.4 [KEYCLOAK-4565] - Javadocument the adapter properties and add the metadata generator [KEYCLOAK-4581] - Swedish translations v2.5.0 Enhancement: [KEYCLOAK-2654] - KC invokes UserInfo Endpoint call against external Identity Provider [KEYCLOAK-2962] - OAuthRequestAuthenticator shouldn't redirect on XHR / AJAX requests [KEYCLOAK-3124] - Have adapter tests running on embedded undertow during default build [KEYCLOAK-3474] - Provide a filter to enable authorizations [KEYCLOAK-3678] - [Fuse] Add example using Camel RestDSL [KEYCLOAK-3823] - Adapters should clear key caches when not before policy is sent [KEYCLOAK-3933] - Remove UserFederationProvidersResource.getUserFederationInstanceWithFallback() once UserFederation SPI is removed [KEYCLOAK-3973] - Migration strategy for deprecated custom UserFed [KEYCLOAK-3987] - Grant the new role from the saml token if it exist [KEYCLOAK-4002] - realmRevisions cache size needs to be configurable [KEYCLOAK-4003] - Slow role checks on Infinispan RoleAdapter with composite roles. [KEYCLOAK-4004] - Display client name in referrer link instead of id [KEYCLOAK-4040] - Unable to import SAML metadata with OrganizationUrl [KEYCLOAK-4046] - Setting the 'Credentials - Temporary' flag when creating a new user causes the user to be disabled in MSAD [KEYCLOAK-4062] - Provide GUI for KeyName format in identity broker and client [KEYCLOAK-4074] - Decoupling of default provider implementations Feature Request: [KEYCLOAK-912] - Admin CLI [KEYCLOAK-3339] - Enable authorization services to EAP6 adapter [KEYCLOAK-3479] - Providers in JEE deployment [KEYCLOAK-3648] - Support for importing SAML response multi-valued attributes [KEYCLOAK-3731] - Support broker initiated SSO [KEYCLOAK-3824] - Add expiration to keys cached by clients [KEYCLOAK-4005] - Add support for \"not before\" in the NodeJS adapter [KEYCLOAK-4009] - Compatibility with AD LDS [KEYCLOAK-4018] - Client-Based Policy [KEYCLOAK-4059] - Support for duplicate emails [KEYCLOAK-4087] - LDAP group mapping should be possible via uid in memberUid mode [KEYCLOAK-4092] - Add key provider for HMAC signatures [KEYCLOAK-4109] - Ability to disable impersonation","title":"Keycloak"},{"location":"releases-notes/keycloak.html#keycloak","text":"","title":"Keycloak"},{"location":"releases-notes/kubernetes.html","text":"Kubernetes Platform # v0.2.19 Added Kubernetes conformance tests, extended the integration tests and use spot instances for E2E runs Calculate and set systemReserved kubeReserved kubelet flags, based on instance type per IG Mount encrypted docker volume for Master Compute Nodes Update default instance types for IGs Update CoreOS to stable release v1967.3.0 (including CVE fixes) v0.2.18 Upgrading to kops v0.2.7 due to an issue with the rbac kubelet-api manifest v0.2.17 Updated to the latest custom kops release v0.2.6 Added the fix for the kubelet webhook authorization Added kubelet webhook authorization to the kubelet for granular perms v0.2.16 Updates to the Kubernetes APIServer Audit Policy v0.2.15 Updated CoreOS release to v1911.4.0 Reduced verbosity of api audit logs Increased default value for master nodes root volumes v0.2.14 Refined the kubelet pod eviction policies v0.2.13 Updated the kubelet pod eviction policies v0.2.12 Updated to kubernetes v1.12.3 v0.2.11 Updating to kubernetes v1.12.1 #PR96 Changing to the recommended ipvs mode for the kube-proxy. #PR98 Added a sample psp and cluster role / binding to ease ephermal testing. #PR96 Update the CoreOS version to 1855.4.0 #PR99 Updating to Canel v3.2.3 #PR101 v0.2.10 Updating the fixed version of kops Added the token controller for bootstrap cleanup v0.2.9 Added the node authorizer to the mix #PR87 Fixed the s3 bucket bug #PR92 Fixed up the image to use the fixed node authorizer image #PR93 v0.2.8 Fixed the iptables restore created by the previous release v0.2.7 Allowing the blocking off vpc ssh to be optional, defaulting to true and mainly as an exception for acp-vpn v0.2.6 Fixed the E2E tests. This was broken due to PSP policies which was blocking the kuberang test from running. v0.2.5 Use AWS TimeSync service v0.2.4 Updating the base image to alpine 3.7 v0.2.3 (kops release) Bumped to custom-v0.1.3 , including a minor fix to the api-server AdmissionController flag use in k8s v1.1 v0.2.3 (ACP Build) Kubernetes v1.10.3: https://github.com/kubernetes/kubernetes/releases/tag/v1.10.3 v0.2.0 (ACP Build) Kubernetes v1.10.1: https://github.com/kubernetes/kubernetes/releases/tag/v1.10.1 CoreOS v1688.5.3: https://github.com/coreos/manifest/releases/tag/v1688.5.3 Etcd v3.3.3: https://github.com/coreos/etcd/blob/master/CHANGELOG-3.3.md#v333-2018-03-29 v0.1.0 (ACP Build) Kubernetes v1.8.4: https://github.com/kubernetes/kubernetes/releases/tag/v1.8.4 CoreOS v1632.2.1: https://github.com/coreos/manifest/releases/tag/v1632.2.1 Etcd v3.3.1: https://github.com/coreos/etcd/blob/master/CHANGELOG-3.3.md#v331-2018-02-12","title":"Kubernetes Platform"},{"location":"releases-notes/kubernetes.html#kubernetes-platform","text":"","title":"Kubernetes Platform"},{"location":"releases-notes/sonarqube.html","text":"SonarQube # 6.7.3.1 Nginx - Remove limit on body size 6.7.3 Update to: https://www.sonarsource.com/resources/product-news/news.html#2018-04-06-sonarqube-673-released Update all existing plugins to latest Install new plugins: https://github.com/stevespringett/dependency-check-sonar-plugin https://github.com/racodond/sonar-gherkin-plugin 6.7.1 Update to: https://www.sonarsource.com/resources/product-news/news.html#2017-12-21-sonarqube-671-released","title":"SonarQube"},{"location":"releases-notes/sonarqube.html#sonarqube","text":"","title":"SonarQube"},{"location":"releases-notes/sysdig.html","text":"Sysdig # 1629 Updated to hotfix to address the dashboards not showing current data issue, lagging x hours behind. 1586 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/v1586 1511 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/v1511 1472 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/v1472 987 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/987 925 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/925 893 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/893 886 Update to 886 (Release notes not available) Includes hotfix for AWS CloudWatch metrics 858 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/858 800 Update to: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/800 776 Update to Sysdig 776: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/776 760 Update to Sysdig 760: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/760 722 Update to Sysdig 722: https://github.com/draios/sysdigcloud-kubernetes/releases/tag/722","title":"Sysdig"},{"location":"releases-notes/sysdig.html#sysdig","text":"","title":"Sysdig"}]}