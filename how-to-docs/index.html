<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="None">
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>How To's - Application Container Platform</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "How To's";
    var mkdocs_page_input_path = "how-to-docs/index.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../index.html" class="icon icon-home"> Application Container Platform</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../index.html">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../services.html">Services</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../releases-notes/index.html">Release Notes</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="index.html">How To's</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#create-an-artifactory-access-token">Create an Artifactory access token</a></li>
    

    <li class="toctree-l2"><a href="#kubernetes-pod-autoscaling">Kubernetes Pod Autoscaling</a></li>
    

    <li class="toctree-l2"><a href="#certificates">Certificates</a></li>
    

    <li class="toctree-l2"><a href="#chisel">Chisel</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#a-working-example">A Working Example</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#debug-issues-with-your-deployments">Debug Issues with your deployments</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#debug-with-secrets">Debug with secrets</a></li>
        
            <li><a class="toctree-l3" href="#debugging-issues-with-your-deployments-to-the-platform">Debugging issues with your deployments to the platform</a></li>
        
            <li><a class="toctree-l3" href="#debugging-deployments">Debugging deployments</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#dms-migration">DMS Migration</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#prerequisite">Prerequisite</a></li>
        
            <li><a class="toctree-l3" href="#dms-setup">DMS Setup</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#downscaling-services-out-of-hours">Downscaling Services Out Of Hours</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#aws-rds-relational-database-service">AWS RDS (Relational Database Service)</a></li>
        
            <li><a class="toctree-l3" href="#kubernetes-pods">Kubernetes Pods</a></li>
        
            <li><a class="toctree-l3" href="#usage">Usage</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#drone-how-to">Drone How To</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#setup">Setup</a></li>
        
            <li><a class="toctree-l3" href="#publishing-docker-images">Publishing Docker images</a></li>
        
            <li><a class="toctree-l3" href="#deployments">Deployments</a></li>
        
            <li><a class="toctree-l3" href="#migrating-your-pipeline">Migrating your pipeline</a></li>
        
            <li><a class="toctree-l3" href="#qas">Q&amp;As</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#aws-ecr-for-private-docker-images">AWS ECR for Private Docker Images</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#creating-a-docker-repository">Creating a Docker Repository</a></li>
        
            <li><a class="toctree-l3" href="#generating-access-credentials">Generating Access Credentials</a></li>
        
            <li><a class="toctree-l3" href="#accessing-a-docker-repository">Accessing a Docker Repository</a></li>
        
            <li><a class="toctree-l3" href="#managing-image-deployments-via-drone-ci">Managing Image Deployments via Drone CI</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#using-ingress">Using Ingress</a></li>
    

    <li class="toctree-l2"><a href="#kube-cert-managaer-cloudflare-ssl">Kube-Cert-Managaer &amp; Cloudflare SSL</a></li>
    

    <li class="toctree-l2"><a href="#getting-a-kubernetes-robot-token">Getting a Kubernetes Robot Token</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#users">Users</a></li>
        
            <li><a class="toctree-l3" href="#project-admins-creating-a-robot-token">Project Admins (Creating a robot token)</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#getting-a-kubernetes-token">Getting a Kubernetes Token</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#users_1">Users</a></li>
        
            <li><a class="toctree-l3" href="#project-admins-creating-a-user-token">Project Admins (Creating a user token)</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#network-policies">Network Policies</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#controlling-egress-traffic">Controlling Egress Traffic</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#run-performance-tests-on-a-service-hosted-on-acp">Run Performance Tests on a service hosted on ACP</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#as-a-service-i-should">As a Service, I should:</a></li>
        
            <li><a class="toctree-l3" href="#assessed-tools-summary">Assessed tools summary:</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#pod-security-policies">Pod Security Policies</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#runasuser">runAsUser</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#using-artifactory-as-a-private-npm-registry">Using artifactory as a private npm registry</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#setting-up-a-local-environment">Setting up a local environment</a></li>
        
            <li><a class="toctree-l3" href="#setting-up-ci-in-drone">Setting up CI in drone</a></li>
        
            <li><a class="toctree-l3" href="#publishing-modules-to-artifactory">Publishing modules to artifactory</a></li>
        
            <li><a class="toctree-l3" href="#using-modules-from-artifactory-as-dependencies">Using modules from artifactory as dependencies</a></li>
        
            <li><a class="toctree-l3" href="#installing-dependencies-in-docker">Installing dependencies in docker</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#provisioned-volumes-and-storage-classes">Provisioned Volumes and Storage Classes</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#backups-for-ebs">Backups for EBS</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#tls-passthrough">TLS Passthrough</a></li>
    

    <li class="toctree-l2"><a href="#writing-dockerfiles">Writing Dockerfiles</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#dockerfile-best-practice">Dockerfile best practice</a></li>
        
            <li><a class="toctree-l3" href="#docker-images-to-build-from">Docker images to build from</a></li>
        
            <li><a class="toctree-l3" href="#technology-specific-images">Technology specific images</a></li>
        
            <li><a class="toctree-l3" href="#home-office-centos-base-image">Home Office CentOS base image</a></li>
        
            <li><a class="toctree-l3" href="#guidance-on-building-new-base-images">Guidance on building new base images</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../developer-docs/index.html">Developer Getting Started Guide</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../newuser.html">New User Guide</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../rbac.html">RBAC Guide</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../external-addresses.html">ACP External IPs</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../service-lifecycle.html">Service Lifecycle</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">Application Container Platform</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
    
      
    
    <li>How To's</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>This tree contains a collection of how-to guides for Developers.</p>
<div class="toc">
<ul>
<li><a href="#create-an-artifactory-access-token">Create an Artifactory access token</a></li>
<li><a href="#kubernetes-pod-autoscaling">Kubernetes Pod Autoscaling</a></li>
<li><a href="#certificates">Certificates</a></li>
<li><a href="#chisel">Chisel</a><ul>
<li><a href="#a-working-example">A Working Example</a></li>
</ul>
</li>
<li><a href="#debug-issues-with-your-deployments">Debug Issues with your deployments</a><ul>
<li><a href="#debug-with-secrets">Debug with secrets</a></li>
<li><a href="#debugging-issues-with-your-deployments-to-the-platform">Debugging issues with your deployments to the platform</a></li>
<li><a href="#debugging-deployments">Debugging deployments</a></li>
</ul>
</li>
<li><a href="#dms-migration">DMS Migration</a><ul>
<li><a href="#prerequisite">Prerequisite</a></li>
<li><a href="#dms-setup">DMS Setup</a></li>
</ul>
</li>
<li><a href="#downscaling-services-out-of-hours">Downscaling Services Out Of Hours</a><ul>
<li><a href="#aws-rds-relational-database-service">AWS RDS (Relational Database Service)</a></li>
<li><a href="#kubernetes-pods">Kubernetes Pods</a></li>
<li><a href="#usage">Usage</a></li>
</ul>
</li>
<li><a href="#drone-how-to">Drone How To</a><ul>
<li><a href="#setup">Setup</a><ul>
<li><a href="#install-drone-cli">Install Drone CLI</a></li>
<li><a href="#activate-your-pipeline">Activate your pipeline</a></li>
<li><a href="#configure-your-pipeline">Configure your pipeline</a></li>
</ul>
</li>
<li><a href="#publishing-docker-images">Publishing Docker images</a><ul>
<li><a href="#publishing-to-quay">Publishing to Quay</a></li>
<li><a href="#publishing-to-artifactory">Publishing to Artifactory</a></li>
</ul>
</li>
<li><a href="#deployments">Deployments</a><ul>
<li><a href="#deployments-and-promotions">Deployments and promotions</a></li>
<li><a href="#drone-as-a-pull-request-builder">Drone as a Pull Request builder</a></li>
<li><a href="#deploying-to-acp">Deploying to ACP</a></li>
<li><a href="#using-another-repo">Using Another Repo</a></li>
<li><a href="#versioned-deployments">Versioned deployments</a></li>
</ul>
</li>
<li><a href="#migrating-your-pipeline">Migrating your pipeline</a><ul>
<li><a href="#secrets-and-signing">Secrets and Signing</a></li>
<li><a href="#docker-in-docker">Docker-in-Docker</a></li>
<li><a href="#services">Services</a></li>
<li><a href="#variable-escaping">Variable Escaping</a></li>
<li><a href="#scanning-images">Scanning Images</a></li>
</ul>
</li>
<li><a href="#qas">Q&amp;As</a><ul>
<li><a href="#q-the-build-fails-with-error-insufficient-privileges-to-use-privileged-mode">Q: The build fails with "ERROR:Â Insufficient privileges to use privileged mode"</a></li>
<li><a href="#q-the-build-fails-with-cannot-connect-to-the-docker-daemon-is-the-docker-daemon-running-on-this-host">Q: The build fails with "Cannot connect to the Docker daemon. Is the docker daemon running on this host?"</a></li>
<li><a href="#q-the-build-fails-when-uploading-to-quay-with-the-error-error-response-from-daemon-get-httpsquayiov2-unauthorized">Q: The build fails when uploading to Quay with the error "Error response from daemon: Get https://quay.io/v2/: unauthorized:..."</a></li>
<li><a href="#q-as-part-of-my-build-process-i-have-two-dockerfiles-to-produce-a-docker-image-how-can-i-share-files-between-builds-in-the-same-step">Q: As part of my build process I have two Dockerfiles to produce a Docker image. How can I share files between builds in the same step?</a></li>
<li><a href="#q-should-i-use-gitlab-with-quay">Q: Should I use Gitlab with Quay?</a></li>
<li><a href="#q-can-i-create-a-token-that-has-permission-to-create-ephemeraltemporary-namespaces">Q: Can I create a token that has permission to create ephemeral/temporary namespaces?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#aws-ecr-for-private-docker-images">AWS ECR for Private Docker Images</a><ul>
<li><a href="#creating-a-docker-repository">Creating a Docker Repository</a></li>
<li><a href="#generating-access-credentials">Generating Access Credentials</a></li>
<li><a href="#accessing-a-docker-repository">Accessing a Docker Repository</a></li>
<li><a href="#managing-image-deployments-via-drone-ci">Managing Image Deployments via Drone CI</a></li>
</ul>
</li>
<li><a href="#using-ingress">Using Ingress</a></li>
<li><a href="#kube-cert-managaer-cloudflare-ssl">Kube-Cert-Managaer &amp; Cloudflare SSL</a></li>
<li><a href="#getting-a-kubernetes-robot-token">Getting a Kubernetes Robot Token</a><ul>
<li><a href="#users">Users</a></li>
<li><a href="#project-admins-creating-a-robot-token">Project Admins (Creating a robot token)</a></li>
</ul>
</li>
<li><a href="#getting-a-kubernetes-token">Getting a Kubernetes Token</a><ul>
<li><a href="#users_1">Users</a></li>
<li><a href="#project-admins-creating-a-user-token">Project Admins (Creating a user token)</a></li>
</ul>
</li>
<li><a href="#network-policies">Network Policies</a><ul>
<li><a href="#controlling-egress-traffic">Controlling Egress Traffic</a></li>
</ul>
</li>
<li><a href="#run-performance-tests-on-a-service-hosted-on-acp">Run Performance Tests on a service hosted on ACP</a><ul>
<li><a href="#as-a-service-i-should">As a Service, I should:</a></li>
<li><a href="#assessed-tools-summary">Assessed tools summary:</a></li>
</ul>
</li>
<li><a href="#pod-security-policies">Pod Security Policies</a><ul>
<li><a href="#runasuser">runAsUser</a><ul>
<li><a href="#dockerfile">Dockerfile</a></li>
<li><a href="#deployment-spec">Deployment Spec</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#using-artifactory-as-a-private-npm-registry">Using artifactory as a private npm registry</a><ul>
<li><a href="#setting-up-a-local-environment">Setting up a local environment</a><ul>
<li><a href="#get-your-username-and-api-key-from-artifactory">Get your username and API key from artifactory</a></li>
<li><a href="#base64-encode-your-api-key">base64 encode your API key</a></li>
<li><a href="#set-local-environment-variables">Set local environment variables</a></li>
</ul>
</li>
<li><a href="#setting-up-ci-in-drone">Setting up CI in drone</a><ul>
<li><a href="#request-a-bot-token-for-artifactory">Request a bot token for artifactory</a></li>
<li><a href="#add-the-token-to-drone-as-a-secret">Add the token to drone as a secret</a></li>
<li><a href="#expose-secret-to-build-steps">Expose secret to build steps</a></li>
<li><a href="#expose-username-to-build-steps">Expose username to build steps</a></li>
</ul>
</li>
<li><a href="#publishing-modules-to-artifactory">Publishing modules to artifactory</a><ul>
<li><a href="#setting-publish-registry">Setting publish registry</a></li>
<li><a href="#add-auth-settings">Add auth settings</a></li>
<li><a href="#add-publish-step-to-drone">Add publish step to drone</a></li>
</ul>
</li>
<li><a href="#using-modules-from-artifactory-as-dependencies">Using modules from artifactory as dependencies</a><ul>
<li><a href="#configure-your-project-to-use-artifactory">Configure your project to use artifactory</a></li>
</ul>
</li>
<li><a href="#installing-dependencies-in-docker">Installing dependencies in docker</a></li>
</ul>
</li>
<li><a href="#provisioned-volumes-and-storage-classes">Provisioned Volumes and Storage Classes</a><ul>
<li><a href="#backups-for-ebs">Backups for EBS</a></li>
</ul>
</li>
<li><a href="#tls-passthrough">TLS Passthrough</a></li>
<li><a href="#writing-dockerfiles">Writing Dockerfiles</a><ul>
<li><a href="#dockerfile-best-practice">Dockerfile best practice</a></li>
<li><a href="#docker-images-to-build-from">Docker images to build from</a></li>
<li><a href="#technology-specific-images">Technology specific images</a></li>
<li><a href="#home-office-centos-base-image">Home Office CentOS base image</a></li>
<li><a href="#guidance-on-building-new-base-images">Guidance on building new base images</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="create-an-artifactory-access-token">Create an Artifactory access token<a class="headerlink" href="#create-an-artifactory-access-token" title="Permanent link">#</a></h1>
<blockquote>
<p>Note: These instructions are intended for the ACP team. If you would like to request an Artifactory token, please use the relevant support request on the <a href="https://hub.acp.homeoffice.gov.uk/help/support/requests/new/artifactory-token">Platform Hub</a>.</p>
</blockquote>
<p>The requester should state the name of the token, how they would like to receive the token and post their GPG key.</p>
<ul>
<li>Create an <a href="https://docker.digital.homeoffice.gov.uk">Artifactory</a> access token using the following command:</li>
</ul>
<pre><code>curl -u&lt;username&gt;:&lt;api-key&gt; -XPOST &quot;https://artifactory.digital.homeoffice.gov.uk/artifactory/api/security/token&quot; -d &quot;username=&lt;robot-username&gt;&quot; -d &quot;scope=member-of-groups:&lt;appropriate-groups&gt;&quot; -d &quot;expires_in=0&quot;
</code></pre>

<p>where <code>&lt;robot-username&gt;</code> is the name of the access token and <code>&lt;appropriate-groups&gt;</code> is a comma separated list of the groups the token should be in (normally this will only be <code>ci</code>).</p>
<blockquote>
<p>Note: If you set the <code>expires_in</code> time higher than 0, you will not be able to revoke the token via the UI.</p>
</blockquote>
<ul>
<li>Once the token has been created, JSON data should be returned which will include the access key. The JSON data you receive will be the only time you will be able to see the access key as it is not shown on Artifactory. You should, however, be able to see the name and expiry date (if you set an expiry time) of the access key in the "Access Keys" section.</li>
</ul>
<h1 id="kubernetes-pod-autoscaling">Kubernetes Pod Autoscaling<a class="headerlink" href="#kubernetes-pod-autoscaling" title="Permanent link">#</a></h1>
<p>For full documentation on kubernetes autoscaling feature please go <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">here</a>. As of writing the ACP cluster supports standard autoscaling based on a CPU metric, there are however plans to support custom-metrics in the near future.</p>
<p>Assuming you have a deployment 'web' and you wish to autoscale the deployment when it hit's a 40% CPU usage with min/max of 5/10 pods.</p>
<pre><code class="YAML">apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: web
spec:
  maxReplicas: 10
  minReplicas: 5
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: web
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageUtilization: 50
</code></pre>

<p><strong>Sysdig Metrics - Experimental</strong></p>
<p>The autoscaler can also consume and make scaling decisions from <a href="https://sysdig.digital.homeoffice.gov.uk">sysdig</a> metrics. Note, this feature is currently experimental but tested as working.</p>
<p>An example of sysdig would be scaling on http_request</p>
<pre><code class="YAML">apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: autoscaler
spec:
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: myapplication
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Object
    object:
      target:
        kind: Service
        name: myservice
      metricName: net.http.request.count
      targetValue: 100
</code></pre>

<h1 id="certificates">Certificates<a class="headerlink" href="#certificates" title="Permanent link">#</a></h1>
<h4><strong>Application Certificates</strong></h4>
<hr />
<p>Before reading about certificates and how you can create and manage them. <a href="../services/#domain-name-system-dns-pattern">Please familiarise yourself with our DNS naming convention first</a></p>
<p>The platform provides two ways of managing <a href="https://en.wikipedia.org/wiki/HTTPS">HTTPS certificates</a>:
- <a href="kube-cert-manager.html">Internal based certificates</a> i.e. <code>hostname.namespace.svc.cluster.local</code> using CFSSL
- <a href="ingress.html">External based certificates</a> for external services i.e. <code>service.homeoffice.gov.uk</code> using kube-cert-manager and letsencrypt</p>
<p>In most systems, it's likely that your service will have a user facing service, that will be served through an external endpoint i.e. it can be routed to externally by users as well as having non-external facing services i.e. internal services.</p>
<p>You would want all communication between the user, through to the service and service dependencies to be encrypted, so that the traffic flow has encryption and that all endpoints trust who they are speaking to.</p>
<h4><strong>Certificates</strong></h4>
<ul>
<li><a href="kube-cert-manager.html">kube-cert-manager and cfssl</a></li>
</ul>
<h4><strong>LetsEncrypt Limits</strong></h4>
<p>Note Letsencrypt while a free service does come with a number of service limits detailed <a href="https://letsencrypt.org/docs/rate-limits/">here</a>. Probably one of the most crucial for projects is the max certificate requests per week; currently standing at 20. In addition, there is a max 5 failures for per hostname with a freeze of 1 hour, so if you accidently mess up configuration you might hit this.</p>
<h1 id="chisel">Chisel<a class="headerlink" href="#chisel" title="Permanent link">#</a></h1>
<blockquote>
<p><em>The Problem</em>: we want to provide services running in ACP access to the third party services as well as the ability to have user-based access controls. At present network access in ACP is provided via Calico, but this becomes redundant when the traffic egresses the cluster. Simply peering networks together either through VPC peering or VPN connections doesn't provide the controls we want. We could rely on user-authentication on third-party service but not all services are authenticated (take POISE) and beyond that peering networks provides no means of auditing traffic that is traversing the bridged networks.</p>
</blockquote>
<p>One pattern we are exploring is the use of a proxy cluster with an authenticated side-kick to route traffic and provide end-to-end encryption. Both ACP Notprod and Prod are peered to an respective proxy cluster that is running a <a href="https://github.com/jpillora/chisel">Chisel</a> server. Below is rough idea of how the chisel service works.</p>
<p><img alt="alt text" src="https://github.com/UKHomeOffice/application-container-platform/blob/master/how-to-docs/pics/chisel.png" title="Chisel" /></p>
<p>The workflow for this is as follows, note the following example is assuming we have peered with a network in the proxy cluster which is exposing x services.</p>
<ul>
<li>A request via BAU the provisioning of a service on the Chisel server.</li>
<li>Once done user is provided credentials for service.</li>
<li>You add into your deployment a chisel container running in client mode and add the configuration as described to route the traffic. In regard to DNS and hostnames, kubernetes pods permit the user to add host entries into the container DNS, enabling you to override.</li>
<li>The traffic is picked up, encrypted over an ssh tunnel and pushed to the Chisel server where the user credentials are evaluated. Assuming everything is ok the traffic is then proxied on to destination.</li>
</ul>
<h3 id="a-working-example">A Working Example<a class="headerlink" href="#a-working-example" title="Permanent link">#</a></h3>
<p>We have a two services called <code>example-api.internal.homeoffice.gov.uk</code> and <code>another-service.example.com</code> and we wish to consume the API from the pods. Lets assume the service has already been provisioned on the Chisel server and we have the credentials at hand.</p>
<pre><code class="YAML">kind: Deployment
metadata:
  name: consumer
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: consumer
    spec:
      hostAliases:
      - hostnames:
        - another-service.example.com
        - example-api.internal.homeoffice.gov.uk
        ip: 127.0.0.1
      securityContext:
        fsGroup: 1000
      volumes:
      - name: bundle
        configMap:
          name: bundle
      containers:
      - name: consumer
        image: quay.io/ukhomeofficedigital/someimage:someversion
      - name: chisel
        image: quay.io/ukhomeofficedigital/chisel:latest
        securityContext:
          runAsNonRoot: true
        env:
        # essentially user:password
        - name: AUTH
          valueFrom:
            secretKeyRef:
              name: chisel
              key: chisel.auth
        # this optional BUT recommended this is fingerprint for the SSH service
        - name: CHISEL_KEY
          valueFrom:
            secretKeyRef:
              name: chisel
              key: chisel.key
        args:
        - client
        - -v
        # this the chisel endpoint service hostname
        - gateway-internal.px.notprod.acp.homeoffice.gov.uk:443
        # this is saying listen on port 10443 and route all traffic to another-service.example.com:443 endpoint
        - 127.0.0.1:10443:another-service.example.com:443
        - 127.0.0.1:10444:example-api.internal.homeoffice.gov.uk:443
        volumeMounts:
        - name: bundle
          mountPath: /etc/ssl/certs
          readOnly: true
</code></pre>

<p>The above embeds the sidekick into the Pod and requests the client to listen on localhost:10443 and 10444 to redirect traffic via the Chisel service. The one annoying point here is the port requirements, placing things on different ports, but unfortunately this is required. You should be able to call the service via <code>curl https://another-service.example.com:10443</code> at this point.</p>
<h1 id="debug-issues-with-your-deployments">Debug Issues with your deployments<a class="headerlink" href="#debug-issues-with-your-deployments" title="Permanent link">#</a></h1>
<h3 id="debug-with-secrets">Debug with secrets<a class="headerlink" href="#debug-with-secrets" title="Permanent link">#</a></h3>
<p>Sometimes your app doesn't want to talk to an API or a DB and you've stored the credentials or just the details of that in secret.</p>
<p>The following approaches can be used to validate that your secret is set correctly</p>
<pre><code class="bash">$ kubectl exec -ti my-pod -c my-container -- mysql -h\$DBHOST -u\$DBUSER -p\$DBPASS
## or
$ kubectl exec -ti my-pod -c my-container -- openssl verify /secrets/certificate.pem
## or
$ kubectl exec -ti my-pod -c my-container bash
## and you'll naturally have all the environment variables set and volumes mounted.
## however we recommend against outputing them to the console e.g. echo $DBHOST
## instead if you want to assert a variable is set correctly use
$ [[ -z $DBHOST ]]; echo $?
## if it returns 1 then the variable is set.
</code></pre>

<h3 id="debugging-issues-with-your-deployments-to-the-platform">Debugging issues with your deployments to the platform<a class="headerlink" href="#debugging-issues-with-your-deployments-to-the-platform" title="Permanent link">#</a></h3>
<p>If you get to the end of the above guide but can't access your application there are a number of places something could be going wrong.
This section of the guide aims to give you some basic starting points for how to debug your application.</p>
<h3 id="debugging-deployments">Debugging deployments<a class="headerlink" href="#debugging-deployments" title="Permanent link">#</a></h3>
<p>We suggest the following steps:</p>
<h4>1. Check your deployment, replicaset and pods created properly</h4>
<pre><code class="bash">$ kubectl get deployments
$ kubectl get rs
$ kubectl get pods
</code></pre>

<h4>2. Investigate potential issues with your pods (this is most likely)</h4>
<p>If the get pods command shows that your pods aren't all running then this is likely where the issue is. You can then try curling your application to see if it is alive and responding as expected. e.g.</p>
<pre><code class="bash">$ curl localhost:4000
</code></pre>

<p>You can get further details on why the pods couldn't be deployed by running:</p>
<pre><code class="bash">$ kubectl describe pods *pods_name_here*
</code></pre>

<p>If your pods are running you can check they are operating as expected by <code>exec</code>ing into them (this gets you a shell on one of your containers).</p>
<pre><code class="bash">$ kubectl exec -ti *pods_name_here* -c *container_name_here* /bin/sh
</code></pre>

<blockquote>
<p><strong>Please note</strong> that the <code>-c</code> argument isn't needed if there is only one container in the pod.*</p>
</blockquote>
<h4>3. Investigate potential issues with your service</h4>
<p>A good way to do this is to run a container in your namespace with a bash terminal:</p>
<pre><code class="bash">$ kubectl run -ti --image quay.io/ukhomeofficedigital/centos-base debugger bash
</code></pre>

<p>From this container you can then try curling your service. Your service will have a nice DNS name by default, so you can for example run:</p>
<pre><code class="bash">$ curl my-service-name
</code></pre>

<h4>4. Investigate potential issues with ingress</h4>
<p>Minikube runs an ingress service using nginx. It's possible to ssh into the nginx container and cat the <code>nginx.conf</code> to inspect the configuration for nginx.</p>
<p>In order to attach to the nginx container, you need to know the name of the container:</p>
<pre><code class="shell">$ kubectl get pods
NAME                               READY     STATUS    RESTARTS   AGE
default-http-backend-2kodr         1/1       Running   1          5d
acp-hello-world-3757754181-x1kdu   1/1       Running   2          6d
ingress-3879072234-5f4uq           1/1       Running   2          5d
</code></pre>

<p>You can attach to the running container with:</p>
<pre><code class="bash">$ kubectl exec -ti &lt;ingress-3879072234-5f4uq&gt; -c &lt;proxy&gt; bash
</code></pre>

<p>where <code>&lt;proxy&gt;</code> is the container name of the nginx proxy inside the pod. You can find the name by describing the pod.</p>
<p>You're inside the container. You can cat the <code>nginx.conf</code> with:</p>
<pre><code class="bash">$ cat /etc/nginx/nginx.conf
</code></pre>

<p>You can also inspect the logs with:</p>
<pre><code class="bash">$ kubectl logs &lt;ingress-3879072234-5f4uq&gt;
</code></pre>

<h1 id="dms-migration">DMS Migration<a class="headerlink" href="#dms-migration" title="Permanent link">#</a></h1>
<h3 id="prerequisite"><strong>Prerequisite</strong><a class="headerlink" href="#prerequisite" title="Permanent link">#</a></h3>
<p>The following need to be true before you follow this guide:
<em> AWS console logon
</em> Access to the DMS service from console
* A region where STS has been activated</p>
<h3 id="dms-setup"><strong>DMS Setup</strong><a class="headerlink" href="#dms-setup" title="Permanent link">#</a></h3>
<p>Login to the AWS console using your auth, switch to a role with the correct access policies and verify you're in the right region. Next, select DMS from the services on the main dashboard to access the data migration home screen. Under the "Get started" section click on the "create migration" button then next to the Replication instance. You should see the following screen:</p>
<p><img alt="Alt text" src="pics/dms-doc-1.png?raw=true" /></p>
<p>The following are the options and example answers for the replication instance:</p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Example answer</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>dev-team-dms</td>
<td>A name for the replication image. This name should be unique.</td>
</tr>
<tr>
<td>Description</td>
<td>DMS instance for migration</td>
<td>Brief description of the instance</td>
</tr>
<tr>
<td>Instance class</td>
<td>dms.t2.medium</td>
<td>The class of replication resource with the configuration you need for your migration.</td>
</tr>
<tr>
<td>VPC</td>
<td>vpc-*</td>
<td>The virtual private cloud resource where you wish to add your dms instance. This should be as close to both the source and target instance as possible.</td>
</tr>
<tr>
<td>Multi-AZ</td>
<td>No</td>
<td>Optional parameter to create a standby replica of your replication instance in another Availability Zone. Used for failover.</td>
</tr>
<tr>
<td>Publicly Accessible</td>
<td>False</td>
<td>Option to access your instance from the internet</td>
</tr>
</tbody>
</table>
<p>You won't need to set any of the advanced settings. To create the instance click on the next button. You should now see a screen like this:</p>
<p><img alt="Alt text" src="pics/dms-doc-2.png?raw=true" /></p>
<p>The following are the options and example answers for the endpoints instances:</p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Example answer</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Endpoint identifer</td>
<td>database-source/target</td>
<td>This is the name you use to identify the endpoint.</td>
</tr>
<tr>
<td>Source/target engine</td>
<td>postgres</td>
<td>Choose the type of database engine that for this endpoint.</td>
</tr>
<tr>
<td>Server name</td>
<td>mysqlsrvinst.abcd123456789.us-west-1.rds.amazonaws.com</td>
<td>Type of server name. For an on-premises database, this can be the IP address or the public hostname. For an Amazon RDS DB instance, this can be the endpoint for the DB instance.</td>
</tr>
<tr>
<td>Port</td>
<td>5432</td>
<td>The port used by the database.</td>
</tr>
<tr>
<td>SSL mode</td>
<td>None</td>
<td>SSL mode for encryption for your endpoints.</td>
</tr>
<tr>
<td>Username</td>
<td>root</td>
<td>The user name with the permissions required to allow data migration.</td>
</tr>
<tr>
<td>Password</td>
<td><strong><em>*</em></strong>*</td>
<td>The password for the account with the required permissions.</td>
</tr>
<tr>
<td>Database Name (target)</td>
<td>dev-db</td>
<td>The name of the attached database to the selected endpoint.</td>
</tr>
</tbody>
</table>
<p>Repeat these options for both source and target and make sure to test connection before clicking next. You might need to append security group rules to allow the replication instance access, for example:</p>
<p>Replication instance has internal ip address 10.20.0.0 and the RDS is on port 5432 and uses TCP. Append rule</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Procol</th>
<th>Port Range</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>Custom TCP rule</td>
<td>TCP</td>
<td>5432</td>
<td>Custom 10.20.0.0/32</td>
</tr>
</tbody>
</table>
<p>Once this has fully been setup click next and you should be able to view the tasks page:</p>
<p><img alt="Alt text" src="pics/dms-doc-3.png?raw=true" />
<img alt="Alt text" src="pics/dms-doc-4.png?raw=true" /></p>
<p>The following are the options and example answers for these tasks:</p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Example answer</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Task name</td>
<td>Migration-task</td>
<td>A name for the task.</td>
</tr>
<tr>
<td>Task Description</td>
<td>Task for migrating</td>
<td>A description for the task.</td>
</tr>
<tr>
<td>Source endpoint</td>
<td>source-instance</td>
<td>The source endpoint for migration.</td>
</tr>
<tr>
<td>Target endpoint</td>
<td>target-instance</td>
<td>The target endpoint for migration.</td>
</tr>
<tr>
<td>Replication instance</td>
<td>replication-instance</td>
<td>The replication instance to be used.</td>
</tr>
<tr>
<td>Migration type</td>
<td>Migrate existing data</td>
<td>Migration method you want to use.</td>
</tr>
<tr>
<td>Start task on create</td>
<td>True</td>
<td>When selected the task begins as soon as it is created.</td>
</tr>
<tr>
<td>Target table preparation</td>
<td>Drop table on target</td>
<td>Migration strategy on target.</td>
</tr>
<tr>
<td>Include LOB columns in replication</td>
<td>Limited LOB mode</td>
<td>Migration of large objects on target.</td>
</tr>
<tr>
<td>Max LOB size</td>
<td>32 kb</td>
<td>Maximum size of large objects.</td>
</tr>
<tr>
<td>Enable logging</td>
<td>False</td>
<td>When selected migration events are logged.</td>
</tr>
</tbody>
</table>
<p>After completion the job will automatically run if "start task on create" has been selected. If not, the job can be started in the tasks section by selecting it and clicking on the "Start/Resume" button.</p>
<h1 id="downscaling-services-out-of-hours">Downscaling Services Out Of Hours<a class="headerlink" href="#downscaling-services-out-of-hours" title="Permanent link">#</a></h1>
<p>In an effort to reduce costs on running the platform, we've enabled to capability to scale down specific resources Out Of Hours (OOH) for Non-Production environments.</p>
<h3 id="aws-rds-relational-database-service">AWS RDS (Relational Database Service)<a class="headerlink" href="#aws-rds-relational-database-service" title="Permanent link">#</a></h3>
<p>Non-Production RDS resources can be transitioned to a stopped state OOH to save on resource utilisation costs. This is currently managed with the use of tags on the RDS instance defining a cronjob schedule to stop and start the instance.</p>
<p>To set a schedule for your RDS instances, please use the related Platform Hub support request template titled <strong><a href="https://hub.acp.homeoffice.gov.uk/help/support/requests/new/rds-scheduling">"Shutdown RDS Instance(s) Out Of Hours"</a></strong>.</p>
<blockquote>
<p><strong>Note:</strong> Shutting down an RDS instance will have cost savings based on the instance size, however you will still be charged for the allocated storage.</p>
</blockquote>
<h3 id="kubernetes-pods">Kubernetes Pods<a class="headerlink" href="#kubernetes-pods" title="Permanent link">#</a></h3>
<p>Automatically scale down Kubernetes Deployments &amp; Statefulsets to 0 replicas during non-working hours for Non-Production or Production Environments.</p>
<p>Downscaling for Deployments &amp; Statefulsets are managed by an annotation set within the manifest, and are processed every 30 seconds for changes, by a service running within the Kubernetes Clusters.</p>
<h3 id="usage">Usage<a class="headerlink" href="#usage" title="Permanent link">#</a></h3>
<p>Set <strong>ONE</strong> of the following annotations on your Deployment / Statefulset:
- <code>downscaler/uptime</code>: A time schedule in which the Deployment should be scaled up
- <code>downscaler/downtime</code>: A time schedule in which the Deployment should be scaled down to 0 replicas</p>
<p>The annotation values for the timeframe must have the following format to be processed correctly: <code>&lt;WEEKDAY-FROM&gt;-&lt;WEEKDAY-TO-INCLUSIVE&gt; &lt;HH&gt;:&lt;MM&gt;-&lt;HH&gt;:&lt;MM&gt; &lt;TIMEZONE&gt;</code></p>
<p>For example, to schedule a Deployment to only run on weekdays during working hours, the following annotation would be set: <code>downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London</code></p>
<blockquote>
<p><strong>Note:</strong> When the deployment is downscaled, an additional annotation <code>downscaler/original-replicas</code> is automatically set to retain a history of the desired replicas prior to the downscale action. If this annotation has been deleted before the service is automatically scaled back up, the downscaler service will not know what to set the replicas back to, and so it won't attempt to scale up the resource.</p>
</blockquote>
<p><strong>Example Spec:</strong></p>
<pre><code class="yml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    downscaler/uptime: Mon-Fri 09:00-17:30 Europe/London
  labels:
    name: example-app
  name: example-app
  namespace: acp-example
spec:
  replicas: 2
  template:
    spec:
      containers:
        image: docker.digital.homeoffice.gov.uk/acp-example-app:v0.0.1@sha256:07397c41ac25c4b19e0485006849201f04168703f0016fad75b8ba5d9885d6d4
...
</code></pre>

<h1 id="drone-how-to">Drone How To<a class="headerlink" href="#drone-how-to" title="Permanent link">#</a></h1>
<ul>
<li>Setup</li>
<li><a href="#install-drone-cli">Install Drone CLI</a></li>
<li><a href="#activate-your-pipeline">Activate your pipeline</a></li>
<li>Adding a repository to Drone</li>
<li><a href="#configure-your-pipeline">Configure your pipeline</a></li>
<li>Publishing Docker images to</li>
<li><a href="#publishing-to-quay">Quay</a></li>
<li><a href="#publishing-to-artifactory">Artifactory</a></li>
<li>Deployments</li>
<li><a href="#deployments-and-promotions">Deployments and promotions</a></li>
<li><a href="#drone-as-a-pull-request-builder">Drone as a pull request builder</a></li>
<li><a href="#deploying-to-acp">Deploying to ACP</a></li>
<li><a href="#using-another-repo">Using Another Repo</a></li>
<li><a href="#versioned-deployments">Versioned deployments</a></li>
<li>Migrating your Pipeline</li>
<li><a href="#secrets-and-signing">Secrets and Signing</a></li>
<li><a href="#docker-in-docker">Docker in Docker</a></li>
<li><a href="#services">Services</a></li>
<li><a href="#variable-escaping">Variable Escaping</a></li>
<li>Scanning for Vulnerabilities</li>
<li><a href="#scanning-images">Scanning Images</a></li>
<li><a href="#qas">QAs</a></li>
</ul>
<h2 id="setup">Setup<a class="headerlink" href="#setup" title="Permanent link">#</a></h2>
<h3 id="install-drone-cli">Install Drone CLI<a class="headerlink" href="#install-drone-cli" title="Permanent link">#</a></h3>
<ul>
<li>Github drone instance: https://drone.acp.homeoffice.gov.uk/</li>
<li>Gitlab drone instance: https://drone-gitlab.acp.homeoffice.gov.uk/</li>
</ul>
<p>Download and install the <a href="https://docs.drone.io/cli-installation/">Drone CLI</a>.</p>
<blockquote>
<p>At the time of writing, we are using version 0.8 of Drone.</p>
</blockquote>
<p>You can also install a release from <a href="https://github.com/drone/drone-cli/releases">Drone CLI's GitHub repo</a>.
Once you have downloaded the relevant file, extract it and move it to the <code>/usr/local/bin</code> directory.</p>
<p>Verify it works as expected:</p>
<pre><code class="bash">$ drone --version
drone version 0.8.0
</code></pre>

<p>Export the <code>DRONE_SERVER</code> and <code>DRONE_TOKEN</code> variables. You can find your token on Drone by clicking the icon in the top right corner and going to <a href="https://drone.acp.homeoffice.gov.uk/account/token">Token</a>.</p>
<pre><code class="bash">export DRONE_SERVER=https://drone.acp.homeoffice.gov.uk
export DRONE_TOKEN=&lt;your_drone_token&gt;
</code></pre>

<p>If your installation is successful, you should be able to query the current Drone instance:</p>
<pre><code class="bash">$ drone info
User: youruser
Email: youremail@gmail.com
</code></pre>

<blockquote>
<p>If the output is</p>
<p><code>bash
Error: you must provide the Drone server address.</code></p>
<p>or</p>
<p><code>Error: you must provide your Drone access token.</code></p>
<p>Please make sure that you have exported the <code>DRONE_SERVER</code> and <code>DRONE_TOKEN</code> variables properly.</p>
</blockquote>
<h3 id="activate-your-pipeline">Activate your pipeline<a class="headerlink" href="#activate-your-pipeline" title="Permanent link">#</a></h3>
<p>Once you are logged in to Drone, you will find a list of repos by clicking the icon in the top right corner and going to <a href="https://drone.acp.homeoffice.gov.uk/account/repos">Repositories</a>.</p>
<p>Select the repo you want to activate.</p>
<p>Navigate to your repository's settings in Github (or Gitlab) and you will see a webhook has been created. You need to update the url for the newly created web hook so that it matches this pattern:</p>
<pre><code>https://drone-external.acp.homeoffice.gov.uk/hook?access_token=some_token
</code></pre>

<blockquote>
<p>If it is already in that format there is no need to change anything.</p>
<p>The token in the payload url will not be the same as the personal token that you exported and it should be left unchanged.</p>
<p><strong>Please note that this does not apply to Gitlab. When you activate the repo in Drone, you should not change anything for a Gitlab repo.</strong></p>
</blockquote>
<h3 id="configure-your-pipeline">Configure your pipeline<a class="headerlink" href="#configure-your-pipeline" title="Permanent link">#</a></h3>
<p>In the root folder of your project, create a <code>.drone.yml</code> file with the following content:</p>
<pre><code class="yaml">pipeline:

  my-build:
    image: docker:18.03
    environment:
      - DOCKER_HOST=tcp://172.17.0.1:2375
    commands:
      - docker build -t &lt;image_name&gt; .
    when:
      branch: master
      event: push
</code></pre>

<p>Commit and push your changes:</p>
<pre><code class="bash">$ git add .drone.yml
$ git commit
$ git push origin master
</code></pre>

<blockquote>
<p><strong>Please note</strong> you should replace the name &lt;...&gt; with the name of your app.</p>
</blockquote>
<p>You should be able to watch your build succeed in the Drone UI.</p>
<h2 id="publishing-docker-images">Publishing Docker images<a class="headerlink" href="#publishing-docker-images" title="Permanent link">#</a></h2>
<h3 id="publishing-to-quay">Publishing to Quay<a class="headerlink" href="#publishing-to-quay" title="Permanent link">#</a></h3>
<p>If your repository is hosted on Gitlab, you don't want to publish your images to Quay. Images published to Quay are public and can be inspected and downloaded by anyone. <a href="#publishing-to-artifactory">You should publish your private images to Artifactory</a>.</p>
<p>Register for a free <a href="https://quay.io">Quay account</a> using your Github account linked to the Home Office organisation.</p>
<p>Once you've logged into Quay check that you have <code>ukhomeofficedigital</code> under Users and Organisations.
If you do not, <a href="https://hub.acp.homeoffice.gov.uk/help/support/requests/new/add-to-quay-org">submit a support request on the platform hub for access to the ukhomeoffice organisation</a>.</p>
<p>Once you have access to view the <code>ukhomeofficedigital</code> repositories, click repositories and
click the <code>+ Create New Repositories</code> that is:</p>
<ul>
<li>public</li>
<li>empty - no need to create a repo from a Dockerfile or link it to an existing repository</li>
</ul>
<p>Add your project to the UKHomeOffice Quay account and <a href="https://hub.acp.homeoffice.gov.uk/help/support/requests/new/quay-robot-request">submit a support request on the platform hub for a new Quay robot</a>.</p>
<p>Add the step to publish the docker image to Quay in your Drone pipeline:</p>
<pre><code class="yaml">image_to_quay:
  image: quay.io/ukhomeofficedigital/drone-docker
  secrets:
    - docker_password
  environment:
    - DOCKER_USERNAME=ukhomeofficedigital+&lt;your_robot_username&gt;
  registry: quay.io
  repo: quay.io/ukhomeofficedigital/&lt;your_quay_repo&gt;
  tags:
    - ${DRONE_COMMIT_SHA}
    - latest
  when:
    branch: master
    event: push
</code></pre>

<p>Where <code>&lt;your_quay_repo&gt;</code> in:</p>
<pre><code class="yaml">quay.io/ukhomeofficedigital/&lt;your_quay_repo&gt;
</code></pre>

<p>is the name of the Quay repo you (should) have already created.</p>
<blockquote>
<p>Note: ${DRONE_COMMIT_SHA} is a Drone environment variable that is passed to the container at runtime.</p>
</blockquote>
<p>The build should fail with the following error:</p>
<pre><code class="bash">Error response from daemon: Get https://quay.io/v2/: unauthorized: Could not find robot with username: ukhomeofficedigital+&lt;your_robot_username&gt; and supplied password.
</code></pre>

<p>The error points to the missing password for the Quay robot. You will need to add this as a drone secret.</p>
<p>You can do this through the Drone UI by going to your repo, clicking the menu icon in the top right and then clicking <strong>Secrets</strong>. You should be presented with a list of the secrets for that repo (if there are any) and you should be able to add secrets giving them a name and value. Add a secret with the name <code>DOCKER_PASSWORD</code> and with the value being the robot token that was supplied to you.</p>
<p>Alternatively, you can use the Drone CLI to add the secret:</p>
<pre><code>$ drone secret add --repository ukhomeoffice/&lt;your_github_repo&gt; --name DOCKER_PASSWORD --value &lt;your_robot_token&gt;
</code></pre>

<p>Restarting the build should be enough to make it pass.</p>
<blockquote>
<p>The Drone CLI allows for more control over the secret as opposed to the UI. For example, the CLI allows you to specify the image and the events that the secret will be allowed to be used with.</p>
<p>Also note that the secret was specified in the <code>secrets</code> section of the pipeline to give it access to the secret. Without this, the pipeline would not be able to use the secret and it would fail. Secrets in this section are automatically uppercased at runtime so it is important that the secret is uppercased in your commands.</p>
</blockquote>
<p>You can also push specifically tagged images by using the <code>DRONE_TAG</code> Drone environment variable and by using the <code>tag</code> event:</p>
<pre><code class="yaml">tagged_image_to_quay:
  image: quay.io/ukhomeofficedigital/drone-docker
  secrets:
    - docker_password
  environment:
    - DOCKER_USERNAME=ukhomeofficedigital+&lt;your_robot_username&gt;
  registry: quay.io
  repo: quay.io/ukhomeofficedigital/&lt;your_quay_repo&gt;
  tags:
    - ${DRONE_TAG}
  when:
    event: tag
</code></pre>

<p>Tag using <code>git tag v1.0</code> and push your tag with <code>git push origin v1.0</code> (replace <code>v1.0</code> with the tag you actually want to use).</p>
<blockquote>
<p>Note: These pipeline configurations are using the Docker plugin for Drone. For more information, see http://plugins.drone.io/drone-plugins/drone-docker/</p>
</blockquote>
<h3 id="publishing-to-artifactory">Publishing to Artifactory<a class="headerlink" href="#publishing-to-artifactory" title="Permanent link">#</a></h3>
<p>Images hosted on <a href="https://docker.digital.homeoffice.gov.uk">Artifactory</a> are private.</p>
<p>If your repository is hosted publicly on GitHub, you shouldn't publish your images to Artifactory. Artifactory is only used to publish private images. <a href="#publishing-to-quay">You should use Quay to publish your public images</a>.</p>
<p><a href="https://hub.acp.homeoffice.gov.uk/help/support/requests/new/artifactory-token">Submit a support request for a new Artifactory access token</a>. You should be supplied an access token in response.</p>
<p>You can inject the token that has been supplied to you with:</p>
<pre><code>$ drone secret add --repository &lt;gitlab_repo_group&gt;/&lt;your_gitlab_repo&gt; --name DOCKER_PASSWORD --value &lt;your_robot_token&gt;
</code></pre>

<p>You can add the following step in your <code>.drone.yml</code>:</p>
<pre><code class="yaml">image_to_artifactory:
  image: quay.io/ukhomeofficedigital/drone-docker
  secrets:
    - docker_password
  environment:
    - DOCKER_USERNAME=&lt;your_robots_username&gt;
  registry: docker.digital.homeoffice.gov.uk
  repo: docker.digital.homeoffice.gov.uk/&lt;your_artifactory_repo&gt;
  tags:
    - ${DRONE_COMMIT_SHA}
    - latest
  when:
    branch: master
    event: push
</code></pre>

<p>Where the <code>&lt;image_name&gt;</code> in:</p>
<pre><code class="yaml">docker tag &lt;image_name&gt; docker.digital.homeoffice.gov.uk/ukhomeofficedigital/&lt;your_artifactory_repo&gt;:$${DRONE_COMMIT_SHA}
</code></pre>

<p>is the name of the image you tagged previously in the build step.</p>
<p>The image should now be published on Artifactory.</p>
<h2 id="deployments">Deployments<a class="headerlink" href="#deployments" title="Permanent link">#</a></h2>
<h3 id="deployments-and-promotions">Deployments and promotions<a class="headerlink" href="#deployments-and-promotions" title="Permanent link">#</a></h3>
<p>Create a step that runs only on deployments:</p>
<pre><code class="yaml">deploy-to-preprod:
  image: busybox
  commands:
    - /bin/echo hello preprod
  when:
    environment: preprod
    event: deployment
</code></pre>

<p>Push the changes to your remote repository.</p>
<p>You can deploy the build you just pushed with the following command:</p>
<pre><code class="bash">$ drone deploy ukhomeoffice/&lt;your_repo&gt; 16 preprod
</code></pre>

<p>Where <code>16</code> is the successful build number on drone that you wish to deploy to the <code>preprod</code> environment.</p>
<p>You can pass additional parameters to your deployment as environment variables:</p>
<pre><code class="bash">$ drone deploy ukhomeoffice/&lt;your_repo&gt; 16 preprod -p DEBUG=1 -p NAME=Dan
</code></pre>

<p>and use them in the step like this:</p>
<pre><code class="yaml">deploy-to-preprod:
  image: busybox
  commands:
    - /bin/echo hello $${NAME}
  when:
    environment: preprod
    event: deployment
</code></pre>

<p>Environments are strings and can be set to any value. When you wish to deploy to several environments you can create a step for each one of them:</p>
<pre><code class="yaml">deploy-to-preprod:
  image: busybox
  commands:
    - /bin/echo hello preprod
  when:
    environment: preprod
    event: deployment

deploy-to-prod:
  image: busybox
  commands:
    - /bin/echo hello prod
  when:
    environment: prod
    event: deployment
</code></pre>

<p>And deploy them accordingly:</p>
<pre><code class="bash">$ drone deploy ukhomeoffice/&lt;your_repo&gt; 16 preprod
$ drone deploy ukhomeoffice/&lt;your_repo&gt; 16 prod
</code></pre>

<p>Read more on <a href="http://docs.drone.io/environment/">environments</a>.</p>
<h3 id="drone-as-a-pull-request-builder">Drone as a Pull Request builder<a class="headerlink" href="#drone-as-a-pull-request-builder" title="Permanent link">#</a></h3>
<p>Drone pipelines are triggered when events occurs. Event triggers can be as simple as a <em>push</em>, <em>a tagged commit</em>, <em>a pull request</em> or as granular as <em>only for pull requests for a branch named <code>test</code></em>. You can limit the execution of build steps at runtime using theÂ <code>when</code>Â block. As an example, this block executes only on pull requests:</p>
<pre><code class="yaml">pr-builder:
  image: docker:18.03
  environment:
    - DOCKER_HOST=tcp://172.17.0.1:2375
  commands:
    - docker build -t &lt;image_name&gt; .
  when:
    event: pull_request
</code></pre>

<p>Drone will only execute that step when a new pull request is raised (and when pushes are made to the branch while a pull request is open).</p>
<p><a href="http://docs.drone.io/conditional-steps/">Read more about Drone conditions</a>.</p>
<h3 id="deploying-to-acp">Deploying to ACP<a class="headerlink" href="#deploying-to-acp" title="Permanent link">#</a></h3>
<blockquote>
<p>Please note that this section assumes that you already have kube files to work with (specifically, deployment, service and ingress files).
Examples of these files can be found in the <a href="https://github.com/UKHomeOffice/kube-signed-commit-check">kube-signed-commit-check</a> project.</p>
</blockquote>
<p>Add a deployment script with the following:</p>
<pre><code class="bash">#!/bin/bash
export KUBE_NAMESPACE=&lt;dev-induction&gt;
export KUBE_SERVER=${KUBE_SERVER}
export KUBE_TOKEN=${KUBE_TOKEN}

kd  -f deployment.yaml \
    -f service.yaml \
    -f ingress.yaml
</code></pre>

<blockquote>
<p>Please note that this is only an example script and it will need to be changed to fit your particular application's needs.</p>
</blockquote>
<p>If you deployed this now you would likely receive an error similar to this:</p>
<pre><code class="bash">error: You must be logged in to the server (the server has asked for the client to provide credentials)
</code></pre>

<p>This error appears because <a href="https://github.com/UKHomeOffice/kd">kd</a> needs 3 environment variables to be set before deploying:</p>
<ul>
<li>
<p><code>KUBE_NAMESPACE</code> - The kubernetes namespace you wish to deploy to. <strong>You need to provide the kubernetes namespace as part of the deployment job</strong>.</p>
</li>
<li>
<p><code>KUBE_TOKEN</code> - This is the token used to authenticate against the kubernetes cluster. <strong>If you do not already have a kube token, <a href="kubernetes-user-token.html">here are docs explaining how to get one</a></strong>.</p>
</li>
<li>
<p><code>KUBE_SERVER</code> - This is the address of the kubernetes cluster that you want to deploy to.</p>
</li>
</ul>
<p>You will need to add <code>KUBE_TOKEN</code> and <code>KUBE_SERVER</code> as drone secrets. Information about how to add Drone secrets can be found in the <a href="#publishing-to-quay">publishing to Quay section</a>.</p>
<p>You can verify that the secrets for your repo are present with:</p>
<pre><code class="bash">$ drone secret ls --repository ukhomeoffice/&lt;your-repo&gt;
</code></pre>

<p>Once the secrets have been added, add a new step to your drone pipeline that will execute the deployment script:</p>
<pre><code class="yaml">deploy_to_uat:
  image: quay.io/ukhomeofficedigital/kd:v0.11.0
  secrets:
    - kube_server
    - kube_token
  commands:
    - ./deploy.sh
  when:
    environment: uat
    event: deployment
</code></pre>

<h3 id="using-another-repo">Using Another Repo<a class="headerlink" href="#using-another-repo" title="Permanent link">#</a></h3>
<p>It is possible to access files or deployment scripts from another repo, there are two ways of doing this.</p>
<p>The recommended method is to clone another repo in the current repo (since this only requires maintaining one .drone.yml) using the following step:</p>
<pre><code class="yaml">predeploy_to_uat:
  image: plugins/git
  commands:
    - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/&lt;your_repo&gt;.git
  when:
    environment: uat
    event: deployment
</code></pre>

<p>Your repository is saved in the workspace, which in turn is shared among all steps in the pipeline.</p>
<p>However, if you decide that you want to trigger a completely different pipeline on a separate repository, you can leverage the <a href="https://github.com/UKHomeOffice/drone-trigger">drone-trigger</a> plugin. If you have a secondary repository, you can setup Drone on that repository like so:</p>
<pre><code class="yaml">pipeline:
  deploy_to_uat:
    image: busybox
    commands:
      - echo ${SHA}
    when:
      event: deployment
      environment: uat
</code></pre>

<p>Once you are ready, you can push the changes to the remote repository. In your main repository you can add the following step:</p>
<pre><code class="yaml">trigger_deploy:
  image: quay.io/ukhomeofficedigital/drone-trigger:latest
  drone_server: https://drone.acp.homeoffice.gov.uk
  repo: UKHomeOffice/&lt;deployment_repo&gt;
  branch: &lt;master&gt;
  deploy_to: &lt;uat&gt;
  params: SHA=${DRONE_COMMIT_SHA}
  when:
    event: deployment
    environment: uat
</code></pre>

<p>The settings are very similar to the <code>drone deploy</code> command:</p>
<ul>
<li><code>deploy_to</code> is the <a href="http://docs.drone.io/conditional-steps/#environment">environment constraint</a></li>
<li><code>params</code> is a list of comma separated list of arguments. In the command line tool, this is equivalent to <code>-p PARAM1=ONE -p PARAM2=TWO</code></li>
<li><code>repo</code> the repository where the deployment scripts are located</li>
</ul>
<p>The next time you trigger a deployment on the main repository with:</p>
<pre><code class="bash">$ drone deploy UKHomeOffice/&lt;your_repo&gt; 16 uat
</code></pre>

<p>This will trigger a new deployment on the second repository.</p>
<p>Please note that in this scenario you need to inspect 2 builds on 2 separate repositories if you just want to inspect the logs.</p>
<h3 id="versioned-deployments">Versioned deployments<a class="headerlink" href="#versioned-deployments" title="Permanent link">#</a></h3>
<p>When you restart your build, Drone will automatically use the latest version of the code. However always using the latest version of the deployment configuration can cause major issues and isn't recommended. For example when promoting from preprod to prod you want to use the preprod version of the deployment configuration. If you use the latest it could potentially break your production environment, especially as it won't necessarily have been tested.</p>
<p>To counteract this you should use a specific version of your deployment scripts. In fact, you should  <code>git checkout</code> the tag or sha as part of your deployment step.</p>
<p>Here is an example of this:</p>
<pre><code class="yaml">predeploy_to_uat:
  image: plugins/git
  commands:
    - git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/UKHomeOffice/&lt;your_repo&gt;.git
  when:
    environment: uat
    event: deployment

deploy_to_uat:
  image: quay.io/ukhomeofficedigital/kd:v0.11.0
  secrets:
    - kube_server
    - kube_token
  commands:
    - apk update &amp;&amp; apk add git
    - git checkout v1.1
    - ./deploy.sh
  when:
    environment: uat
    event: deployment
</code></pre>

<h2 id="migrating-your-pipeline">Migrating your pipeline<a class="headerlink" href="#migrating-your-pipeline" title="Permanent link">#</a></h2>
<h3 id="secrets-and-signing">Secrets and Signing<a class="headerlink" href="#secrets-and-signing" title="Permanent link">#</a></h3>
<p>It is no longer necessary to sign your <code>.drone.yml</code> so the <code>.drone.yml.sig</code> can be deleted. Secrets can be defined in the Drone UI or using the CLI. Secrets created using the UI will be available to push, tag and deployment events. To restrict to selected events, or to allow pull request builds to access secrets you must use the CLI.</p>
<p>Pipelines by default do not have access to any Drone secrets that you have added. You must now define which secrets a pipeline is allowed access to in a <code>secrets</code> section in your pipeline. Here is an example of a pipeline that has access to the <code>DOCKER_PASSWORD</code> secret which will be used to push an image to Quay:</p>
<pre><code class="yaml">image_to_quay:
  image: quay.io/ukhomeofficedigital/drone-docker
  secrets:
    - docker_password
  environment:
    - DOCKER_USERNAME=ukhomeofficedigital+&lt;your_robot_username&gt;
  registry: quay.io
  repo: quay.io/ukhomeofficedigital/&lt;your_quay_repo&gt;
  tags:
    - latest
  when:
    branch: master
    event: push
</code></pre>

<blockquote>
<p>Note: Secrets names in the <code>secrets</code> section will have their names uppercased at runtime.</p>
</blockquote>
<p>Organisation secrets are no longer available. This means that if you are using any organisation secrets such as <code>KUBE_TOKEN_DEV</code>, you will need to add a secret in Drone to replace it.</p>
<h3 id="docker-in-docker">Docker-in-Docker<a class="headerlink" href="#docker-in-docker" title="Permanent link">#</a></h3>
<p>The Docker-in-Docker (dind) service is no longer required. Instead, change the Docker host to <code>DOCKER_HOST=tcp://172.17.0.1:2375</code> in the <code>environment</code> section of your pipline, and you will be able to access the shared Docker server on the drone agent. Note that it is only possible to run one Docker build at a time per Drone agent.</p>
<p>Since privileged mode was primarily used for docker in docker, you should remove the <code>privileged: true</code> line from your <code>.drone.yml</code>.</p>
<p>You can also use your freshly built image directly and run commands as part of your pipeline.</p>
<p>Example:</p>
<pre><code class="yaml">pipeline:

  build_image:
    image: docker:18.03
    environment:
      - DOCKER_HOST=tcp://172.17.0.1:2375
    commands:
      - docker build -t hello_world .
    when:
      branch: master
      event: push

  test_image:
    image: hello_world
    commands:
      - ./run-hello-world.sh
    when:
      branch: master
      event: push
</code></pre>

<h3 id="services">Services<a class="headerlink" href="#services" title="Permanent link">#</a></h3>
<p>If you use the <code>services</code> section of your <code>.drone.yml</code> it is possible to reference them using the DNS name of the service.</p>
<p>For example, if using the following section:</p>
<pre><code class="yaml">services:
  database:
    image: mysql
</code></pre>

<p>The mysql server would be available on <code>tcp://database:3306</code></p>
<h3 id="variable-escaping">Variable Escaping<a class="headerlink" href="#variable-escaping" title="Permanent link">#</a></h3>
<p>Any Drone variables (secrets and environment variables) must now be escaped by having two $$ instead of one. Examples:</p>
<pre><code class="yaml">${DOCKER_PASSWORD} --&gt; $${DOCKER_PASSWORD}
${DRONE_TAG} --&gt; $${DRONE_TAG}
${DRONE_COMMIT_SHA} --&gt; $${DRONE_COMMIT_SHA}

</code></pre>

<h3 id="scanning-images">Scanning Images<a class="headerlink" href="#scanning-images" title="Permanent link">#</a></h3>
<p>ACP provides Anchore as scanning solution to images built into the Drone pipeline, allowing users to scan both ephemeral <em>(built within the context of the drone, but not pushed to a repository yet)</em> as well and well any public images.</p>
<pre><code class="YAML">pipeline:
  build:
    image: docker:17.09.0-ce
    environment:
    - DOCKER_HOST=tcp://172.17.0.1:2375
    commands:
    - docker build -t docker.digital.homeoffice.gov.uk/myimage:$${DRONE_BUILD_NUMBER} .

  scan:
    # The location of the drone plugin
    image: quay.io/ukhomeofficedigital/anchore-submission:latest
    # The optional path of a Dockerfile
    dockerfile: Dockerfile
    # Note the lack of double $ here (due to the way drone injects variables&quot;
    image_name: docker.digital.homeoffice.gov.uk/myimage:${DRONE_BUILD_NUMBER}
    # This indicates we are willing tolerate any vulnerabilities which are below medium
    tolarates: medium
    # An optional whitelist (comman separated list of CVE's)
    whitelist: CVE_SOMENAME_1,CVE_SOMENAME_2
    # An optional whitelist file containing a list of CSV relative to the repo path
    whitelist_file: &lt;PATH&gt;
    # By default the pligin will exit will fail if any vulnerabilities are discovered which are not tolarated,
    # you change this behaviour by setting the bellow
    fail_on_detection: false
</code></pre>

<h2 id="qas">Q&amp;As<a class="headerlink" href="#qas" title="Permanent link">#</a></h2>
<h3 id="q-the-build-fails-with-error-insufficient-privileges-to-use-privileged-mode">Q: The build fails with <em>"ERROR:Â Insufficient privileges to use privileged mode"</em><a class="headerlink" href="#q-the-build-fails-with-error-insufficient-privileges-to-use-privileged-mode" title="Permanent link">#</a></h3>
<p>A: Remove <code>privileged: true</code> from your <code>.drone.yml</code>. As explained in the <a href="#migrating-your-pipeline">migrating your pipeline section</a>, the primary use of this was for Docker-in-Docker which is not required.</p>
<h3 id="q-the-build-fails-with-cannot-connect-to-the-docker-daemon-is-the-docker-daemon-running-on-this-host">Q: The build fails with <em>"Cannot connect to the Docker daemon. Is the docker daemon running on this host?"</em><a class="headerlink" href="#q-the-build-fails-with-cannot-connect-to-the-docker-daemon-is-the-docker-daemon-running-on-this-host" title="Permanent link">#</a></h3>
<p>A: Make sure that your steps contain the environment variable <code>DOCKER_HOST=tcp://172.17.0.1:2375</code> like in this case:</p>
<pre><code class="yaml">my-build:
  image: docker:18.03
  environment:
    - DOCKER_HOST=tcp://172.17.0.1:2375
  commands:
    - docker build -t &lt;image_name&gt; .
  when:
    branch: master
    event: push
</code></pre>

<h3 id="q-the-build-fails-when-uploading-to-quay-with-the-error-error-response-from-daemon-get-httpsquayiov2-unauthorized">Q: The build fails when uploading to Quay with the error <em>"Error response from daemon: Get https://quay.io/v2/: unauthorized:..."</em><a class="headerlink" href="#q-the-build-fails-when-uploading-to-quay-with-the-error-error-response-from-daemon-get-httpsquayiov2-unauthorized" title="Permanent link">#</a></h3>
<p>A: This is likely because the secret wasn't added correctly or the password is incorrect. Check that the secret has been added to Drone and that you have added the <code>secrets</code> section in your <code>.drone.yaml</code> it to the pipeline that requires it.</p>
<h3 id="q-as-part-of-my-build-process-i-have-two-dockerfiles-to-produce-a-docker-image-how-can-i-share-files-between-builds-in-the-same-step">Q: As part of my build process I have two <code>Dockerfiles</code> to produce a Docker image. How can I share files between builds in the same step?<a class="headerlink" href="#q-as-part-of-my-build-process-i-have-two-dockerfiles-to-produce-a-docker-image-how-can-i-share-files-between-builds-in-the-same-step" title="Permanent link">#</a></h3>
<p>A: When the pipeline starts, Drone creates a Docker data volume that is passed along all active steps in the pipeline. If the first step creates a <code>test.txt</code> file, the second step can use that file. As an example, this pipeline uses a two step build process:</p>
<pre><code class="yaml">pipeline:

  first-step:
    image: busybox
    commands:
      - echo hello &gt; test.txt
    when:
      branch: master
      event: push

  second-step:
    image: busybox
    commands:
      - cat test.txt
    when:
      branch: master
      event: push
</code></pre>

<h3 id="q-should-i-use-gitlab-with-quay">Q: Should I use Gitlab with Quay?<a class="headerlink" href="#q-should-i-use-gitlab-with-quay" title="Permanent link">#</a></h3>
<p>A: Please don't. If your repository is hosted in Gitlab then use Artifactory to publish your images. Images published to Artifactory are kept private.</p>
<p>If you still want to use Quay, you should consider hosting your repository on the open (Github).</p>
<h3 id="q-can-i-create-a-token-that-has-permission-to-create-ephemeraltemporary-namespaces">Q: Can I create a token that has permission to create ephemeral/temporary namespaces?<a class="headerlink" href="#q-can-i-create-a-token-that-has-permission-to-create-ephemeraltemporary-namespaces" title="Permanent link">#</a></h3>
<p>A: No. This is because there is currently no way to give access to namespaces via regex.</p>
<p>I.e. There is no way to give access to any namespace with the format: <code>my-temp-namespace-*</code> (where * would be build number or something similar).</p>
<p>Alternatively, you can be given a named namespace in the CI cluster. Please create an issue on our <a href="https://github.com/UKHomeOffice/application-container-platform-bau">BAU board</a> if you require this.</p>
<h1 id="aws-ecr-for-private-docker-images">AWS ECR for Private Docker Images<a class="headerlink" href="#aws-ecr-for-private-docker-images" title="Permanent link">#</a></h1>
<p>AWS ECR (Elastic Container Registry) is now available as a self-service feature via the Platform Hub. Each project has the capability to create their own Docker Repositories and define individual access to each via the use of IAM Credentials.</p>
<h4>Contents</h4>
<ul>
<li><strong><a href="#creating-a-docker-repository">Creating a Docker Repository</a></strong></li>
<li><strong><a href="#generating-access-credentials">Generating Access Credentials</a></strong></li>
<li><strong><a href="#accessing-a-docker-repository">Accessing a Docker Repository</a></strong></li>
<li><strong><a href="#managing-image-deployments-via-drone-ci">Managing Image Deployments via Drone CI</a></strong></li>
</ul>
<h2 id="creating-a-docker-repository">Creating a Docker Repository<a class="headerlink" href="#creating-a-docker-repository" title="Permanent link">#</a></h2>
<p>Anybody that is part of a Project within the Platform Hub will have the ability to create a new Docker Repository.</p>
<ol>
<li>Login to the Platform Hub via https://hub.acp.homeoffice.gov.uk</li>
<li>Navigate to the Projects list: https://hub.acp.homeoffice.gov.uk/projects/list</li>
<li>Select your Project from the list to go to the detail page (e.g. https://hub.acp.homeoffice.gov.uk/projects/detail/acp)<ul>
<li>Ensure you have a <strong>Service</strong> defined within your Project for the Docker Repository to be associated with (check under the <code>SERVICES</code> tab)</li>
</ul>
</li>
<li>Select the <code>ALL DOCKER REPOS</code> tab</li>
<li>Select the <code>REQUEST NEW DOCKER REPO</code> button</li>
<li>Choose the Service to associate this Repository with and provide the name of the Repository to be created (e.g. <code>hello-world-app</code>)</li>
</ol>
<p>The request to create a new Docker Repository can take a few seconds to complete. You can view the status of a Repository by navigating to the <code>ALL DOCKER REPOS</code> tab and viewing the list. Once the request has completed, your Repository should have the <code>Active</code> label associated with it.</p>
<p>This repository won't automatically refresh, but you can hit the <code>REFRESH</code> button above the Repository list or just manually refresh your browser window for updates.</p>
<h2 id="generating-access-credentials">Generating Access Credentials<a class="headerlink" href="#generating-access-credentials" title="Permanent link">#</a></h2>
<p>Access to ECR Repositories is managed via AWS IAM. These IAM credentials are generated via the Platform Hub and access can be managed per user, per Docker Repository.</p>
<ol>
<li>Navigate to the <code>ALL DOCKER REPOS</code> tab for your Project within the Platform Hub</li>
<li>For the Repository you have created, select the <code>MANAGE ACCESS</code> button</li>
<li>At this stage, you can:<ul>
<li>Create a Robot Account(s), which can be used in deployment pipelines in Drone CI for publishing new images to AWS ECR</li>
<li>Select which Project Members have the ability to pull images, and additionally push updates using their own IAM credentials (separate to the Robot Account(s) and CI builds)</li>
</ul>
</li>
<li>For this example, select your own User and press <code>Save</code>.<ul>
<li><strong>Note:</strong> Generally users should never be granted write access, as any write actions should be performed via CI (using the Robot Accounts).</li>
</ul>
</li>
<li>Press the <code>REFRESH</code> button at the top of the page and check the User Access has a status of <code>active</code></li>
</ol>
<p>Robot Accounts are visible under the Docker Repository, and once they reveal an <code>active</code> status the IAM Credentials are displayed alongside it.</p>
<h2 id="accessing-a-docker-repository">Accessing a Docker Repository<a class="headerlink" href="#accessing-a-docker-repository" title="Permanent link">#</a></h2>
<p>Accessing the AWS Container Registry to Pull &amp; Push images is currently a two-step process:
1. Use IAM Credentials to generate a temporary authorisation token
1. Use the temporary authorisation token to authenticate your docker client with ECR</p>
<blockquote>
<p><strong>Note:</strong> The authorisation token generated for docker login is only valid for 12 hours, and so the process above will need to be repeated.</p>
</blockquote>
<h4>Pre-Requisites</h4>
<p>To follow the below steps you must have:
<em> AWS CLI (version 1.11.91 or above, check with <code>aws --version</code>)
  * Install Guides: <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-linux.html">Linux</a>, <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-macos.html">OSX</a>, <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-windows.html">Windows</a>
</em> Docker (version 17.06 or above, check with <code>docker --version</code>)</p>
<h4>Step 1: Retrieve an authorisation token</h4>
<ol>
<li>Navigate to the <code>Connected Identities</code> page: https://hub.acp.homeoffice.gov.uk/identities</li>
<li>Under <code>Amazon ECR</code> you will have access to your own personal IAM Credentials. These credentials will work across multiple projects whose Repositories you have been granted access to.</li>
</ol>
<p>With the AWS IAM Credentials retrieved from the <code>Connected Identities</code> page, setup a local IAM Profile via the Terminal:</p>
<pre><code class="bash">$ aws configure --profile acp-ecr

AWS Access Key ID [None]: XXXXXXXXXXXXXXXXXXXX
AWS Secret Access Key [None]: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Default region name [None]: eu-west-2
Default output format [None]: json

$ export AWS_PROFILE=acp-ecr
</code></pre>

<p>Now, using the aws-cli you can request an authorisation token to perform a docker login:</p>
<pre><code class="bash">$ aws ecr get-login --no-include-email

docker login -u AWS -p &lt;long-auth-token&gt; https://340268328991.dkr.ecr.eu-west-2.amazonaws.com
</code></pre>

<h4>Step 2: Login with Authorisation Token</h4>
<p>Following a successful <code>ecr get-login</code>, a full docker login command should be returned. Copy and paste the command exactly, to login to the ECR endpoint:</p>
<pre><code class="bash">$ docker login -u AWS -p &lt;long-auth-token&gt; https://340268328991.dkr.ecr.eu-west-2.amazonaws.com

WARNING! Using --password via the CLI is insecure. Use --password-stdin.
Login Succeeded
</code></pre>

<blockquote>
<p><strong>Note:</strong> If you get an error from Step 1 such as <code>Unknown options: --no-include-email</code>, your aws-cli client needs updating. You can omit <code>--no-include-email</code> rather than updating your aws-cli client, but the resulting docker login command will include a deprecated <code>-e none</code> flag (needs to be removed prior to running the command).</p>
</blockquote>
<h4>Pulling &amp; Pushing Images</h4>
<p>Within the ACP Kubernetes Clusters, you do not need to provide an <code>imagePullSecret</code> as was previously required for images in Artifactory. The ACP Clusters will authenticate behind-the-scenes and be able to successfully pull images from any Docker Repositories you create via the Platform Hub.</p>
<p>The Docker Repositories section of the Platform Hub will provide a URL such as follows for the Repository you have created: <code>340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app</code></p>
<p>Now that you have locally authenticated with AWS ECR, you can pull and push (if write access was granted) images as normal:</p>
<pre><code class="bash">$ docker build . -t 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1

Sending build context to Docker daemon  32.78MB
...
Successfully built 882e2cadb649
Successfully tagged 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1

$ docker push 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1

The push refers to repository [340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app]
afbe4b47c182: Pushed
78147c906fce: Pushed
86177d14466d: Pushed
f55514f6bd18: Pushed
ce74984572d7: Pushed
67d7e5db87ee: Pushed
12d012372115: Pushed
b0bb54920d03: Pushed
835c2760f26b: Pushed
e9bcacee1741: Pushed
cd7100a72410: Pushed
v0.0.1: digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f size: 2628

$ docker pull 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app:v0.0.1@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f

sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f: Pulling from acp/hello-world-app
Digest: sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f
Status: Image is up to date for 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app@sha256:0309d2655ecef6b4181ee93edfb91f386fc2ebc7849cc88f6e7a18b0d349c35f
</code></pre>

<h2 id="managing-image-deployments-via-drone-ci">Managing Image Deployments via Drone CI<a class="headerlink" href="#managing-image-deployments-via-drone-ci" title="Permanent link">#</a></h2>
<p>The Docker Authorisation Token generated via the aws-cli command is only valid for 12 hours, and so this can't be used as a Drone Secret for Docker Image builds. Instead, you would need to store the IAM Credentials for a Robot Account as Drone Secrets and perform the <code>aws ecr get-login</code> + <code>docker login ..</code> step on each build.</p>
<p>To simplify this process you can use a custom Drone ECR plugin, which:
- Builds a docker image in the root repository directory, with custom build arguments passed in (optional)
- Authenticates to ECR using your AWS IAM credentials (stored as Drone Secrets)
- Pushes the image to ECR with the given tags in the list (latest and commit sha)</p>
<p><strong>Example Pipeline:</strong></p>
<pre><code class="yml">pipeline:
  build_push_to_ecr:
    image: quay.io/ukhomeofficedigital/ecr:latest
    secrets:
    - AWS_ACCESS_KEY_ID
    - AWS_SECRET_ACCESS_KEY
    repo: 340268328991.dkr.ecr.eu-west-2.amazonaws.com/acp/hello-world-app
    build_args:
    - APP_BUILD=${DRONE_COMMIT_SHA}
    tags:
    - latest
    - ${DRONE_COMMIT_SHA}
</code></pre>

<blockquote>
<p>The <strong><a href="https://github.com/UKHomeOffice/docker-ecr">UKHomeOffice ECR image</a></strong> above is based off the official <a href="https://hub.docker.com/r/plugins/ecr">Docker ECR Plugin</a>, with amendments to run in ACP Drone CI.</p>
</blockquote>
<h1 id="using-ingress">Using Ingress<a class="headerlink" href="#using-ingress" title="Permanent link">#</a></h1>
<p>An <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a> is a type of Kubernetes resource that allows you to expose your services outside the cluster. It gets deployed and managed exactly like other Kube resources.</p>
<p>Our ingress setup offers two different ingresses based on how restrictively you want to expose your services:
- internal - only people within the VPN can access services
- external - anyone with internet access can access services</p>
<p>The annotation <code>kubernetes.io/ingress.class: "nginx-internal"</code> is used to specify whether the ingress is internal. (<code>kubernetes.io/ingress.class: "nginx-external"</code> is used for an external ingress.)</p>
<p>In the following example the terms "myapp" and "myproject" have been used, these will need to be changed to the relevant names for your project. Where internal is used, this can be changed for an external ingress - everything else stays the same.</p>
<pre><code class="yaml">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    # used to select which ingress this resource should be configured on
    kubernetes.io/ingress.class: &quot;nginx-internal&quot;
    # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated.
    ingress.kubernetes.io/secure-backends: &quot;true&quot;
    # This replaces the old annotation secure-backends
    ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;
  name: myapp-server-internal
spec:
  rules:
  - host: &quot;myapp.myproject.homeoffice.gov.uk&quot;
    http:
      paths:
      - backend:
          serviceName: myapp
          servicePort: 8000
        path: /
  tls:
  - hosts:
    - &quot;myapp.myproject.homeoffice.gov.uk&quot;
    # the name of the kubernetes secret in your namespace with tls.crt and tls.key
    secretName: myapp-github-internal-tls
</code></pre>

<blockquote>
<p>Always ensure you are using TLS between the ingress controller and your pods by placing the annotation: ingress.kubernetes.io/backend-protocol: "HTTPS". At the moment; this is not enforced though there are plans to enforce by policy at a later date.</p>
</blockquote>
<h1 id="kube-cert-managaer-cloudflare-ssl">Kube-Cert-Managaer &amp; Cloudflare SSL<a class="headerlink" href="#kube-cert-managaer-cloudflare-ssl" title="Permanent link">#</a></h1>
<p>Services:
- <a href="https://github.com/PalmStoneGames/kube-cert-manager">kube-cert-manager</a> is used to retrieve Letencrypt certificates.
- <a href="https://github.com/cloudflare/cfssl">cfssl</a> is an internal certificate service used to provide internal tls between pods / services.</p>
<h4><strong>Domains and Challenge types</strong></h4>
<p>At present two Let's Encrypt challenge types are supported for certificates which is controlled via the <code>stable.k8s.psg.io/kcm.provider</code> annotation on the Ingress resource; note if no annotation is present the default is <code>stable.k8s.psg.io/kcm.provider: route53</code>.</p>
<ul>
<li>route53: the domain must be hosted within the ACP route53 account, namely to allow kube-cert-manager to add the service record. If you are unsure if this is the case please check with the ACP team. DNS is optional for external sites but a requirement for sites seated behind the VPN.</li>
<li>http: indicates a callback url for authentication. The domain can either be controlled externally via yourself or via the ACP team. Either way the dns record must be a CNAME to the external ingress hostname <em>(please check with the ACP team if you dont know)</em>. Obviously this challenge type can only be used on an external site. Note any IP white-listing on the ingress can still be used.</li>
</ul>
<h4><strong>As a developer I want to grab a certificate from Letsencrypt</strong></h4>
<p>Below is an example of an ingress resource for a HTTPS host reachable over the internet. Note, your ingress resource using IP whitelisting is irrelivant here in regard to Letsencrypt i.e you can still protect the site with and ACL.</p>
<ul>
<li>The host <code>my-app.my-project.homeoffice.gov.uk</code> has a CNAME record to <code>ingress-external.prod.acp.homeoffice.gov.uk</code> (or relevant <code>ingress-external</code> address for the cluster you are using)</li>
</ul>
<pre><code class="YAML">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated.
    ingress.kubernetes.io/secure-backends: &quot;true&quot;
    # This replaces the old annotation secure-backends
    ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;
    kubernetes.io/ingress.class: nginx-external
    # ensure kube-cert-manager uses a http01 challenge
    stable.k8s.psg.io/kcm.provider: http
  labels:
    # this is a toggle to indicate kube-cert-manager should handle this resource
    stable.k8s.psg.io/kcm.class: default
  name: my-app
spec:
  rules:
  - host: my-app.my-project.homeoffice.gov.uk
    http:
      paths:
      - backend:
          serviceName: my-app
          servicePort: 443
        path: /
  tls:
  - hosts:
    - my-app.my-project.homeoffice.gov.uk
    secretName: my-app-external-tls
</code></pre>

<h4><strong>As a developer I want a certificate from a site behind the vpn</strong></h4>
<p>Using Letsencrypt</p>
<p>Below is an example of an ingress resource for a HTTPS host reachable via VPN or services on the private network.</p>
<ul>
<li>The host <code>my-app.my-project.homeoffice.gov.uk</code> has a CNAME record to <code>ingress-internal.prod.acp.homeoffice.gov.uk</code> (or relevant <code>ingress-internal</code> address for the cluster you are using)</li>
<li>The host <code>my-app.my-project.homeoffice.gov.uk</code> is managed via Route53 within the same account that the cluster is running within (done by ACP team)</li>
<li>The host <code>my-app.my-project.homeoffice.gov.uk</code> (or TLD) is listed as a <a href="https://github.com/UKHomeOffice/policy-admission/blob/master/pkg/authorize/kubecertmanager/doc.go#L33">Hosted Domain</a> within the custom policy admission controller (done by ACP team)</li>
</ul>
<pre><code class="YAML">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    # indicate the ingress SHOULD speak TLS between itself and pods - in version 0.18.0 of ingress this was deprecated.
    ingress.kubernetes.io/secure-backends: &quot;true&quot;
    # This replaces the old annotation secure-backends
    ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;
    kubernetes.io/ingress.class: nginx-internal
  labels:
    stable.k8s.psg.io/kcm.class: default
  name: my-app
spec:
  rules:
  - host: my-app.my-project.homeoffice.gov.uk
    http:
      paths:
      - backend:
          serviceName: my-app
          servicePort: 443
        path: /
  tls:
  - hosts:
    - my-app.my-project.homeoffice.gov.uk
    secretName: my-app-internal-tls
</code></pre>

<h4><strong>Using LetsEncrypt with Ingress</strong></h4>
<p>Assuming you are not bringing your own certificates, LetsEncrypt can be used to acquire certificates for both internal <em>(behind vpn NOT cluster TLS certs)</em> and external certificates. Simply place the annotation <code>stable.k8s.psg.io/kcm.class: default</code> into the ingress resource; A full list of the supported features can be found <a href="https://github.com/PalmStoneGames/kube-cert-manager/blob/master/docs/ingress.md">here</a>. Note at present we ONLY allow you to request certificates via the ingress resource, not by the third party resource.</p>
<h4><strong>As a developer I want a certificate for my pod / service</strong></h4>
<h5><strong>The CA bundle</strong></h5>
<p>By default, in all namespaces a CA bundle has been added which can been mounted into the /etc/ssl/certs of the container and which contains the root CA used to verify authenticity of the certificates. An example of using it is given below.</p>
<p>Below is example of how to acquire a certificate from CloudflareSSL.</p>
<pre><code class="YAML">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: example
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        name: example
    spec:
      volumes:
      - name: bundle
        configMap:
          name: bundle
      - name: certs
        emptyDir: {}
      initContainers:
      - name: certs
        # PLEASE do not use latest, but check for the latest tag in the releases page of https://github.com/UKHomeOffice/cfssl-sidekick
        image: quay.io/ukhomeofficedigital/cfssl-sidekick:latest
        securityContext:
          runAsNonRoot: true
        args:
        - --certs=/certs
        - --domain=myservice.${KUBE_NAMESPACE}.svc.cluster.local
        - --domain=another_domain_name
        - --expiry=8760h
        env:
        - name: KUBE_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        # an emptyDir which the sidekicks writes the certificates
        - name: certs
          mountPath: /certs
        # The platform CA Bundle hold the root ca used to verify the certificate chain
        - name: bundle
          mountPath: /etc/ssl/certs
          readOnly: true
      containers:
      - name: your_application
        image: quay.io/ukhomeofficedigital/some_image
        ...
        ports:
        - name: https
          port: 443
          targetPort: 443
        volumeMounts:
        # You can configure your application to pick up the certificates from here (tls.pem and tls-key.pem)
        - name: certs
          mountPath: /certs
          readOnly: true
</code></pre>

<p>To break down what is happening. Firstly we are adding two volumes <code>bundle</code> and <code>certs</code>.</p>
<ul>
<li>the <code>bundle</code> volume is mapped to a configmap which as indicated above is published by us into every namespace and contains the a certificate bundle. This is mounted into the default PKI directory of the container <code>/etc/ssl/certs</code> and permits the container to trust the service.</li>
<li>the <code>certs</code> is a emptyDir which is a <a href="https://en.wikipedia.org/wiki/Tmpfs">tmpfs</a> volume and used to share the cecertificates between the sidekick and your application.</li>
</ul>
<p>We then inject into the <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">initContainers</a> the sidekick service. The sidekick is responsible for</p>
<ul>
<li>locally generating a private key <em>(the private key itself never crosses the wire)</em></li>
<li>generating a CSR for the certificate and requesting a signing from cloudflare service.</li>
<li>once the certificate has been signed its placed into <code>--certs=dir</code> directory which in this case is the emptyDir shared across the containers.</li>
</ul>
<p>Note, if you wish to trust certificates generated by this service simply mounted the bundle into the certificates dorectory.</p>
<h1 id="getting-a-kubernetes-robot-token">Getting a Kubernetes Robot Token<a class="headerlink" href="#getting-a-kubernetes-robot-token" title="Permanent link">#</a></h1>
<h3 id="users">Users<a class="headerlink" href="#users" title="Permanent link">#</a></h3>
<ol>
<li>
<p>Log into the <a href="https://hub.acp.homeoffice.gov.uk">Platform Hub</a>.</p>
</li>
<li>
<p>Go to the <a href="https://hub.acp.homeoffice.gov.uk/projects/list">Projects</a> section and find your project. Click on the <strong>Services</strong> tab and find the service that requires a robot token.</p>
</li>
<li>
<p>Go to the <strong>Kube Robot Tokens</strong> tab. Any robot tokens that have been created for that service will be listed. You can see the full token by clicking on the eye icon next to the token.</p>
</li>
</ol>
<p>If there are no robot tokens for that service, or the required one is not there, you will need to ask your project admin(s) to create a robot token.</p>
<h3 id="project-admins-creating-a-robot-token">Project Admins (Creating a robot token)<a class="headerlink" href="#project-admins-creating-a-robot-token" title="Permanent link">#</a></h3>
<ol>
<li>
<p>Log into the <a href="https://hub.acp.homeoffice.gov.uk">Platform Hub</a>.</p>
</li>
<li>
<p>Go to the <a href="https://hub.acp.homeoffice.gov.uk/projects/list">Projects</a> section and find your project. Click on the <strong>Services</strong> tab and find the service that requires a robot token.</p>
</li>
<li>
<p>Go to the <strong>Kube Robot Tokens</strong> tab and click the <strong>Create a Kubernetes robot token for this service</strong> button.</p>
</li>
<li>
<p>Select the required cluster, RBAC group(s), robot name and description for the robot token and click <strong>Create</strong>.</p>
</li>
</ol>
<blockquote>
<p>An explanation of RBAC groups can be found here: <a href="https://github.com/UKHomeOffice/application-container-platform/blob/master/docs/rbac.md">RBAC Groups</a></p>
</blockquote>
<ol>
<li>Users who are part of the project will be able to view the token in the same place you created it (Project -&gt; Service -&gt; Kube Robot Tokens).</li>
</ol>
<h1 id="getting-a-kubernetes-token">Getting a Kubernetes Token<a class="headerlink" href="#getting-a-kubernetes-token" title="Permanent link">#</a></h1>
<h3 id="users_1">Users<a class="headerlink" href="#users_1" title="Permanent link">#</a></h3>
<ol>
<li>
<p>Log into the <a href="https://hub.acp.homeoffice.gov.uk">Platform Hub</a>.</p>
</li>
<li>
<p>Go to the <a href="https://hub.acp.homeoffice.gov.uk/projects/list">Projects</a> section and find your project. On the <strong>Overview &amp; People</strong> tab, you should see a list of team members and the project admin (who will have the admin tag next to their name).</p>
</li>
<li>
<p>Talk to your project admin and ask them to generate a user token for you.</p>
</li>
<li>
<p>Once your token has been created, you will be able to find it in the <a href="https://hub.acp.homeoffice.gov.uk/identities">Connected Identities</a> section. You will need to expand the <strong>Kubernetes</strong> identity and show your full token by clicking the eye icon next to it.</p>
</li>
</ol>
<h3 id="project-admins-creating-a-user-token">Project Admins (Creating a user token)<a class="headerlink" href="#project-admins-creating-a-user-token" title="Permanent link">#</a></h3>
<ol>
<li>
<p>Log into the <a href="https://hub.acp.homeoffice.gov.uk">Platform Hub</a>.</p>
</li>
<li>
<p>Go to the <a href="https://hub.acp.homeoffice.gov.uk/projects/list">Projects</a> section and find your project. Click on the <strong>Kube User Tokens</strong> tab, click <strong>Select a project team member</strong> and select the requesters name from the list.</p>
</li>
<li>
<p>Click <strong>CREATE A NEW KUBERNETES USER TOKEN FOR THIS USER</strong>.</p>
</li>
<li>
<p>Select the required cluster and RBAC group(s) needed for the token and click <strong>Create</strong>.</p>
</li>
</ol>
<blockquote>
<p>An explanation of RBAC groups can be found here: <a href="https://github.com/UKHomeOffice/application-container-platform/blob/master/docs/rbac.md">RBAC Groups</a></p>
</blockquote>
<ol>
<li>Once the token is created the requester should be able to see it in their <strong>Connected Identities</strong> section for use in their Kube config.</li>
</ol>
<p><strong>Note:</strong> Tokens can take a while to propagate so you may have to wait for up to 10 minutes before using a new token.</p>
<h1 id="network-policies">Network Policies<a class="headerlink" href="#network-policies" title="Permanent link">#</a></h1>
<p>By default a deny-all policy is applied to every namespace in each cluster.</p>
<p>You can however add network policies to your own projects to allow for certain connections and these will be applied on top of the default deny-all policy.</p>
<p>Here is an example network policy for allowing a connection from the ingress-internal namespace:</p>
<pre><code class="yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-network-policy
  namespace: &lt;your-namespace-here&gt;
spec:
  podSelector:
    matchLabels:
      role: artifactory
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-internal
      ports:
        - protocol: TCP
          port: 443
</code></pre>

<p>The port number should be the same as the one that your service is listening on.</p>
<h2 id="controlling-egress-traffic">Controlling Egress Traffic<a class="headerlink" href="#controlling-egress-traffic" title="Permanent link">#</a></h2>
<p>Kubernetes v1.8 with Calico v2.6 adds support to limit egress traffic via the use of Kubernetes Network Policies.</p>
<p>An example of a policy document blocking ALL egress traffic for a given namespace is below:</p>
<pre><code class="yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-egress
  namespace: &lt;your-namespace-here&gt;
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
  - Egress
</code></pre>

<p><strong>NOTE:</strong> The above document will also prevent DNS access for all pods in the namespace. To allow DNS egress traffic via the <code>kube-system</code> namespace, you can apply the following Network Policy document within your namespace (which takes precedence over <code>deny-all-egress</code>):</p>
<pre><code class="yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-access
  namespace: &lt;your-namespace-here&gt;
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
</code></pre>

<p>For more information, please see the following:
- <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Kubernetes documentation on network policies</a>
- <a href="https://docs.projectcalico.org/master/getting-started/kubernetes/tutorials/advanced-policy">Kubernetes advanced network policy examples</a></p>
<h1 id="run-performance-tests-on-a-service-hosted-on-acp">Run Performance Tests on a service hosted on ACP<a class="headerlink" href="#run-performance-tests-on-a-service-hosted-on-acp" title="Permanent link">#</a></h1>
<h3 id="as-a-service-i-should">As a Service, I should:<a class="headerlink" href="#as-a-service-i-should" title="Permanent link">#</a></h3>
<ul>
<li>Always have a baseline set of metrics of my isolated service</li>
<li>Understand what those metrics need to be for each functionality i.e. how long file uploads should take vs a generic GET request</li>
<li>Make sure the baseline does not include any other components i.e. networks, infrastructure etc.</li>
<li>Expose a set of metrics, see <a href="../services.html##metrics-monitoring">Metrics</a></li>
<li>Make performance testing part of my Continuous Integration workflow</li>
<li>Have a history of performance over time</li>
</ul>
<h3 id="assessed-tools-summary">Assessed tools summary:<a class="headerlink" href="#assessed-tools-summary" title="Permanent link">#</a></h3>
<ul>
<li>An example usage of Blazemeter's Taurus in a drone pipeline can be seen in the <a href="https://github.com/UKHomeOffice/taurus-project-x">taurus-project-x repo</a>.</li>
<li><a href="https://github.com/shoreditch-ops/artillery">Artillery</a> (<a href="https://www.npmjs.com/package/artillery">npm</a>) was also tested w/ the <a href="https://github.com/shoreditch-ops/artillery-plugin-statsd">statsd plugin</a>, visualising data in grafana.</li>
<li>SonarQube plugin jmeter-sonar is now deprecated. The latest version of sonarqube does not to have plugin support for jmeter</li>
<li>another option is <a href="https://docs.k6.io/docs">k6</a> - tool is written in go and tests are written in javascript. To visualise the only option is InfluxDB and Grafana.</li>
</ul>
<h1 id="pod-security-policies">Pod Security Policies<a class="headerlink" href="#pod-security-policies" title="Permanent link">#</a></h1>
<p>By default all user deployments will inherit a default PodSecurityPolicy applied in the Kubernetes Clusters, which define a set of conditions that a pod must be configured with in order to run successfully.</p>
<p>The specification for the default policy is as follows:</p>
<pre><code class="yaml">apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  privileged: false
  fsGroup:
    rule: RunAsAny
  hostPID: false
  hostIPC: false
  hostNetwork: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - configMap
  - downwardAPI
  - emptyDir
  - gitRepo
  - persistentVolumeClaim
  - projected
  - secret
</code></pre>

<h2 id="runasuser"><code>runAsUser</code><a class="headerlink" href="#runasuser" title="Permanent link">#</a></h2>
<p>This condition requires that the pod specification deploys an image with a non-root user. The user defined in the specification (image spec OR pod spec) must be numeric, so that Kubernetes will be able to verify that it is a non-root user. If this is not done, you may receive any of the following errors in your event log and your pod will be prevented from starting up successfully:
- <code>container's runAsUser breaks non-root policy</code>
- <code>container has runAsNonRoot and image will run as root</code>
- <code>container has runAsNonRoot and image has non-numeric user &lt;username&gt;, cannot verify user is non-root</code></p>
<blockquote>
<p><strong>Note:</strong> You can view all recent events in your namespace by running the following command: <code>kubectl -n my-namespace get events --sort-by=.metadata.creationTimestamp</code>.</p>
</blockquote>
<p>To update your deployment accordingly for the above condition, there are multiple ways to achieve this:</p>
<h3 id="dockerfile">Dockerfile<a class="headerlink" href="#dockerfile" title="Permanent link">#</a></h3>
<p>Within the <code>Dockerfile</code> for the image you are attempting to run, ensure the <code>USER</code> specified references the User ID rather than the username itself. For example:</p>
<pre><code>FROM quay.io/gambol99/keycloak-proxy:v2.1.1
LABEL maintainer=&quot;rohith.jayawardene@digital.homeoffice.gov.uk&quot;

RUN adduser -D -u 1000 keycloak

USER 1000
</code></pre>

<blockquote>
<p><strong>Note:</strong> The following common images have been updated to reference the UID within their respective Dockerfiles. If you use any of these images, updating your deployments to use these versions (or any newer versions) will meet the <code>MustRunAsNonRoot</code> requirement for this particular container:</p>
</blockquote>
<pre><code>quay.io/ukhomeofficedigital/cfssl-sidekick:v0.0.6
quay.io/ukhomeofficedigital/elasticsearch:v1.5.3
quay.io/ukhomeofficedigital/jira:v7.9.1
quay.io/ukhomeofficedigital/keycloak:v3.4.3-2
quay.io/ukhomeofficedigital/kibana:v0.4.4
quay.io/ukhomeofficedigital/go-keycloak-proxy:v2.1.1
quay.io/ukhomeofficedigital/nginx-proxy:v3.2.9
quay.io/ukhomeofficedigital/nginx-proxy-govuk:v3.2.9.0
quay.io/ukhomeofficedigital/redis:v0.1.2
quay.io/ukhomeofficedigital/squidproxy:v0.0.5
</code></pre>

<h3 id="deployment-spec">Deployment Spec<a class="headerlink" href="#deployment-spec" title="Permanent link">#</a></h3>
<p>In the <code>securityContext</code> section of your deployment spec, the <code>runAsUser</code> field can be used to set a UID that the image should be run as.</p>
<p>An example spec would include:</p>
<pre><code class="yaml">    spec:
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      containers:
      - name: &quot;{{ .IMAGE_NAME }}&quot;
        image: &quot;{{ .IMAGE }}:{{ .VERSION }}&quot;
        ...
</code></pre>

<h1 id="using-artifactory-as-a-private-npm-registry">Using artifactory as a private npm registry<a class="headerlink" href="#using-artifactory-as-a-private-npm-registry" title="Permanent link">#</a></h1>
<p>A step-by-step guide.</p>
<p>This guide makes the following assumptions:</p>
<ul>
<li>you have drone ci set up for your project already</li>
<li>you are using <code>node@8</code> and <code>npm@5</code> or later</li>
<li>you are connected to ACP VPN</li>
</ul>
<h2 id="setting-up-a-local-environment">Setting up a local environment<a class="headerlink" href="#setting-up-a-local-environment" title="Permanent link">#</a></h2>
<h3 id="get-your-username-and-api-key-from-artifactory">Get your username and API key from artifactory<a class="headerlink" href="#get-your-username-and-api-key-from-artifactory" title="Permanent link">#</a></h3>
<p>Visit https://artifactory.digital.homeoffice.gov.uk/artifactory/webapp/#/profile, make a note of your username, and if you don't already have an API key then generate one.</p>
<h3 id="base64-encode-your-api-key">base64 encode your API key<a class="headerlink" href="#base64-encode-your-api-key" title="Permanent link">#</a></h3>
<pre><code>echo -n &lt;api key&gt; | base64
</code></pre>

<h3 id="set-local-environment-variables">Set local environment variables<a class="headerlink" href="#set-local-environment-variables" title="Permanent link">#</a></h3>
<p>Copy your encoded password, and set the following environment variables in your bash profile:</p>
<pre><code>export NPM_AUTH_USERNAME=&lt;username&gt;
export NPM_AUTH_TOKEN=&lt;base64 encoded api key&gt;
</code></pre>

<p>You might then need to <code>source</code> your profile to load these environment variables.</p>
<h2 id="setting-up-ci-in-drone">Setting up CI in drone<a class="headerlink" href="#setting-up-ci-in-drone" title="Permanent link">#</a></h2>
<h3 id="request-a-bot-token-for-artifactory">Request a bot token for artifactory<a class="headerlink" href="#request-a-bot-token-for-artifactory" title="Permanent link">#</a></h3>
<p>You can do this through the <a href="https://hub.acp.homeoffice.gov.uk/help/support/requests/new/artifactory-bot">ACP Hub</a>. You'll need to provide a username for the bot when you create it.</p>
<p>One of the ACP team will create a token and send it to you as an encrypted gpg file via email.</p>
<p>Decrypt the token</p>
<pre><code>gpg --decrypt ./path/to/file.gpg
</code></pre>

<h3 id="add-the-token-to-drone-as-a-secret">Add the token to drone as a secret<a class="headerlink" href="#add-the-token-to-drone-as-a-secret" title="Permanent link">#</a></h3>
<p>First, base64 encode the token:</p>
<pre><code>echo -n &quot;&lt;token&gt;&quot; | base64
</code></pre>

<p>Then add this token to drone as a secret:</p>
<pre><code>drone secret add UKHomeOffice/&lt;repo&gt; NPM_AUTH_TOKEN &lt;base64-encoded-token&gt; --event pull_request
</code></pre>

<p>Note: you will need to make the secret available to pull request builds to be able to run npm commands in pull request steps</p>
<h3 id="expose-secret-to-build-steps">Expose secret to build steps<a class="headerlink" href="#expose-secret-to-build-steps" title="Permanent link">#</a></h3>
<p>You will need to configure any steps which use npm to be able to access the secret. Do this by adding a <code>secret</code> property to those steps as follows:</p>
<pre><code class="yaml">  my_step:
    image: node:8
    secrets:
      - npm_auth_token
    commands:
      - npm install
      - npm test
</code></pre>

<h3 id="expose-username-to-build-steps">Expose username to build steps<a class="headerlink" href="#expose-username-to-build-steps" title="Permanent link">#</a></h3>
<p>In addition, you will need to add the username (as you provided when creating your token) as an environment variable. The easiest way to do this is as a "matrix" variable, which makes the username available to all steps without needing to configure them all individually.</p>
<pre><code class="yaml">matrix:
  NPM_AUTH_USERNAME:
    - &lt;username&gt;
</code></pre>

<h2 id="publishing-modules-to-artifactory">Publishing modules to artifactory<a class="headerlink" href="#publishing-modules-to-artifactory" title="Permanent link">#</a></h2>
<p>It is generally recommended to use a common namespace to publish your modules under. npm allows namespace specific configuration, which makes it easier to ensure that modules are always installed from artifactory, and will not accidentally try to install a public module with the same name.</p>
<h3 id="setting-publish-registry">Setting publish registry<a class="headerlink" href="#setting-publish-registry" title="Permanent link">#</a></h3>
<p>Add <code>publishConfig</code> to package.json. This ensures that the module can only ever be published to the private registry, and misconfiguration won't accidentally make it public</p>
<pre><code class="json">&quot;publishConfig&quot;: {
  &quot;registry&quot;: &quot;https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/&quot;
}
</code></pre>

<h3 id="add-auth-settings">Add auth settings<a class="headerlink" href="#add-auth-settings" title="Permanent link">#</a></h3>
<p>In your project's <code>.npmrc</code> file (create one if it does not already exist) add the following lines:</p>
<pre><code>//artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME}
//artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN}
//artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:email=test@example.com
</code></pre>

<p>The email address can be anything, it just needs to be set.</p>
<h3 id="add-publish-step-to-drone">Add publish step to drone<a class="headerlink" href="#add-publish-step-to-drone" title="Permanent link">#</a></h3>
<p>Add the following step to your <code>.drone.yml</code> file to publish a new version whenever you release a tag.</p>
<pre><code class="yaml">  publish:
    image: node:8
    secrets:
      - npm_auth_token
    commands:
      - npm publish
    when:
      event: tag
</code></pre>

<p>Now, when you push new tags to github then drone should publish them to the artifactory npm registry automatically.</p>
<h2 id="using-modules-from-artifactory-as-dependencies">Using modules from artifactory as dependencies<a class="headerlink" href="#using-modules-from-artifactory-as-dependencies" title="Permanent link">#</a></h2>
<h3 id="configure-your-project-to-use-artifactory">Configure your project to use artifactory<a class="headerlink" href="#configure-your-project-to-use-artifactory" title="Permanent link">#</a></h3>
<p>In the project which is has private modules as dependencies, add the following line to <code>.npmrc</code> in the root of the project (create this file if it does not exist).</p>
<pre><code>@&lt;namespace&gt;:registry = https://artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/
</code></pre>

<p>This will ensure that any module under that namespace will only ever install from artifactory, and never from the public registry</p>
<p>If using multiple namespaces then add a line for each namespace.</p>
<p>If the modules you are installing are not namespaced in artifactory, you can add the line with the namespace removed (i.e. <code>registry = ...</code>) but this will have a negative impact on install speed.</p>
<p>You should then add the following line to your project's <code>.npmrc</code> if they are not already there:</p>
<pre><code>//artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:username=${NPM_AUTH_USERNAME}
//artifactory.digital.homeoffice.gov.uk/artifactory/api/npm/npm-virtual/:_password=${NPM_AUTH_TOKEN}
</code></pre>

<p>You should now be able to install modules from artifactory into your local development environment.</p>
<h2 id="installing-dependencies-in-docker">Installing dependencies in docker<a class="headerlink" href="#installing-dependencies-in-docker" title="Permanent link">#</a></h2>
<p>If you build a docker image as part of your CI pipeline, you will need to copy the <code>.npmrc</code> file into your image before installing there.</p>
<p>Example <code>Dockerfile</code>:</p>
<pre><code>FROM quay.io/ukhomeofficedigital/nodejs-base:v8

ARG NPM_AUTH_USERNAME
ARG NPM_AUTH_TOKEN

COPY .npmrc /app/.npmrc
COPY package.json /app/package.json
COPY package-lock.json /app/package-lock.json
RUN npm install --production --no-optional
COPY . /app

USER nodejs

CMD node index.js
</code></pre>

<p>When building the image, you will then need to pass the username and token variables into docker with the <code>--build-arg</code> flag.</p>
<pre><code>docker build --build-arg NPM_AUTH_USERNAME=$${NPM_AUTH_USERNAME} --build-arg NPM_AUTH_TOKEN=$${NPM_AUTH_TOKEN} .
</code></pre>

<h1 id="provisioned-volumes-and-storage-classes">Provisioned Volumes and Storage Classes<a class="headerlink" href="#provisioned-volumes-and-storage-classes" title="Permanent link">#</a></h1>
<p>In order to use volumes with your pod, we use kubernetes provisioned volume claims and storage classes, to read more about this please see <a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/">Kubernetes Dynamic Provisioning</a>.</p>
<p>On each cluster in ACP, we have the the following storage classes for you to use:</p>
<pre><code>gp2-encrypted
gp2-encrypted-eu-west-2a
gp2-encrypted-eu-west-2b
gp2-encrypted-eu-west-2c
io1-encrypted-eu-west-2
io1-encrypted-eu-west-2a
io1-encrypted-eu-west-2b
io1-encrypted-eu-west-2c
st1-encrypted-eu-west-2
st1-encrypted-eu-west-2a
st1-encrypted-eu-west-2b
st1-encrypted-eu-west-2c
</code></pre>

<p>The <code>io1-*</code> (provisioned iops) storage classes have <code>iopsPerGB: "50"</code></p>
<h2 id="backups-for-ebs">Backups for EBS<a class="headerlink" href="#backups-for-ebs" title="Permanent link">#</a></h2>
<p>Once the ebs has been created, if you'd like to enable EBS snapshots for backups, please raise a ticket via the <a href="https://github.com/UKHomeOffice/application-container-platform-bau">BAU support</a> so that we can add AWS tags to the volume, which will be picked up by <a href="https://github.com/UKHomeOffice/docker-ebs-snapshot">ebs-snapshot</a>. Please remember to specify the retention policy in days to keep the snapshots for.</p>
<h1 id="tls-passthrough">TLS Passthrough<a class="headerlink" href="#tls-passthrough" title="Permanent link">#</a></h1>
<p>There are occasions when you don't want the TLS to be terminated on the ingress and prefer to terminate in the pod instead. Note, by terminating in the pod the ingress will no longer be able to perform any L7 actions, so all of the feature set is lost (effectively it's become a TLS proxy using SNI to route the traffic)</p>
<h4><strong>Example steps</strong></h4>
<p>First create a kubernetes secret containing the certificate you wish to use.</p>
<pre><code class="shell">$ kubectl create secret tls tls --cert=cert.pem --key=cert-key.pem
</code></pre>

<p>Create the deployment and service.</p>
<pre><code class="YAML">---
apiVersion: v1
kind: Service
metadata:
  labels:
    name: tls-passthrough
  name: tls-passthrough
spec:
  type: ClusterIP
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: 10443
  selector:
    name: tls-passthrough
---
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: tls-passthrough
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        name: tls-passthrough
    spec:
      volumes:
      - name: certs
        secret:
          secretName: tls
      containers:
      - name: proxy
        image: quay.io/ukhomeofficedigital/nginx-proxy:v3.2.0
        ports:
        - name: https
          containerPort: 10443
          protocol: TCP
        env:
        - name: PROXY_SERVICE_HOST
          value: &quot;127.0.0.1&quot;
        - name: PROXY_SERVICE_PORT
          value: &quot;8080&quot;
        - name: SERVER_CERT
          value: /certs/tls.crt
        - name: SERVER_KEY
          value: /certs/tls.key
        - name: ENABLE_UUID_PARAM
          value: &quot;FALSE&quot;
        - name: NAXSI_USE_DEFAULT_RULES
          value: &quot;FALSE&quot;
        - name: PORT_IN_HOST_HEADER
          value: &quot;FALSE&quot;
        volumeMounts:
        - name: certs
          mountPath: /certs
          readOnly: true
      - name: fake-application
        image: kennethreitz/httpbin:latest
</code></pre>

<p>Push out the ingress resource indicating you want ssl-passthrough enabled.</p>
<pre><code class="YAML">---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
    kubernetes.io/ingress.class: &quot;nginx-external&quot;
  name: tls-passthrough
spec:
  rules:
  - host: tls-passthrough.notprod.homeoffice.gov.uk
    http:
      paths:
      - backend:
          serviceName: tls-passthrough
          servicePort: 443
        path: /
</code></pre>

<h1 id="writing-dockerfiles">Writing Dockerfiles<a class="headerlink" href="#writing-dockerfiles" title="Permanent link">#</a></h1>
<h3 id="dockerfile-best-practice">Dockerfile best practice<a class="headerlink" href="#dockerfile-best-practice" title="Permanent link">#</a></h3>
<p>We recommend using dockers excellent guidance for this!<br />
https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/</p>
<h3 id="docker-images-to-build-from">Docker images to build from<a class="headerlink" href="#docker-images-to-build-from" title="Permanent link">#</a></h3>
<p>This document lists all of our docker base images that you can build from:</p>
<h3 id="technology-specific-images">Technology specific images<a class="headerlink" href="#technology-specific-images" title="Permanent link">#</a></h3>
<ul>
<li><a href="https://github.com/UKHomeOffice/docker-nodejs">NodeJS onbuild image - recommended by default</a></li>
<li><a href="https://github.com/UKHomeOffice/docker-nodejs-base">NodeJS base image - if you need more flexibility</a></li>
<li><a href="https://github.com/UKHomeOffice/docker-scala-sbt">Scala image</a></li>
<li><a href="https://github.com/UKHomeOffice/docker-openjdk8">JDK image</a></li>
<li><a href="https://github.com/UKHomeOffice/docker-java8-mvn">Maven image with Java 8</a></li>
<li><a href="https://github.com/UKHomeOffice/docker-ruby">Ruby image</a></li>
</ul>
<h3 id="home-office-centos-base-image">Home Office CentOS base image<a class="headerlink" href="#home-office-centos-base-image" title="Permanent link">#</a></h3>
<p>If none of the technology specific images work for you you can either build on top of them or build from the base centos image:<br />
https://github.com/UKHomeOffice/docker-centos-base</p>
<p>If you build an image that will be of use to other teams then please add it to the list of technology specific images above! And please make sure it adheres to the below guidance on building new base images.</p>
<h3 id="guidance-on-building-new-base-images">Guidance on building new base images<a class="headerlink" href="#guidance-on-building-new-base-images" title="Permanent link">#</a></h3>
<p>All base images should be built with a set of onbuild commands in them to make sure anything built on top of them will automatically update the base OS, for example:</p>
<pre><code>yum install -y curl &amp;&amp; yum clean all &amp;&amp; rpm --rebuilddb
</code></pre>

<p>The <a href="https://github.com/UKHomeOffice/docker-nodejs-base/blob/master/Dockerfile">nodejs base image</a> is a good example of this.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../developer-docs/index.html" class="btn btn-neutral float-right" title="Developer Getting Started Guide">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../releases-notes/index.html" class="btn btn-neutral" title="Release Notes"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../releases-notes/index.html" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../developer-docs/index.html" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>

<!--
MkDocs version : 1.0.4
Build Date UTC : 2019-03-01 13:07:28
-->
